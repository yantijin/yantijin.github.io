<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A Cost-Effective Deadline-Constrained Dynamic Scheduling Algorithm for Scientic Workflows in a Cloud Environment</title>
    <url>/2018/11/06/A-Cost-Effective-Deadline-Constrained-Dynamic-Scheduling-Algorithm-for-Scientic-Workflows-in-a-Cloud-Environment/</url>
    <content><![CDATA[<h2 id="Benefits-and-Issues-of-Cloud"><a href="#Benefits-and-Issues-of-Cloud" class="headerlink" title="Benefits and Issues of Cloud"></a>Benefits and Issues of Cloud</h2><h3 id="Benefits"><a href="#Benefits" class="headerlink" title="Benefits"></a>Benefits</h3><ul>
<li>Infinite economical resources</li>
<li>Direct on-demand provisioning</li>
<li>Elasticity</li>
</ul>
<h3 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h3><ul>
<li>Performance variation</li>
<li>Instance acquisition and terminal delay</li>
<li>Heterogeneous IaaS resources</li>
</ul>
<h2 id="Basic-Assumptions"><a href="#Basic-Assumptions" class="headerlink" title="Basic Assumptions"></a>Basic Assumptions</h2><ul>
<li><p>Be in the same data center or region, so the bandwidth is roughly equal.</p>
</li>
<li><p>Different types of VMs, and no limitation on the number of VM.</p>
</li>
<li><p>When an VM leased, requires an initial boot time for initialization. When released, requires some time for proper shutdown.</p>
</li>
<li><p>Charged for time intervals they use a VM.</p>
</li>
<li><p>Data transfer cost is assumed to zero because of internal data transfer is free for most cloud environments</p>
</li>
<li><p>VM type($VM_v$) is defined by $\{(ET_{t_i})_v,C_v\}$, specifies its estimated processing time for each task $t_i$ and cost per time interval.</p>
</li>
<li><p>Data transfer time TT($e_{ij}$) is $\frac{dt_i}{\beta}$, where  $dt_i$ is the size of the output data file to be transfered from $t_i$ to $t_j$, and $\beta$ is the average bandwidth within the cloud datacenter</p>
</li>
<li><p><strong>Objective</strong>: minimize the total  execution cost while meeting the user defined deadline constraints</p>
<script type="math/tex; mode=display">
Minimize~C = \sum^{|VMpool|}_{k=1}C_{type(v_k)} * [(\frac{et_k- st_k}{time~interval})]\\
subject~to~TET \leq D where, TET = \max_{t_i}\{(AFT(t_i\}</script></li>
</ul>
<p><img src="/2018/11/06/A-Cost-Effective-Deadline-Constrained-Dynamic-Scheduling-Algorithm-for-Scientic-Workflows-in-a-Cloud-Environment/structure.png" alt="structure"></p>
<h2 id="Symbols-and-Meanings"><a href="#Symbols-and-Meanings" class="headerlink" title="Symbols and Meanings"></a>Symbols and Meanings</h2><p><img src="/2018/11/06/A-Cost-Effective-Deadline-Constrained-Dynamic-Scheduling-Algorithm-for-Scientic-Workflows-in-a-Cloud-Environment/symbols.png" alt="symbols"></p>
<ul>
<li><script type="math/tex; mode=display">MET(t_1) = \min_{VM_v\in VM_{set}}\{ET(t_i, VM_v)\}</script></li>
<li><script type="math/tex; mode=display">
\begin{equation}
\left\{
    \begin{array}{**rc1**}
    EST(t_{entry}) &=& 0\\
    EST(t_i) &=& \max_{t_p \in t_i's~parent} \{EST(t_p)+MET(t_p)+TT(ep_i)\}
    \end{array}
\right.
\end{equation}</script></li>
<li><script type="math/tex; mode=display">EFT(t_i) = EST(t_i)+MET(t_i)</script></li>
<li><script type="math/tex; mode=display">
XFT(t_i) = \begin{equation}
\left\{
    \begin{array}{**rc1**}
    AST(t_i)+ET(t_i, type(v_k)), if ~t_i~ is ~ in~execution\\
    \max_{t_p \in t_i's~parent}\{XFT(t_p) +TT(e_{pi}\}+ET(t,type(v_k)),\\
    if~ t_i~is~waiting~for~execution
    \end{array}
\right.
\end{equation}</script></li>
<li><script type="math/tex; mode=display">
\begin{equation}
\left\{
    \begin{array}{**rc1**}
      XST(t_{entry}) &=& acquistiondelay\\
    XST(t_i) &=& \max_{tp\in t_i's~parent}\{XFT(t_p)+TT(e_{pi}\}
    \end{array}
\right.
\end{equation}</script></li>
<li><script type="math/tex; mode=display">
XIST(v_k) = \begin{equation}
\left\{
    \begin{array}{**rc1**}
    AST(t_p)+ET(t_p,type(v_k)), if~t_p~is~in~execution\\
    XST(t_p)+ET(t_p,type(v_k)),if~t_p~is~waiting~for~execution
    \end{array}
\right.
\end{equation}</script></li>
<li><script type="math/tex; mode=display">
\begin{equation}
\left\{
    \begin{array}{**rc1**}
    LFT(t_{exit} &=& D)\\
    LFT(t_i) &=& \min_{t_c\in t_i's~children}\{LFT(t_c)-MET(t_c)-TT(e_{ic})\}
    \end{array}
\right.
\end{equation}</script></li>
<li><script type="math/tex; mode=display">LST(t_i) = LFT(t_i) - MET(t_i)</script></li>
<li><script type="math/tex; mode=display">
\begin{equation}
\left\{
    \begin{array}{**rc1**}
    XET(t_{exit},VM_{v}) &=& ET(t_{exit},VM_v)\\
    XET(t_i,VM_v) &=& ET(t_i,VM_v)+\max_{t_c\in t_i's~parent}\{XET(t_c,VM_v)\}
    \end{array}
\right.
\end{equation}</script></li>
<li><script type="math/tex; mode=display">MET_W = \max_{t_i\in W}(EFT(t_i))</script></li>
<li><script type="math/tex; mode=display">CLI(v_k) = [st_k,st_k+n\times time~intervals]</script></li>
</ul>
]]></content>
      <categories>
        <category>Cloud Workflow Scheduling</category>
      </categories>
      <tags>
        <tag>Cloud Workflow Scheduling</tag>
      </tags>
  </entry>
  <entry>
    <title>A Fast Algorithm for Projected Clustering</title>
    <url>/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/</url>
    <content><![CDATA[<h2 id="投影聚类法"><a href="#投影聚类法" class="headerlink" title="投影聚类法"></a>投影聚类法</h2><h3 id="Basic-Conceprions"><a href="#Basic-Conceprions" class="headerlink" title="Basic Conceprions"></a>Basic Conceprions</h3><ul>
<li><strong>投影聚类</strong>:考虑一些多维空间中的一组数据点。 投影聚类是数据点的子集C以及维度的子集p，使得C中的点紧密地聚集在维度V的子空间中。</li>
</ul>
<h3 id="基本符号定义"><a href="#基本符号定义" class="headerlink" title="基本符号定义"></a>基本符号定义</h3><ul>
<li>N:数据点的数量</li>
<li>$C = \{x_1,x_2,…,x_t\}$是在一个类中数据点的集合</li>
<li>cluster C的<strong>中心点</strong>定义为: $\bar{x}_c = \sum^t_{i=1} x_i/t$</li>
<li>定义一个类的<strong>半径radius</strong>为 一个点到类中心点的平均距离 $\bar{x}_c = \sum^t_{i=1} d(\bar{x}_c,x_i)/t$</li>
</ul>
<h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><ul>
<li><p>算法分为三个部分： 初始化阶段(initialization phase)；迭代阶段(iteration phase)；类别调整阶段(cluster refinement phase)</p>
</li>
<li><p><strong>初始化阶段</strong>：使用Greedy method</p>
<p><img src="/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/1.png" alt="1"></p>
<p>首先从数据集中随机确定一个中心点，加入M，然后计算其他每一个点到M中所有中心点中最近的距离，然后取距离最大的点作为下一个中心点加入M，重新计算剩余点到M中所有中心点中最近的距离，依次迭代下去，直到得到满足个数的集合M</p>
</li>
<li><p><strong>迭代阶段</strong>：</p>
<p>(1)首先从集合M中选择k个作为初始的中心点，然后重复(2)-(6)</p>
<p>(2)对于M中的每一个中心点$m_i$：</p>
<p>​    $\delta_i$是$m_i$距离$m_i$最近的中心点的距离</p>
<p>​    $L_i$是以$m_i$为圆心，$\delta_i$为半径中包含的点的集合</p>
<p>$L = \{L_1,…,L_k\}$</p>
<p>(3)为集合中每一个中心点找到与之相关的维度,l最少为2(D = FindDimensions(k,l,L))</p>
<p>(4)通过找到的维度将所有点按照距离来进行分类，((C1,…,Ck) = AssignPoints(D1,…,Dk))</p>
<p>(5)通过分类和维度来计算目标函数；如果比目前最好的目标函数还要好，就记录下中心点的分类和当前目标函数的值。</p>
<p>(6)从当前中心点中找到最坏的一些中心点，然后去除掉之后，从M中重新加入一些点来构成新的集合</p>
</li>
<li><p><strong>聚类改进阶段(refinement phase)</strong><br><img src="/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/2.png" alt="2"><br><img src="/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/3.png" alt="3"><br><img src="/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/4.png" alt="4"></p>
<h3 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h3></li>
<li><p>FindDimensions(k,l,L):</p>
<p><img src="/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/5.png" alt="5"></p>
<ul>
<li><p>其中 $X_{ij}$为$L_i$中的点到中心点$m_i$在维度j上的距离</p>
</li>
<li><p>$Y_i$为对第i个中心点来说，对应的$L_i$到中心点$m_i$上所有维度的平均距离</p>
</li>
<li><p>而后计算标准偏差$\sigma_i$,就可以得到一个为每一个维度进行一个归一化的评价</p>
<p>为$Z_{ij} = (X_{ij}-Y_i)/\sigma_i$</p>
</li>
<li><p>对于每一个中心点$m_i$对应的$Z_{ij}$，选择最小的l个并将这l个记录在$D_i$中，最终返回$D = \{D_1,D_2,…,D_k\}$</p>
</li>
</ul>
</li>
<li><p>AssignPoints函数：<br><img src="/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/6.png" alt="6"></p>
<p>注意计算的是曼哈顿距离(街区距离)</p>
<p>选取距离最小的作为某分类(其规则是关于$D_i$的维度上)中的点</p>
</li>
<li><p>evaluateClusters函数：<br><img src="/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/7.png" alt="7"></p>
<p>评估一组中心点的质量，从点到它们所属的cluster的质心的曼哈顿距离的平均值作为评价标准</p>
</li>
<li><p>丢弃：首先确定哪些是坏的中心点：</p>
<ul>
<li>(1)周围有最少点的中心点是坏的中心点</li>
<li>(2)若中心点周围的点数比$\frac{N}{k}*minDeviation$少，那么他是坏的中心点，mindDeviation是一个小于1的常数(一般用0.1)</li>
</ul>
</li>
<li><p><strong>聚类改进阶段</strong>：再进行(2)-(4)一次，唯一不同的是此次用C而不是用L</p>
</li>
</ul>
<p><a href="https://github.com/cmmp/pyproclus">此算法的Python代码</a>，写的很规范，完全是根据paper的流程写的，非常清晰。</p>
]]></content>
      <categories>
        <category>Algorithm Library</category>
        <category>Clustering Algorithm</category>
      </categories>
      <tags>
        <tag>Clustering Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>AAAI 21 paperlist</title>
    <url>/2021/01/03/AAAI%202021%20papers/</url>
    <content><![CDATA[<h2 id="AAAI-2021-papers"><a href="#AAAI-2021-papers" class="headerlink" title="AAAI 2021 papers"></a>AAAI 2021 papers<span id="more"></span></h2><h3 id="Dynamic-systems"><a href="#Dynamic-systems" class="headerlink" title="Dynamic systems"></a>Dynamic systems</h3><ul>
<li><a href="https://arxiv.org/abs/2012.08863">On the Verification of Neural ODEs with Stochastic Guarantees</a></li>
<li>Forecasting Reservoir Inflow via Recurrent Neural ODEs</li>
<li>The LOB Recreation Model: Predicting the Limit Order Book from TAQ History Using an Ordinary Differential Equation Recurrent Neural Network</li>
<li>ECG ODE-GAN: Learning Ordinary Differential Equations of ECG Dynamics via Generative Adversarial Learning</li>
<li><p>Vid-ODE: Continuous-Time Video Generation with Neural Ordinary Differential Equation</p>
</li>
<li><p>A Hybrid Stochastic Gradient Hamiltonian Monte Carlo Method</p>
</li>
</ul>
<h3 id="Generative-models"><a href="#Generative-models" class="headerlink" title="Generative models"></a>Generative models</h3><ul>
<li>Flow-Based Generative Models for Learning Manifold to Manifold Mappings</li>
<li>OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport</li>
<li>MolGrow: A Graph Normalizing Flow for Hierarchical Molecular Generation</li>
<li>Accelerating Continuous Normalizing Flow with Trajectory Polynomial Regularization</li>
</ul>
<h3 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h3><ul>
<li>Deep Switching Auto-Regressive Factorization: Application to Time Series Forecasting</li>
<li>Dynamic Gaussian Mixture Based Deep Generative Model for Robust Forecasting on Sparse Multivariate Time Series</li>
<li>Second Order Techniques for Learning Time-Series with Structural Breaks</li>
<li>Correlative Channel-Aware Fusion for Multi-View Time Series Classification</li>
<li>Learnable Dynamic Temporal Pooling for Time Series Classification</li>
<li>Learning Representations for Incomplete Time Series Clustering</li>
<li>Temporal Latent Autoencoder: A Method for Probabilistic Multivariate Time Series Forecasting</li>
<li>Continuous-Time Attention for Sequential Learning</li>
<li>Graph Neural Network-Based Anomaly Detection in Multivariate Time Series</li>
<li>ShapeNet: A Shapelet-Neural Network Approach for Multivariate Time Series Classification</li>
<li>Time Series Anomaly Detection with Multiresolution Ensemble Decoding</li>
<li>Joint-Label Learning by Dual Augmentation for Time Series Classification</li>
<li>Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</li>
<li>Generative Semi-Supervised Learning for Multivariate Time Series Imputation</li>
<li>Outlier Impact Characterization for Time Series Data</li>
<li>Meta-Learning Framework with Applications to Zero-Shot Time-Series Forecasting</li>
</ul>
<h3 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h3><ul>
<li>Contrastive and Generative Graph Convolutional Networks for Graph-Based SemiSupervised Learning</li>
<li>Overcoming Catastrophic Forgetting in Graph Neural Networks</li>
<li>Isolation Graph Kernel</li>
<li>Semi-Supervised Node Classification on Graphs: Markov Random Fields vs. Graph Neural Networks</li>
<li>Hyperbolic Variational Graph Neural Network for Modeling Dynamic Graphs</li>
<li>MolGrow: A Graph Normalizing Flow for Hierarchical Molecular Generation</li>
<li>Heterogeneous Graph Structure Learning for Graph Neural Networks</li>
<li>Graph Neural Network-Based Anomaly Detection in Multivariate Time Series</li>
<li>Deep Graph Spectral Evolution Networks for Graph Topological Evolution</li>
<li>Scalable Graph Networks for Particle Simulations</li>
<li>Synchronous Dynamical Systems on Directed Acyclic Graphs: Complexity and Algorithms</li>
<li>Fitting the Search Space of Weight-Sharing NAS with Graph Convolutional Networks</li>
<li>Contrastive Self-Supervised Learning for Graph Classification</li>
<li>Computationally Tractable Riemannian Manifolds for Graph Embeddings\</li>
<li>Graph Neural Networks with Heterophily</li>
<li>Learning Graph Neural Networks with Approximate Gradient Descent</li>
<li>Probabilistic Dependency Graphs</li>
<li>Scalable and Explainable 1-Bit Matrix Completion via Graph Signal Learning</li>
<li>GraphMix: Improved Training of GNNs for Semi-Supervised Learning</li>
<li>Power up! Robust Graph Convolutional Network via Graph Powering</li>
<li>Identity-Aware Graph Neural Networks</li>
<li>Beyond Low-Frequency Information in Graph Convolutional Networks</li>
<li>Rethinking Graph Regularization for Graph Neural Networks</li>
</ul>
<h3 id="Optimiser"><a href="#Optimiser" class="headerlink" title="Optimiser"></a>Optimiser</h3><p>ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning</p>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>A Novel Task-Duplication Based Clustering Algorithm for Heterogeneous Computing Environments</title>
    <url>/2018/12/17/A-Novel-Task-Duplication-Based-Clustering-Algorithm-for-Heterogeneous-Computing-Environments/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><ul>
<li><p>DAG调度属于静态调度问题，现有算法可以分为三大类：</p>
<ul>
<li><p>list scheduling:主要思想是计算出每个任务的优先级，然后按照优先级来进行处理。K.Shin定义了三种类型的任务优先级：</p>
<p>(1)S-Level:按照从当前结点到最后结点的最长路径(不考虑传输代价)</p>
<p>(2)B-Level:按照从当前结点到最后结点的最长路径(考虑传输代价)</p>
<p>(3)T-Level:从开始结点到当前结点的最长路径(考虑传输代价)</p>
</li>
<li><p>cluster-based scheduling:首先进行任务聚类，然后对类分配到不同的处理器上，然后可能对类进一步合并</p>
</li>
<li><p>task duplication-based scheduling：思想就是可以减少传输代价，充分利用处理器空闲的时间</p>
</li>
</ul>
</li>
</ul>
<h3 id="以往算法的问题及本文主要贡献"><a href="#以往算法的问题及本文主要贡献" class="headerlink" title="以往算法的问题及本文主要贡献"></a>以往算法的问题及本文主要贡献</h3><ul>
<li>以往算法有以下问题：<ul>
<li>(1)非常依赖一些关键的参数，比如最早完成时间，但是提前计算得到的这些参数可能并不准确，会导致得到局部最优解</li>
<li>(2)在现有的算法下，一旦task duplication是无效的就会弃用，但是实际上有可能当其他复制的task被执行之后这个任务复制又会变得有用了</li>
</ul>
</li>
<li>本文贡献：(Task Duplication based Clustering Algorithm)<ul>
<li>(1)New definition of <strong>Key Parameters</strong>:(文章中说和baseline algorithm TANH比较，比如定义est的时候，如果前后两个任务在一个处理器上，就不用考虑传输代价，会比较准确，不过这不是应该的么，这也算创新？？)</li>
<li>(2)Improving the initial clustering:现有的算法都是将一个父节点的任务进行复制，而TDCA也会考虑等待其他类中的父节点将结果传输过来</li>
<li>(3)Considering of the chain reaction：进行任务复制的时候看链式反应现象，有可能现在没用，但是过一段时间就有用了</li>
</ul>
</li>
</ul>
<h2 id="问题及基本符号定义"><a href="#问题及基本符号定义" class="headerlink" title="问题及基本符号定义"></a>问题及基本符号定义</h2><h3 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h3><ul>
<li>DAG=<V,E,P,T,C>. V:任务节点的集合; n:任务的数目; E:边的集合(依赖关系); e:边的数量; P是处理器的集合，共m种; T是$n\times m$的矩阵，T(i,p)代表第i个任务在第p种处理器上进行的代价; C是$n\times n$的矩阵，代表了任务之间的传输代价(communication costs)</V,E,P,T,C></li>
<li>PRED(i):代表i的父节点的集合; SUCC(i)代表i的子节点的集合;</li>
</ul>
<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><ul>
<li>基于TANH算法进行改进的，所以先对TANH算法进行简介，然后则是本文提出的TDCA算法</li>
</ul>
<h3 id="TANH算法"><a href="#TANH算法" class="headerlink" title="TANH算法"></a>TANH算法</h3><ul>
<li><p>Parameter calculation:</p>
<p>The processor is defined as <strong>favorite</strong> if using that for execution of task j(j $\in$ V) results in a minimum completion time for the task j.  </p>
<ul>
<li><em>est(i)</em>:最早开始时间</li>
<li><em>ect(i)</em>:最早完成时间</li>
<li>favorite processors: 取值范围为<em>fproc(i,1)</em>到<em>fproc(i,m)</em></li>
<li>favorite predecessor: fpred(i)</li>
<li><em>level(i)</em>: B-level</li>
<li><em>last(i)</em>: latest allowable starting time</li>
<li><em>lact(i)</em>: latest allowable completion time</li>
</ul>
</li>
<li><p>Initial Cluster Generation:首先按照<em>level</em>值进行不减的排序，而后每次选择未被安排的任务，并加入到新的cluster中，然后迭代进行如下的过程：</p>
<p>每次添加i的一个父节点(一般是fproc(i)),直到每个节点都被加入到类中。整个类被安排到一个不被占用的处理器上，如果所有处理器都被占用，则将其安排到一个虚拟的处理器上，并且一个任务的执行时间定为此任务的平均运行时间</p>
</li>
<li><p>Task duplication or clustering merging:如果存在类被安排到虚拟结点，则此步要进行cluster的合并，思想是将任务计算量大的和任务计算量小的合并在一起，从而使得被合并的类的总计算量只有少量的增长。如果没有类被安排到虚拟节点，则尝试进行任务复制，比如任务集合$\{x_1,x_2,…,x_k\}$, TANH可以把任务$x_1$到$x_{i-1}$复制到另一个处理器q，q是从$fproc(x_{i-1},1)$到$fproc(x_{i-1},m)$中第一个不被占用的处理器，而后把$fpred(x_i),fpred(fpred(x_i)),…$，依次加入</p>
</li>
</ul>
<h3 id="TDCA算法"><a href="#TDCA算法" class="headerlink" title="TDCA算法"></a>TDCA算法</h3><ul>
<li>分为四个阶段：聚类初始化，任务复制来减少完成时间，类别的合并来得到一个初步的调度，增加阶段用于利用处理器的空闲时间。</li>
</ul>
<h4 id="符号定义-1"><a href="#符号定义-1" class="headerlink" title="符号定义"></a>符号定义</h4><ul>
<li><em>Communication Cost</em>:<script type="math/tex; mode=display">
\begin{equation}
\delta(j,i,q,p) = \left\{
    \begin{array}{**rc1**}
  0&q=p\\
  C(j,i)& q\neq p
    \end{array}
\right.<j,i> \in E
\end{equation}</script></li>
</ul>
<ul>
<li><em>Earliest Starting Time</em>:<script type="math/tex; mode=display">
est(i,p) = \max_{j\in PRED(i)}\{\min_{q\in P}\{ect(j,q)+\delta(j,i,q,p)\}\}</script></li>
</ul>
<ul>
<li><p><em>Earlist Completion Time</em>:</p>
<script type="math/tex; mode=display">
ect(i,p) = est(i,p) + T(i,p)</script></li>
<li><p><em>The r<strong>th</strong> Favorite Processor</em>:</p>
<script type="math/tex; mode=display">
ect(i,fproc(i,1))\leq ... \leq ect(i,fproc(i,r))</script></li>
</ul>
<ul>
<li><em>Critical Predecessor</em>:假设所有任务都分配给最喜欢的处理器，cpred（i）是将其结果发送到任务i的最后一个前任。<script type="math/tex; mode=display">
cpred(i) = argmax_{j\in PRED(i)}\{ect(j,fproc(j,1)) + \delta(j,i,fproc(j,1),fproc(i,1))\}</script></li>
</ul>
<ul>
<li><p><em>Task Priority</em>:(采用B-Level)</p>
<script type="math/tex; mode=display">
level(i) = max_{k\in SUCC(i)}\{level(k)+C(i,k)\}+\max_{q\in P}(T(i,q))</script></li>
<li><p><em>Critical Predecessor Trail</em>:The critical predecessor trail of a task i is defined as cpred(i), cpred(cpred(i)),…, up to the entry-node.</p>
</li>
</ul>
<h4 id="任务初始聚类"><a href="#任务初始聚类" class="headerlink" title="任务初始聚类"></a>任务初始聚类</h4><ul>
<li><p>在TANH中，如果i有多个父节点，而$j = fpred(i)$还没有被安排，那么直接将其安排到i的处理器p上，但是TDCA还要考虑是否满足下式，不满足则说明可以直接将j安排到favorite processor上，然后等从j给i传输信息</p>
<script type="math/tex; mode=display">
ect(j,p)\leq ect(j,fproc(j,1)) + C(j,i)</script></li>
<li><p>如果j = cpred(i)和i不属于一个类，当上述不等式不成立时，选择未被安排的父节点k使得上式成立，如果这样的结点有很多个，我们选择最小的ect(k,p)</p>
</li>
<li><p>迭代停止条件:(1)到达开始的结点；(2)如果j=cpred(i)没有被同一个处理器安排，并且找不到未被安排的父节点满足上述不等式，那么就停止迭代</p>
</li>
</ul>
<p>两点评论：</p>
<p>(1)由于ect只是估计并不准确，并且有可能fproc(j,1)已经被占用，因此上述不等式不能正确的知道j是否对于i有益</p>
<p>(2)如果任务数量n远大于处理器的数量m，那么会没有不被占用的处理器，此时停止任务聚类，然后根据当前的安排，将未被安排的任务安排到一个可以最小化开始时间的处理器上面</p>
<p><img src="/2018/12/17/A-Novel-Task-Duplication-Based-Clustering-Algorithm-for-Heterogeneous-Computing-Environments/1.png" alt="1"></p>
<h4 id="Task-Duplication"><a href="#Task-Duplication" class="headerlink" title="Task Duplication"></a>Task Duplication</h4><p>假设$X_p = (x_1,x_2,…,x_k)$表示安排到处理器p上的任务集合，那么现在有两种情况需要处理:(1)先进行检查看对于$i\in [2,k]$,是否存在$cpred(x_i)\neq x_{i-1}$,如果存在则说明前一个结点不是$x_i$的父节点的最晚结点，故而中间需要等待，即中间存在空闲时间，可以用来执行复制的任务;(2)检查$x_1$是否是开始结点，如果不是，则对$x_1$的父结点的踪迹(这个点是啥意思？？)不断执行任务复制</p>
<p><img src="/2018/12/17/A-Novel-Task-Duplication-Based-Clustering-Algorithm-for-Heterogeneous-Computing-Environments/2.png" alt="2"></p>
]]></content>
      <categories>
        <category>Cloud Workflow Scheduling</category>
      </categories>
      <tags>
        <tag>Cloud Workflow Scheduling</tag>
      </tags>
  </entry>
  <entry>
    <title>Basic Conceptions</title>
    <url>/2018/11/13/Basic-Conceptions/</url>
    <content><![CDATA[<h2 id="Basic-conception"><a href="#Basic-conception" class="headerlink" title="Basic conception"></a>Basic conception</h2><ul>
<li><p><strong>特征向量(feature vector)</strong>: 每一个分量对应一种<strong>属性</strong>(attribute)/<strong>特征</strong>(feature)的值，构成的空间为<strong>属性空间</strong></p>
</li>
<li><p>一般地，令$D = \{x_1,x_2,…x_m\}$表示包含m个示例的<strong>数据集</strong>，每个示例由d个属性描述，则每个示例$x_i = (x_{i1};x_{i2};…;x_{id})$是d维样本空间$\chi$中的一个向量，$x_i\in \chi$,d为样本$x_i$的<strong>维数</strong></p>
</li>
<li><p>训练样本的结果称为<strong>标记(label)</strong>；拥有了标记信息的示例，称为<strong>样例(example)</strong>,一般用$(x_i,y_i)$表示</p>
</li>
<li><p>学习任务大致可分为两类：<strong>监督学习(supervised learning)</strong>和<strong>无监督学习(unsupervised learning)</strong>，分类(classification)和回归(regression)是前者的代表，而聚类则是后者的代表。</p>
</li>
<li><p><strong>泛化(generalization)能力</strong>:所学的模型适用于新样本的能力；</p>
</li>
<li><p>通常假设样本空间全体样本服从一个未知的<strong>分布(distribution)</strong>,获得的每个样本都是<strong>独立同分布的(independent and identically distributed),i.i.d</strong></p>
</li>
<li><p><strong>归纳(induction)</strong>:从特殊到一般的泛化过程；<strong>演绎(deduction)</strong>:从一般到特化.</p>
</li>
<li><p><strong>演绎空间</strong>：样本x每一个属性的可能取值所构成的空间，其中有可能是*,代表此属性取什么都行，也有可能整个空间为空集。</p>
<p><strong>版本空间(version space)</strong>:如果假设空间中有多个与训练集一致的假设，即存在一个与训练集一致的“假设集合”</p>
</li>
<li><p><strong>归纳偏好(inductive bias)</strong>:算法在学习过程中对某种类型假设的偏好</p>
<p>“奥卡姆剃刀”原则：若有多个假设与观察一致，则选择最简单的那个。</p>
</li>
<li><p><strong>No Free Lunch Theorem</strong>: 所有算法的期望性能是相同的，<strong>前提</strong>是所有“问题”出现的机会相同、或者所有问题同等重要，但是实际情况不是这样</p>
</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>basic conception</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CloudSim Structure</title>
    <url>/2018/11/09/CloudSim-Structure/</url>
    <content><![CDATA[<h2 id="Basic-Concepts-and-Terminologies"><a href="#Basic-Concepts-and-Terminologies" class="headerlink" title="Basic Concepts and Terminologies"></a>Basic Concepts and Terminologies</h2><h3 id="Layered-Design"><a href="#Layered-Design" class="headerlink" title="Layered Design"></a>Layered Design</h3><p><img src="/2018/11/09/CloudSim-Structure/StructureofCloudPlatform.png" alt="StructureofCloudPlatform"></p>
<ul>
<li><p><strong>User-Level Middleware</strong>: providing PaaS capabilities</p>
<p>(1) Includes the software frameworks such as Web 2.0 Interfaces </p>
<p>(2) provides the programming environments and composition tools that ease the creation, deployment, and execution of applications in Clouds </p>
</li>
<li><p><strong>Core Middleware</strong>:</p>
<p>(1) Implements the platform level services that provide runtime environment enabling Cloud computing capabilities to application services built using User-Level Middlewares </p>
<p>(2) Core services at this layer includes Dynamic SLA(Service Level-Agreement) Management, Accounting, Billing, Execution monitoring and management, and Pricing.  </p>
</li>
<li><p><strong>System Level</strong>:</p>
<p>(1) Exist massive physical resources (storage servers and application servers) that power the data centers.</p>
<p>(2) Servers are transparently managed by the higher level virtualization</p>
<p>(3) VMs are isolated from each other.</p>
</li>
</ul>
<h3 id="Federation-Inter-Networking-of-Clouds"><a href="#Federation-Inter-Networking-of-Clouds" class="headerlink" title="Federation (Inter-Networking) of Clouds"></a>Federation (Inter-Networking) of Clouds</h3><ul>
<li><p>现有系统不支持用于动态协调不同数据中心之间的负载分解的机制和策略，以便确定用于托管应用服务的最佳位置以实现合理的服务满意度水平。</p>
</li>
<li><p>云服务提供商无法预测消费其服务的用户的地理分布，因此负载协调必须自动发生，并且服务的分配必须根据负载行为的变化而变化。</p>
<p><img src="/2018/11/09/CloudSim-Structure/2.png" alt="StructureofCloudPlatform"></p>
<ul>
<li><strong>Cloud coordinator</strong>的作用<ul>
<li>(1)提供云服务：包括基础设施和平台级别的</li>
<li>(2)跟踪数据中心的负载，并且承担与其他云服务商在需求高峰期的时候进行服务的动态规划的协调</li>
<li>(3)检测应用的执行和约定的SLAs(服务等级协议)被传送</li>
</ul>
</li>
<li><strong>Cloud Exchange(CEx)</strong>的作用<ul>
<li>bring together service providers and consumers</li>
<li>汇总云代理的基础需求，并根据Cloud Coordinators发布的可用的资源进行评估</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="CloudSim-Architecture"><a href="#CloudSim-Architecture" class="headerlink" title="CloudSim Architecture"></a>CloudSim Architecture</h2><h3 id="Basic-architecture"><a href="#Basic-architecture" class="headerlink" title="Basic architecture"></a>Basic architecture</h3><p><img src="/2018/11/09/CloudSim-Structure/3.png" alt="3"></p>
<ul>
<li><strong>SimJava discrete event simulation engine</strong>: <ul>
<li>Implements the core functionalities required for higher-level simulation frameworks, creation of system components.(services, host, data center, broker, virtual machines), communication between components and management of the simulation clock.</li>
</ul>
</li>
<li><strong>GridSim</strong><ul>
<li>用于建模多个网格基础设施的高级软件组件，包括网络和相关的流量配置文件;</li>
<li>基础网格组件，比如 resources, data sets, workflow traces, and information services.</li>
</ul>
</li>
<li><strong>CloudSim</strong><ul>
<li>拓展了Gridsim的公开的核心函数</li>
<li>赋予虚拟云端的数据中心环境的模型和仿真提供了很好的支持，比如，虚拟机，内存和带宽的专用管理界面</li>
<li>可以实例化和透明管理数千个系统组成的云计算基础系统架构</li>
<li>可以配置VMs，管理应用的运行，并且可以动态监视</li>
<li>云服务商可以研究：不同分配hosts方法的效益等</li>
</ul>
</li>
<li><strong>User Code</strong><ul>
<li>exposes configuration related functionalities for <strong>hosts</strong>(number of machines, their specifications and so on), <strong>applications</strong>(number of tasks and their requirements), <strong>VMs</strong>, number of <strong>users</strong> and their <strong>application types</strong> and <strong>broker</strong> scheduling policies.</li>
</ul>
</li>
</ul>
<h3 id="Modeling-the-clouds"><a href="#Modeling-the-clouds" class="headerlink" title="Modeling the clouds"></a>Modeling the clouds</h3><ul>
<li>与云相关的核心硬件基础架构服务由<strong>数据中心组件</strong>在模拟器中建模，用于处理服务请求，请求是在VM中沙箱化的应用程序元素，需要在Datacenter的主机组件上分配一部分处理能力。 </li>
<li><strong>NOTE</strong>:VM的处理包括VM整个生命周期的一些列操作：为VM配置一个主机，VM的创建，VM的销毁和VM的移植。</li>
<li>Datacenter是由一系列的的主机(host)构成，主机可以管理VMs，主机在云中代表一个物理计算节点。 Host提供了支持单核和多核节点的建模与仿真的接口</li>
<li>分配VMs到Hosts是<strong>虚拟机配置</strong>部分的功能： 这一部分有很多方法，从用户角度的，从系统角度的，默认的是FCFS(First Come First Service)  </li>
<li>The allocation of cores to VMs is done based on a host allocation. Each host components instantiates a VM scheduler component that implements the space-shared or time shared policies for allocating cores to VMs. </li>
</ul>
<h3 id="Modeling-the-VM-allocation"><a href="#Modeling-the-VM-allocation" class="headerlink" title="Modeling the VM allocation"></a>Modeling the VM allocation</h3><ul>
<li><p>云计算与网格计算最大的一个不同之处就是采用了大量的虚拟化技术和工具。</p>
</li>
<li><p>需要考虑的因素就是：避免创建需要比主机中可用处理能力更强的处理能力的VM</p>
</li>
<li><p>CloudSim支持VM调度两种层级的操作，一种是host level(主机层)，确定给每个VM分配多少计算能力；一种是VM level(虚拟机层)，给存储在他的execution engine的特定的task分配可用处理资源。每一个层级都包含time-shared and space-shared resource allocation policies.</p>
<p><img src="/2018/11/09/CloudSim-Structure/4.png" alt="4"></p>
<p><strong>note</strong>:</p>
<ul>
<li><p><strong>space-shared for VMs</strong>: only one VM can run at a given instance of time</p>
<p><strong>time-shared for VMs</strong>: the core is shared, the amount of processing power available to the VM is lesser than space-shared</p>
</li>
<li><p><strong>space-shared for tasks</strong>: a VM has two cores, so it sun two at a time but the other two are queued until the completion of the earlier task units</p>
<p><strong>time-shared for tasks</strong>: enable the task to be scheduled at an earlier time, but significantly affecting the completion time of task units that are ahead the queue</p>
</li>
</ul>
</li>
</ul>
<h3 id="Modeling-the-Cloud-Market"><a href="#Modeling-the-Cloud-Market" class="headerlink" title="Modeling the Cloud Market"></a>Modeling the Cloud Market</h3><ul>
<li><strong>Modeling the cost and pricing policies</strong>,four market-related properties are associated to a data center: cost per processing,cost per unit of memory,cost per unit of storage,cost per unit of used bandwidth.</li>
</ul>
<h3 id="Design-and-implementation-of-CloudSim"><a href="#Design-and-implementation-of-CloudSim" class="headerlink" title="Design and implementation of CloudSim"></a>Design and implementation of CloudSim</h3><ul>
<li><strong>DataCenter</strong>: <ul>
<li>Core infrastructure level services(hardware, software) offered by resource providers in a Cloud Computing environment. </li>
<li>Instantiates a generalized resource provisioning component that implements a set of policies for allocating bandwidth, memory and storage devices.</li>
</ul>
</li>
<li><strong>DataCenterBroker</strong>:<ul>
<li>Responsible for mediating between users and service providers depending on users’ QoS requirements and deploys service tasks across Clouds.</li>
</ul>
</li>
<li><strong>SANStorage</strong><ul>
<li>Models a storage area network that is commonly available to Cloud-based data centers for storing large chunks of data</li>
<li>提供接口可在任何时间对根据网络带宽的可用性对任意大的数据量进行存储和检索，但是在运行访问SAN中的文件会导致任务单元执行的额外延迟。</li>
</ul>
</li>
<li><strong>Virtual Machine</strong><ul>
<li>Model an instance of Vm. Hosr部分负责VM的管理，可以同时实例化很多VMs，并且根据前述的政策(空间/时间分享)运行</li>
<li>每个VM中有存储其基本特征的组成部分：memory, processor, storage, VM’s internal scheduling policy.</li>
</ul>
</li>
<li><strong>Cloudlet</strong><ul>
<li>Models the Cloud-based application services(content delivery（内容交付）, social networking, business workflow)</li>
<li>每个应用程序组件都具有预先指定的指令长度（继承自GridSim的Gridlet组件）以及成功托管应用程序所需的数据传输量（提前和提取后）。</li>
</ul>
</li>
<li><strong>CloudCoordinator</strong><ul>
<li>provides federation capacity to a data center.</li>
<li>(1)communicating with other peer CloudCoordinator services and Cloud Brokers</li>
<li>(2)monitor the internal state of a data center that plays integral role in load-balancing/application scaling decision making.(触发负载迁移的事件是有CloudSim用户通过Sensor组件来设置的)</li>
</ul>
</li>
<li><strong>BWProvisioner</strong><ul>
<li>Models the provisioning policy of bandwidth to VMs.</li>
</ul>
</li>
<li><strong>MemoryProvisioner</strong><ul>
<li>Represents the provisioning policy for allocating memory to VMs.</li>
</ul>
</li>
<li><strong>VMProvisioner</strong><ul>
<li>Represents the provisioning policy that a VM Monitor utilizes for allocating VMs to Hosts.</li>
</ul>
</li>
<li><strong>VMMAllocationPolicy</strong><ul>
<li>Implemented by a Host component that models the policies (time-shared, space shared) required for allocating processing power to VMs.</li>
</ul>
</li>
</ul>
<p><img src="/2018/11/09/CloudSim-Structure/12.png" alt="12"></p>
<h3 id="Communication-among-entities"><a href="#Communication-among-entities" class="headerlink" title="Communication among entities"></a>Communication among entities</h3><p><img src="/2018/11/09/CloudSim-Structure/11.png" alt="11"></p>
]]></content>
      <categories>
        <category>Cloud Workflow Scheduling</category>
      </categories>
      <tags>
        <tag>CloudSim toolkit introduction</tag>
      </tags>
  </entry>
  <entry>
    <title>DMOEA-epsilonC</title>
    <url>/2018/11/27/DMOEA-epsilonC/</url>
    <content><![CDATA[<h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ul>
<li>首次将$\epsilon$约束法和分解的方法合并起来，</li>
<li>在优化的过程中，使用了主函数变化的策略，</li>
<li>由于不同问题的计算复杂度不同，则需要对计算资源进行动态分配</li>
<li>并且使用了解和子问题的匹配，子问题和解的匹配等方法来平衡解的多样性和收敛性之间的关系</li>
</ul>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="多目标进化算法-MOEA"><a href="#多目标进化算法-MOEA" class="headerlink" title="多目标进化算法(MOEA)"></a>多目标进化算法(MOEA)</h3><ul>
<li>分类:基于他们的选择策略，我们可以将他们分为三类:<ul>
<li>(1)基于Parero支配</li>
<li>(2)基于performance indicator</li>
<li>(3)基于分解策略的</li>
</ul>
</li>
<li>一般处理多目标问题的两个基本方法: 权重法和$\epsilon$约束法</li>
<li><strong>$\epsilon$约束法</strong>：选择一个目标作为主函数，其他的目标作为约束来处理，并且为每一个约束来添加一个<strong>上界</strong></li>
</ul>
<h2 id="基本框架介绍"><a href="#基本框架介绍" class="headerlink" title="基本框架介绍"></a>基本框架介绍</h2><h3 id="Problem-Formulation-under-epsilon-constraint"><a href="#Problem-Formulation-under-epsilon-constraint" class="headerlink" title="Problem Formulation under $\epsilon$ constraint"></a>Problem Formulation under $\epsilon$ constraint</h3><p><img src="/2018/11/27/DMOEA-epsilonC/1.png" alt="1"></p>
<p>那么Fig1中多目标优问题可以转化为:</p>
<script type="math/tex; mode=display">
minimize~~~f_{main} = f_s(x) +\rho \sum^{m}_{i=1}f_i(x)\\
subject~~~to~~ 
\begin{equation}
\left\{
\begin{array}{**rc1**}
\frac{f_i(x)-z_i}{z^{nad}_{i}-z_{i}}\leq \epsilon_i, \forall i \in \{i,1,...,m\}/\{s\}\\
x = (x_1,x_2,...,x_n)\in \Omega
\end{array}
\right.
\end{equation}\\
其中， 0\leq \epsilon = (\epsilon_1,...,\epsilon_{s-1},\epsilon_{s+1},...,\epsilon_{m})\leq 1\\
s是从m中任意选的，\rho >0 是一个很小的正数\\
z = (z_1,...,z_m)和z^{nad} = (z^{nad}_1,...,z^{nad}_m)分别是理想点和最低点</script><ul>
<li><strong>理想点(ideal point $z\in R^m$)</strong>：是通过独立地最小化每一个目标函数得到的，即满足<script type="math/tex; mode=display">
minimize~~f_i(x)\\
subject~to~x\in \Omega~~~for~i=1,...,m.</script></li>
</ul>
<ul>
<li><p><strong>最低点(nadir point $z^{nad}$)</strong>:最低点是PF(Pareto Front)的上界，$z^{nad}$的每个元素定义为</p>
<script type="math/tex; mode=display">
z^{nad}_{i} = max\{f_i|F = (f_1,f_2,...,f_m)\in PF\}\\
where~ i=1,2,...,m</script><p><strong>算法框架</strong></p>
<p><img src="/2018/11/27/DMOEA-epsilonC/2.png" alt="2"></p>
</li>
</ul>
<ul>
<li><p>对两个解进行比较的基本规则：</p>
<ul>
<li>(1)可行解优于不可行解</li>
<li>(2)对于两个可行解，对应目标函数值更好的更优</li>
<li>(3)对于两不可行解，选择对约束破坏更小的解</li>
</ul>
</li>
<li><p>基本参数介绍：</p>
<ul>
<li><strong>N</strong>: 上界向量的个数，和种群大小相同</li>
<li><strong>T</strong>: 邻居的个数<strong>neighborhood size</strong></li>
<li><strong>$\delta$</strong>：选择 mate solutions的概率</li>
<li><strong>$n_r$</strong>: 替换的最大数量(Maximum number of replacement)</li>
<li><strong>$IN_m$</strong>: 在迭代中采用更换主函数策略的代数间隔</li>
<li><strong>$DRA_interval$</strong>：在迭代中采用动态资源分配策略的代数间隔</li>
<li><strong>S</strong>: 外部存档种群（EP）的最大规模</li>
<li><strong>NFE</strong>: 函数评价值的最大值</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/2018/11/27/DMOEA-epsilonC/3.png" alt="3"></p>
<ul>
<li>After the main objective alternation strategy is utilized, a solution which is good for the current subproblem will no longer perform well since the objective function of this subproblem has been changed.  </li>
</ul>
<p><img src="/2018/11/27/DMOEA-epsilonC/4.png" alt="4"></p>
<ul>
<li>Different subproblems have different computational difficulties, therefore, it is reasonable to assign different amounts of computational effort to them based on their utility values which are defined in the line 3 of Algorithm 3 </li>
</ul>
<p><img src="/2018/11/27/DMOEA-epsilonC/5.png" alt="5"></p>
<ul>
<li>When a new solution is generated, it may perform badly for the current subproblem but perform well for another subproblem. </li>
</ul>
<p><img src="/2018/11/27/DMOEA-epsilonC/6.png" alt="6"></p>
<ul>
<li>An EP is maintained in addition to the evolving population. Thus when a new solution is generated, the EP should be updated. And if the number of individuals in EP exceeds S, EP is pruned until its size equals to S. </li>
<li>首先选择bounded points(有最小或最大目标值的解)，然后从剩余解中迭代选择距离已选择点最远的点</li>
</ul>
]]></content>
      <categories>
        <category>Cloud Workflow Scheduling</category>
      </categories>
      <tags>
        <tag>Cloud Workflow Scheduling</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamic GNN PaperList</title>
    <url>/2020/09/18/Dynamic-GNN-PaperList/</url>
    <content><![CDATA[<p>AAAI 2018:</p>
<ul>
<li>Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution</li>
<li>Dynamic Network Embedding by Modeling Triadic Closure Process</li>
<li>DepthLGP: Learning Embeddings of Out-of-Sample Nodes in Dynamic Networks</li>
</ul>
<p>AAAI2019：</p>
<ul>
<li>A Generative Model for Dynamic Networks with Applications</li>
<li>Communication-optimal distributed dynamic graph clustering</li>
</ul>
<p>AAAI2020：</p>
<ul>
<li>EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</li>
<li>Dynamic Network Pruning with Interpretable Layerwise Channel Selection</li>
</ul>
<p>ICLR2019：</p>
<ul>
<li>DyRep: Learning Representations over Dynamic Graphs </li>
<li>Dynamic Graph Representation Learning via Self-Attention Networks</li>
</ul>
<p>ICLR2020：</p>
<ul>
<li>The Logical Expressiveness of Graph Neural Networks  </li>
</ul>
<p>WWW2018：</p>
<ul>
<li>Fast and Accurate Random Walk with Restart on Dynamic Graphs with Guarantees</li>
</ul>
<p>IJCAI2018:</p>
<ul>
<li>Dynamic Network Embedding : An Extended Approach for Skip-gram based Network Embedding</li>
<li>14 Deep into Hypersphere: Robust and Unsupervised Anomaly Discovery in Dynamic Networks</li>
</ul>
<p>IJCAI2019：</p>
<ul>
<li>AddGraph: Anomaly Detection in Dynamic Graph using Attention-based Temporal GCN</li>
</ul>
<p>SIGIR2019：</p>
<ul>
<li>Network Embedding and Change Modeling in Dynamic Heterogeneous Networks</li>
</ul>
<p>SIGIR2020:</p>
<ul>
<li>Learning Dynamic Node Representations with Graph Neural Networks</li>
<li>Dynamic Link Prediction by Integrating Node Vector Evolution and Local Neighborhood Representation</li>
</ul>
<p>KDD2018:</p>
<ul>
<li>NetWalk: A Flexible Deep Embedding Approach for Anomaly Detection in Dynamic Networks</li>
</ul>
<p>KDD2019:</p>
<ul>
<li>Fast and Accurate Anomaly Detection in Dynamic Graphs with a Two-Pronged Approach</li>
<li>Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks</li>
</ul>
<p>KDD2020:</p>
<ul>
<li>Laplacian Change Point Detection for Dynamic Graphs</li>
<li>Dynamic Heterogeneous Graph Neural Network for Real-time Event Prediction</li>
<li>Neural Dynamics on Complex Networks</li>
</ul>
<p>ICML2018：</p>
<ul>
<li>Fast Approximate Spectral Clustering for Dynamic Networks</li>
</ul>
<p>ICML2019：</p>
<ul>
<li>Improved Dynamic Graph Learning through Fault-Tolerant Sparsification</li>
</ul>
<p>NeurIPS 2019:</p>
<ul>
<li>Variational Graph Recurrent Neural Network</li>
</ul>
<p>ICDE2018：</p>
<ul>
<li>Efficient SimRank Tracking in Dynamic Graphs.</li>
<li>On Efficiently Detecting Overlapping Communities over Distributed Dynamic Graphs</li>
</ul>
<p>ICDE2019：</p>
<ul>
<li>Computing a Near-Maximum Independent Set in Dynamic Graphs</li>
<li>Finding Densest Lasting Subgraphs in Dynamic Graphs: A Stochastic Approach</li>
<li>Tracking Influential Nodes in Time-Decaying Dynamic Interaction Networks</li>
<li>Adaptive Dynamic Bipartite Graph Matching: A Reinforcement Learning Approach</li>
<li>A Fast Sketch Method for Mining User Similarities Over Fully Dynamic Graph Streams</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Generative Models</title>
    <url>/2022/06/06/Deep-Generative-Models/</url>
    <content><![CDATA[<h2 id="Deep-Generative-Models-VAE-Flow-Diffusion-GAN"><a href="#Deep-Generative-Models-VAE-Flow-Diffusion-GAN" class="headerlink" title="Deep Generative Models[VAE+Flow+Diffusion+GAN]"></a>Deep Generative Models[VAE+Flow+Diffusion+GAN]<span id="more"></span></h2><div class="pdfobject-container" data-target="DGM.pdf" data-height="1000px"></div>]]></content>
      <categories>
        <category>machine learning</category>
        <category>Review</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamic Systems Paperlist</title>
    <url>/2020/09/12/Dynamic-Systems-Paperlist/</url>
    <content><![CDATA[<blockquote>
<p>PaperList For Dynamic systems with DNN <span id="more"></span></p>
</blockquote>
<h1 id="awesome-neural-ode"><a href="#awesome-neural-ode" class="headerlink" title="awesome-neural-ode"></a>awesome-neural-ode</h1><p>A collection of resources regarding the interplay between differential equations, dynamical systems, deep learning, control and scientific machine learning.</p>
<h2 id="Differential-Equations-in-Deep-Learning"><a href="#Differential-Equations-in-Deep-Learning" class="headerlink" title="Differential Equations in Deep Learning"></a>Differential Equations in Deep Learning</h2><h3 id="General-Architectures"><a href="#General-Architectures" class="headerlink" title="General Architectures"></a>General Architectures</h3><ul>
<li><p>Recurrent Neural Networks for Multivariate Time Series with Missing Values: <a href="https://arxiv.org/abs/1606.01865">Scientific Reports18</a></p>
</li>
<li><p>Deep Equilibrium Models: <a href="https://arxiv.org/abs/1909.01377">NeurIPS19</a></p>
</li>
<li><p>Fast and Deep Graph Neural Networks: <a href="https://arxiv.org/pdf/1911.08941.pdf">AAAI20</a></p>
</li>
</ul>
<h3 id="Neural-ODEs"><a href="#Neural-ODEs" class="headerlink" title="Neural ODEs"></a>Neural ODEs</h3><ul>
<li>Neural Ordinary Differential Equations: <a href="https://arxiv.org/pdf/1806.07366.pdf">NeurIPS18</a></li>
<li>Augmented Neural ODEs: <a href="https://arxiv.org/abs/1904.01681">NeurIPS19</a></li>
<li>Dissecting Neural ODEs: <a href="https://arxiv.org/abs/2002.08071">arXiv20</a></li>
<li>Latent ODEs for Irregularly-Sampled Time Series: <a href="https://arxiv.org/abs/1907.03907">arXiv19</a></li>
<li>Learning unknown ODE models with Gaussian processes: <a href="https://arxiv.org/abs/1803.04303">arXiv18</a></li>
<li>ODE2VAE: Deep generative second order ODEs with Bayesian neural networks: <a href="https://arxiv.org/pdf/1905.10994.pdf">NeurIPS19</a></li>
<li>Stable Neural Flows: <a href="https://arxiv.org/abs/2003.08063">arXiv20</a></li>
<li>On Second Order Behaviour in Augmented Neural ODEs <a href="https://arxiv.org/abs/2006.07220">arXiv20</a></li>
<li>Snode: Spectral discretization of neural odes for system identification <a href="https://arxiv.org/abs/1906.07038">arXiv19</a></li>
<li>Learning Differential Equations that are Easy to Solve <a href="https://arxiv.org/abs/2007.04504">NeurIPS20</a>    <a href="https://github.com/jacobjinkelly/easy-neural-ode">code</a></li>
<li>An Ode to an ODE <a href="https://arxiv.org/abs/2006.11421">arXiv20</a></li>
<li>ANODEV2: A Coupled Neural ODE Evolution Framework <a href="https://arxiv.org/abs/1906.04596">arXiv19</a></li>
</ul>
<h4 id="Speed-up-Training-of-Neural-ODEs"><a href="#Speed-up-Training-of-Neural-ODEs" class="headerlink" title="Speed up Training of Neural ODEs"></a>Speed up Training of Neural ODEs</h4><ul>
<li>Accelerating Neural ODEs with Spectral Elements: <a href="https://arxiv.org/abs/1906.07038">arXiv19</a></li>
<li>Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE: <a href="https://arxiv.org/abs/2006.02493">ICML20</a></li>
<li>“Hey, that’s not an ODE”: Faster ODE Adjoints with 12 Lines of Code <a href>arXiv20</a></li>
<li>How to Train you Neural ODE: <a href="https://arxiv.org/abs/2002.02798">arXiv20</a></li>
<li>Hypersolvers: Toward Fast Continuous-Depth Models <a href="http://proceedings.neurips.cc/paper/2020/hash/f1686b4badcf28d33ed632036c7ab0b8-Abstract.html">NeurIPS20</a></li>
</ul>
<h3 id="Neural-SDEs"><a href="#Neural-SDEs" class="headerlink" title="Neural SDEs"></a>Neural SDEs</h3><ul>
<li>Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise: <a href="https://arxiv.org/abs/1906.02355">arXiv19</a></li>
<li>Neural Jump Stochastic Differential Equations: <a href="https://arxiv.org/abs/1905.10403">arXiv19</a></li>
<li>Towards Robust and Stable Deep Learning Algorithms for Forward Backward Stochastic Differential Equations: <a href="https://arxiv.org/abs/1910.11623">arXiv19</a></li>
<li>Scalable Gradients and Variational Inference for Stochastic Differential Equations: <a href="https://arxiv.org/abs/2001.01328">AISTATS20</a></li>
</ul>
<h3 id="Neural-CDEs"><a href="#Neural-CDEs" class="headerlink" title="Neural CDEs"></a>Neural CDEs</h3><ul>
<li>Neural Controlled Differential Equations for Irregular Time Series: <a href="https://arxiv.org/abs/2005.08926">ArXiv2020</a></li>
<li>Neural CDEs for Long Time Series via the Log-ODE Method: <a href="https://arxiv.org/abs/2009.08295">ArXiv2020</a></li>
</ul>
<h3 id="Normalizing-Flows"><a href="#Normalizing-Flows" class="headerlink" title="Normalizing Flows"></a>Normalizing Flows</h3><ul>
<li><p>Monge-Ampère Flow for Generative Modeling: <a href="https://arxiv.org/pdf/1809.10188.pdf">arXiv18</a></p>
</li>
<li><p>FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models: <a href="https://arxiv.org/abs/1810.01367">ICLR19</a></p>
</li>
<li><p>Equivariant Flows: sampling configurations for multi-body systems with symmetric energies: <a href="https://arxiv.org/pdf/1910.00753.pdf">arXiv18</a></p>
</li>
</ul>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><ul>
<li>Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning: <a href="https://arxiv.org/abs/1905.11666">NeurIPS19</a></li>
<li>Graph Neural Ordinary Differential Equations  <a href="https://arxiv.org/abs/1911.07532">arXiv19</a></li>
<li>Continuous graph neural networks <a href="https://arxiv.org/abs/1912.00967">ICML2020</a></li>
<li>Neural Dynamics on Complex Networks <a href="https://arxiv.org/pdf/1908.06491.pdf">arXiv19</a></li>
</ul>
<h2 id="Energy-based-models"><a href="#Energy-based-models" class="headerlink" title="Energy based models"></a>Energy based models</h2><h3 id="Hamilton"><a href="#Hamilton" class="headerlink" title="Hamilton"></a>Hamilton</h3><ul>
<li>Hamiltonian Neural Networks: <a href="https://arxiv.org/abs/1906.01563">NeurIPS19</a>  <a href="https://github.com/greydanus/hamiltonian-nn">code</a></li>
<li>Hamiltonian generative networks<a href="https://arxiv.org/abs/1909.13789">ICLR2020</a>  <a href="https://github.com/gaspardbb/HamiltonianGenerativeNetworks">code</a></li>
<li>Sparse Symplectically Integrated Neural Networks <a href="https://arxiv.org/abs/2006.12972">NeurIPS20</a>      <a href="https://github.com/dandip/ssinn">code</a></li>
<li>Nonseparable symplectic neural networks   <a href="https://openreview.net/forum?id=B5VvQrI49Pa">code</a></li>
<li>SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems <a href="https://arxiv.org/abs/2001.03750">arxiV20</a></li>
<li>Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control: <a href="https://arxiv.org/abs/1909.12077">arXiv19</a></li>
<li>Symplectic Recurrent Neural Network <a href="https://arxiv.org/abs/1909.13334">arXiv19</a></li>
</ul>
<h4 id="Applications-1"><a href="#Applications-1" class="headerlink" title="Applications"></a>Applications</h4><ul>
<li>Hamiltonian graph networks with ode integrators <a href="https://arxiv.org/abs/1909.12790">arXiv19</a></li>
</ul>
<h3 id="Lagrange"><a href="#Lagrange" class="headerlink" title="Lagrange"></a>Lagrange</h3><ul>
<li>Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning: <a href="https://arxiv.org/abs/1907.04490">ICLR19</a></li>
<li>Unsupervised Learning of Lagrangian Dynamics from Images for Prediction and Control <a href="https://proceedings.neurips.cc/paper/2020/file/79f56e5e3e0e999b3c139f225838d41f-Supplemental.pdf">NeurIPS20</a></li>
<li>Lagrangian Neural Networks: <a href="https://arxiv.org/abs/2003.04630">ICLR20 DeepDiffEq</a></li>
</ul>
<h2 id="Deep-Learning-Methods-for-Differential-Equations"><a href="#Deep-Learning-Methods-for-Differential-Equations" class="headerlink" title="Deep Learning Methods for Differential Equations"></a>Deep Learning Methods for Differential Equations</h2><h3 id="Solving-Differential-Equations"><a href="#Solving-Differential-Equations" class="headerlink" title="Solving Differential Equations"></a>Solving Differential Equations</h3><h3 id="Learning-PDEs"><a href="#Learning-PDEs" class="headerlink" title="Learning PDEs"></a>Learning PDEs</h3><ul>
<li>PDE-Net: Learning PDEs From Data: <a href="https://arxiv.org/abs/1710.09668">ICML18</a></li>
<li>PDE-Net 2.0: Learning PDEs from Data <a href="https://arxiv.org/abs/1812.04426">Journal of Computational Physics</a></li>
<li>Solving parametric PDE problems with artificial neural networks <a href="https://arxiv.org/abs/1707.03351">arXiv</a></li>
</ul>
<h3 id="Model-Discovery"><a href="#Model-Discovery" class="headerlink" title="Model Discovery"></a>Model Discovery</h3><ul>
<li>Universal Differential Equations for Scientific Machine Learning: <a href="https://arxiv.org/abs/2001.04385">arXiv20</a></li>
</ul>
<h2 id="Deep-Control"><a href="#Deep-Control" class="headerlink" title="Deep Control"></a>Deep Control</h2><h3 id="Model-Predictive-Control"><a href="#Model-Predictive-Control" class="headerlink" title="Model-Predictive-Control"></a>Model-Predictive-Control</h3><ul>
<li>Differentiable MPC for End-to-end Planning and Control: <a href="https://arxiv.org/abs/1810.13400">NeurIPS18</a></li>
<li>Neural lyapunov model predictive control <a href="https://arxiv.org/abs/2002.10451">arXiv20</a></li>
</ul>
<h2 id="Dynamical-System-View-of-Deep-Learning"><a href="#Dynamical-System-View-of-Deep-Learning" class="headerlink" title="Dynamical System View of Deep Learning"></a>Dynamical System View of Deep Learning</h2><h3 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h3><ul>
<li><p>A Comprehensive Review of Stability Analysis of Continuous-Time Recurrent Neural Networks: <a href="https://ieeexplore.ieee.org/abstract/document/6814892">IEEE Transactions on Neural Networks 2006</a></p>
</li>
<li><p>AntysimmetricRNN: A Dynamical System View on Recurrent Neural Networks: <a href="https://openreview.net/pdf?id=ryxepo0cFX">ICLR19</a></p>
</li>
<li><p>Recurrent Neural Networks in the Eye of Differential Equations: <a href="https://arxiv.org/pdf/1904.12933.pdf">arXiv19</a></p>
</li>
<li><p>Visualizing memorization in RNNs: <a href="https://distill.pub/2019/memorization-in-rnns/">distill19</a></p>
</li>
<li><p>One step back, two steps forward: interference and learning in recurrent neural networks: <a href="https://arxiv.org/abs/1805.09603">arXiv18</a></p>
</li>
<li><p>Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics: <a href="https://arxiv.org/pdf/1906.10720.pdf">arXiv19</a></p>
</li>
<li><p>System Identification with Time-Aware Neural Sequence Models: <a href="https://arxiv.org/abs/1911.09431">AAAI20</a></p>
</li>
<li><p>Universality and Individuality in recurrent networks: <a href="https://arxiv.org/abs/1907.08549">NeurIPS19</a></p>
</li>
</ul>
<h3 id="Theory-and-Perspectives"><a href="#Theory-and-Perspectives" class="headerlink" title="Theory and Perspectives"></a>Theory and Perspectives</h3><ul>
<li>Deep information propagation [arXiv16] (<a href="https://arxiv.org/abs/1611.01232">https://arxiv.org/abs/1611.01232</a>)</li>
<li>A mean field optimal control formulation of deep learning <a href="https://link.springer.com/article/10.1007/s40687-018-0172-y">Research in Mathematical Science</a></li>
<li>A Proposal on Machine Learning via Dynamical Systems: <a href="https://link.springer.com/content/pdf/10.1007/s40304-017-0103-z.pdf">Communications in Mathematics and Statistics 2017</a></li>
<li>Deep learning as optimal control problems:models and numerical methods <a href="https://arxiv.org/abs/1904.05657">arXiv19</a></li>
<li>Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective: <a href="https://arxiv.org/abs/1908.10920">arXiv19</a></li>
<li>Stable Architectures for Deep Neural Networks: <a href="https://arxiv.org/pdf/1705.03341.pdf">IP17</a></li>
<li>Beyond Finite Layer Neural Network: Bridging Deep Architects and Numerical Differential Equations: <a href="https://arxiv.org/abs/1710.10121">ICML18</a></li>
<li>Review: Ordinary Differential Equations For Deep Learning: <a href="https://arxiv.org/abs/1911.00502">arXiv19</a></li>
</ul>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><ul>
<li><p>Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks: <a href="https://papers.nips.cc/paper/1033-gradient-and-hamiltonian-dynamics-applied-to-learning-in-neural-networks.pdf">NIPS96</a></p>
</li>
<li><p>Maximum Principle Based Algorithms for Deep Learning: <a href="https://arxiv.org/abs/1710.09513">JMLR17</a></p>
</li>
<li><p>Hamiltonian Descent Methods: <a href="https://arxiv.org/pdf/1809.05042.pdf">arXiv18</a></p>
</li>
<li><p>Port-Hamiltonian Approach to Neural Network Training: <a href="https://arxiv.org/abs/1909.02702">CDC19</a>, <a href="https://github.com/Zymrael/PortHamiltonianNN">code</a></p>
</li>
<li><p>An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks: <a href="https://arxiv.org/abs/1803.01299">arXiv19</a></p>
</li>
<li><p>Optimizing Millions of Hyperparameters by Implicit Differentiation: <a href="https://arxiv.org/abs/1911.02590">arXiv19</a></p>
</li>
<li><p>Shadowing Properties of Optimization Algorithms: <a href="https://papers.nips.cc/paper/9431-shadowing-properties-of-optimization-algorithms">NeurIPS19</a></p>
</li>
</ul>
<h2 id="Software-and-Libraries"><a href="#Software-and-Libraries" class="headerlink" title="Software and Libraries"></a>Software and Libraries</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><ul>
<li><p><strong>torchdyn</strong>: PyTorch library for all things neural differential equations. <a href="https://github.com/diffeqml/torchdyn">repo</a>, <a href="https://torchdyn.readthedocs.io/">docs</a></p>
</li>
<li><p><strong>torchdiffeq</strong>: Differentiable ODE solvers with full GPU support and O(1)-memory backpropagation: <a href="https://github.com/rtqichen/torchdiffeq">repo</a></p>
</li>
<li><p><strong>torchsde</strong>: Stochastic differential equation (SDE) solvers with GPU support and efficient sensitivity analysis: <a href="https://github.com/google-research/torchsde">repo</a></p>
</li>
<li><p><strong>torchSODE</strong>: PyTorch Block-Diagonal ODE solver: <a href="https://github.com/Zymrael/torchSODE">repo</a></p>
</li>
</ul>
<h3 id="Julia"><a href="#Julia" class="headerlink" title="Julia"></a>Julia</h3><ul>
<li><p><strong>DiffEqFlux</strong>: Neural differential equation solvers with O(1) backprop, GPUs, and stiff+non-stiff DE solvers.<br>Supports stiff and non-stiff neural ordinary differential equations (neural ODEs), neural stochastic differential<br>equations (neural SDEs), neural delay differential equations (neural DDEs), neural partial differential<br>equations (neural PDEs), and neural jump stochastic differential equations (neural jump diffusions).<br>All of these can be solved with high order methods with adaptive time-stepping and automatic stiffness<br>detection to switch between methods. <a href="https://github.com/JuliaDiffEq/DiffEqFlux.jl">repo</a></p>
</li>
<li><p><strong>NeuralNetDiffEq</strong>: Implementations of ODE, SDE, and PDE solvers via deep neural networks: <a href="https://github.com/JuliaDiffEq/NeuralNetDiffEq.jl">repo</a></p>
</li>
</ul>
<h2 id="Websites-and-Blogs"><a href="#Websites-and-Blogs" class="headerlink" title="Websites and Blogs"></a>Websites and Blogs</h2><ul>
<li>Scientific ML Blog (Chris Rackauckas and SciML): <a href="http://www.stochasticlifestyle.com/">link</a></li>
</ul>
<h2 id="Slides"><a href="#Slides" class="headerlink" title="Slides"></a>Slides</h2><ul>
<li><p><a href="https://implicit-layers-tutorial.org/implicit_tutorial.pdf">NeurIPS20 Implicit Layers</a></p>
</li>
<li><p><a href="https://web.stanford.edu/~yplu/ODETalk.pdf">ODE Talk</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamic Systems for Deep Learning</title>
    <url>/2020/09/06/Dynamic-Systems-for-Deep-Learning/</url>
    <content><![CDATA[<blockquote>
<p>本文将从动力学系统角度去理解深度神经网络。<span id="more"></span></p>
</blockquote>
<h2 id="Neural-ODE"><a href="#Neural-ODE" class="headerlink" title="Neural ODE"></a>Neural ODE</h2><ul>
<li><p>Euler method:</p>
<script type="math/tex; mode=display">
h_N = h_{N-1} + \Delta tg((N-1)\Delta t, h_{N-1})</script></li>
<li><p>ResNet</p>
<script type="math/tex; mode=display">
h_{l+1} = h_{l} + NNetwork(h_l)</script><p>不同之处在于 ： ResNet每一步都有不同的参数，而常微分方程参数是共享的</p>
</li>
<li><p>Memory Cost: O(1), 并且是可逆的</p>
</li>
<li><p>类似于ResNet的连接方式</p>
<script type="math/tex; mode=display">
h_{t+1} = h_t + f(h_t, \theta_t)\\
\frac{dh_t}{dt} = f(h(t), t, \theta)</script></li>
<li><p>定义Loss Function:</p>
<script type="math/tex; mode=display">
L(z(t_1)) = L( z(t_0) + \int_{t_0}^{t_1}f(z(t), t, \theta)dt) = L(ODESolve(z(t_0), f, t_0, t_1, \theta))</script><p>当使用Euler Method 去解决$\frac{dh(t)}{dt} = NNetwork(h(t), t, \theta)$时， 则有迭代公式为</p>
<script type="math/tex; mode=display">
h_{k} = h_{k-1} + NNetwork(h_{k-1})</script><p>那么， 损失的梯度可以表示为</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial \theta} = \frac{\partial loss(h_k,y_{targets}; \theta)}{\partial \theta}</script><p>当然，此处可以使用其他数值解法，如RK2, RK4等等，但是存在以下<strong>问题</strong>， 如果<strong>时间步数很多</strong>，则会产生内存问题；此外，由于数值错误或者不稳定性甚至是求解器本身的不可微性，会导致反向传播无法进行。</p>
</li>
<li><p>伴随敏感度方法：</p>
<ul>
<li><p>define $z(t+ \epsilon) = \int ^{t+\epsilon}_t f(z(t), t, \theta)dt + z(t) = T_\epsilon(z(t), t)$, and $a(t) = \frac{dL}{dZ(t)}$</p>
<p>we can get</p>
<script type="math/tex; mode=display">
\frac{dL}{dz(t)} = \frac{dL}{dz(t+\epsilon)} \frac{dz(t+\epsilon)}{dz(t)} \longrightarrow a(t) = a(t+ \epsilon) \frac{dT_{\epsilon}(z(t),t)}{dz(t)}</script></li>
<li><p>下面证明 $\frac{da(t)}{dt}= -a(t)^T \frac{df(z(t), t, \theta)}{dz}$</p>
<p><img src="/2020/09/06/Dynamic-Systems-for-Deep-Learning/adjoint_deduction.png" alt="adjoint_deduction" style="zoom:50%;"></p>
</li>
<li><p>因此，整体前向和反向传播网络如下图所示：</p>
<p><img src="/2020/09/06/Dynamic-Systems-for-Deep-Learning/n_ode.png" alt="neural ODE" style="zoom:50%;"></p>
</li>
<li><p>下面讨论损失函数对于$\theta$的梯度，和上述几乎同样的推导过程。</p>
</li>
</ul>
</li>
</ul>
<p>整体推导过程：</p>
<script type="math/tex; mode=display">
\frac{d}{dt}\begin{bmatrix}
z\\
\theta\\
t
\end{bmatrix}(t) = f_{aug}([z, \theta, t]):=\begin{bmatrix}
f([z,\theta, t])\\
0\\
1\\
\end{bmatrix}</script><script type="math/tex; mode=display">
a_{aug}:=\begin{bmatrix}
a\\a_{\theta}\\a_{t}
\end{bmatrix}</script><p>其中 $a_{\theta}(t):=\frac{dL}{d\theta(t)}, a_t(t):=\frac{dL}{dt(t)}$</p>
<p>f函数的雅可比矩阵为 </p>
<script type="math/tex; mode=display">
\frac{\partial f_{aug}}{\partial [z, \theta, t]}=\begin{bmatrix}
\frac{\partial f}{\partial z}& \frac{\partial f}{\partial \theta}&\frac{\partial f}{\partial t}\\
0&0&0\\
0&0&0\\
\end{bmatrix}(t)</script><p>则类似上述推导可以得出对应的伴随敏感状态为</p>
<script type="math/tex; mode=display">
\frac{d a_{aug}(t)}{dt} = -[a(t)~~ a_{\theta}(t)~~ a_t(t)] \frac{\partial f_{aug}}{\partial [z, \theta, t]}(t) = -[a\frac{\partial f}{\partial z}~~ a\frac{\partial f}{\partial \theta}~~a\frac{\partial f}{\partial z}](t)</script><h2 id="FASTER-ODE-ADJOINTS-WITH-12-LINES-OF-CODE"><a href="#FASTER-ODE-ADJOINTS-WITH-12-LINES-OF-CODE" class="headerlink" title="FASTER ODE ADJOINTS WITH 12 LINES OF CODE"></a>FASTER ODE ADJOINTS WITH 12 LINES OF CODE</h2><blockquote>
<p>类似于event_triggered MPC的思想，满足一定条件时不进行计算，不满足时才进行计算, <a href="https://github.com/patrick-kidger/FasterNeuralDiffEq">code</a></p>
</blockquote>
<p>对于下述广义形式的ODE问题：</p>
<script type="math/tex; mode=display">
y(t) = y(\tau) + \int^t_{\tau} f(s, y(s))ds~~~y(t)\in R^d</script><p>假设$\hat y(t)$表示对应时刻的估计值，一旦有估计值，可以继续进行计算得到 $\hat y_{candidate}(t+\Delta)$, 此外， $y_{err}\in R^d$表示进行此步计算后，每个channel的numerical error。则对于给定的绝对容忍度$\text{ATOL}$（一般$1e-9$）和相对误差容忍度，以及对应的norm（比如$||y||=\sqrt{\frac{1}{d}\sum^d_{i=1}y_i^2}$）,则定义</p>
<script type="math/tex; mode=display">
\text{SCALE} = \text{ATOL} + \text{RTOL} . \text{max}(\hat y, \hat y_{candidate}(t+\Delta t))\in R^d</script><p>其中$\text{max}$是按每个channel来取的，则对应的误差率为 $\text{error ratio}$</p>
<script type="math/tex; mode=display">
r = ||\frac{y_{err}}{\text{SCALE}}||\in R</script><p>如果$r&lt;1$，则认为这个误差是可以被接受的，不需要重新计算，直接令 $\hat y(t+\Delta) = \hat y_{candidate}(t+\Delta)$；反之，则认为上述的误差太大，选取更小的$\Delta$并进行测试。</p>
<h2 id="Deep-Equilibrium-Models"><a href="#Deep-Equilibrium-Models" class="headerlink" title="Deep Equilibrium Models"></a>Deep Equilibrium Models</h2><blockquote>
<p> 整体思想是权重共享，然后将输入不断得给到一个layer中去，假设最终收敛，称之为不动点。</p>
</blockquote>
<script type="math/tex; mode=display">
\lim_{i\rightarrow \infty}z_{1:T}^{[i]}=\lim_{i\rightarrow \infty}f_{\theta}(z_{1:T}^{[i]};x_{1:T})\equiv f_{\theta}(z_{1:T}^*;x_{1:T})=z^*_{1:T}</script><ul>
<li><p>Back propagation: $(.)$代表$f_{\theta}(z_{1:T}^*;x_{1:T})$中的参数（比如$\theta, x_{1:T}$）</p>
<script type="math/tex; mode=display">
\frac{\partial z_{1:T}^*}{\partial(.)} = \frac{df_\theta(z^*_{1:T};x_{1:T})}{d(.)} + \frac{\partial f_{\theta}(z^*_{1:T};x_{1:T})}{\partial z^*_{1:T}}\frac{\partial  z^*_{1:T}}{\partial (.)}</script><p>因此可得</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
    (I-\frac{\partial f_{\theta}(z^*_{1:T};x_{1:T})}{\partial z^*_{1:T}})
\frac{\partial  z^*_{1:T}}{\partial (.)} &= \frac{df_{\theta}(z^*_{1:T};x_{1:T})}{d(.)}\\
\end{aligned}
\end{equation}</script><p>令 $g_{\theta}(z_{1:T}^\star)=f_{\theta}(z^\star_{1:T};x_{1:T})-z_{1:T}^\star$ ,可得</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
J_{g_{\theta}\big|{z_{1:T}^*}} &= -    (I-\frac{\partial f_{\theta}(z^*_{1:T};x_{1:T})}{\partial z^*_{1:T}})\\
\frac{\partial z_{1:T}^*}{\partial(.)} &= -J^{-1}_{g_{\theta}}\big|_{z_{1:T}^*}\frac{df_{\theta}(z_{1:T}^*;x_{1:T})}{d(,)}\\
\frac{\partial l}{\partial (.)} &= \frac{\partial l}{\partial z_{1:T}^*}\frac{\partial z_{1:T}^*}{\partial(.)}
\end{aligned}
\end{equation}</script></li>
<li><p>对于不动点进行估计的有效方法</p>
<script type="math/tex; mode=display">
g_{\theta}(z_{1:T}^*;x_{1:T}) =f_{\theta}(z_{1:T}^*;x_{1:T})-z_{1:T}^*\rightarrow 0</script><p>并采用拟牛顿法中的Broyden方法对于雅可比矩阵的逆进行近似</p>
<script type="math/tex; mode=display">
J^{-1}_{g_{\theta}}\big|_{z_{1:T}^*} \approx B_{g_\theta}^{[i+1]} = B_{g_\theta}^{[i]} + \frac{\Delta z^{[i+1]} - B_{g_{\theta}}^{i}\Delta g_{\theta}^{[i+1]}}{\Delta z^{[i+1]^T}B_{g_{\theta}}^{[i]}\Delta g_{\theta}^{[i+1]}}\Delta z^{[i+1]^T}B_{g_{\theta}}^{[i]}</script><p>其中$\Delta z^{[i+1]} = z_{1:T}^{[i+1]}-z_{1:T}^{[i]}$, $\Delta g_{\theta}^{[i+1]} = g_{\theta}(z_{1:T}^{[i+1]};x_{1:T}) - g_{\theta}(z_{1:T}^{[i]};x_{1:T})$,初始$g_{\theta}$可以设置为0.</p>
</li>
<li><p>另一种方案是back propagation直接计算 $-\frac{\partial l}{\partial z_{1:T}^\star}J^{-1}_{g_{\theta}}\big|_{z_{1:T}^\star}$</p>
<script type="math/tex; mode=display">
J^{^T}_{g_{\theta}}\big|_{z_{1:T}^*} x^T + (\frac{\partial l }{\partial z_{1:T}^*})^T = 0</script><p>然后用拟牛顿法来解这个问题</p>
</li>
</ul>
<h2 id="ODE2VAE"><a href="#ODE2VAE" class="headerlink" title="ODE2VAE"></a>ODE2VAE</h2><p>对于下述问题：</p>
<script type="math/tex; mode=display">
\ddot z_t = \frac{d^2 z_t}{d^2 t}=f_{W}(z_t, \dot z_t)</script><p> 可以等价变为两个一阶ode的形式</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\dot s_t &= v_t\\
\dot v_t &= f_W(s_t, v_t)
\end{aligned}
\end{equation}\\</script><p>即 </p>
<script type="math/tex; mode=display">
\begin{bmatrix}
s_T\\
v_T
\end{bmatrix} =\begin{bmatrix}
s_0\\
v_0
\end{bmatrix} + \int^T_0\begin{bmatrix}v_t\\f_W(s_t, v_t)
\end{bmatrix}dt</script><p>其中$W=\{W_l\}_{l=1}^L$，表示L层中所有的权重和偏差参数，假设对$p(W)$进行一个先验的假设，则对此进行采样，就会确定一个神经网络，从而决定ODE的轨迹</p>
<h3 id="二阶ODE流模型"><a href="#二阶ODE流模型" class="headerlink" title="二阶ODE流模型"></a>二阶ODE流模型</h3><p>对于上述的ODE模型，根据Neural ODE中的结果，可知</p>
<script type="math/tex; mode=display">
\frac{\partial \log q(z_t|W)}{\partial t} = -Tr(\frac{d\tilde f_W(z_t)}{dz_t}) = -Tr\begin{pmatrix}
\frac{\partial v_t}{\partial s_t}&\frac{\partial v_t}{\partial v_t}\\
\frac{\partial f_W(s_t, v_t)}{\partial s_t}&\frac{\partial f_W(s_t, v_t)}{\partial v_t}
\end{pmatrix}= -Tr(\frac{\partial f_W(s_t, v_t)}{\partial v_t})</script><p>从而可知</p>
<script type="math/tex; mode=display">
\log q(z_T|W)=\log q(z_0|W) - \int^T_0Tr(\frac{\partial f_W(s_t, v_t)}{\partial v_t})dt</script><h3 id="ODE2VAE模型"><a href="#ODE2VAE模型" class="headerlink" title="ODE2VAE模型"></a>ODE2VAE模型</h3><p>infer在低维空间上的速度和位置轨迹，但是仍然能将数据拟合的很好。下面假设动态方程：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
s_0&\sim p(s_0)\\
v_0 &\sim p(v_0)\\
s_t &= s_0 + \int^t_0v_\tau d\tau\\
v_t &= v_0 + \int^t_0 f_{true}(s_\tau, v_\tau)d\tau\\
x_i &\sim p(x_i|s_i), i\in [0,N]\\
\end{aligned}
\end{equation}</script><p>下面给出变分推断基本假设</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
q(W, z_{0:N}|x_{0:N}) &= q(W)q_{enc}(z_0|x_{0:N})q_{ode}(z_{1:N}|x_{0:N},z_0,W)\\
q(W) &= \mathcal{N}(W|m. sI)\\
q_{enc}(z_0|x_{0:N})&=q_{enc}\big( \begin{pmatrix}s_0\\v_0\end{pmatrix}\big| x_{0:N}\big)\\
&= \mathcal{N}\big(\begin{pmatrix}\mu_s(x_0)\\\mu_v(x_{0:m)}\end{pmatrix}, \begin{pmatrix}
diag(\sigma_s(x_0))&0\\
0&diag(\sigma_v(\sigma_v(x_{0:m})\big)
\end{pmatrix})
\end{aligned}
\end{equation}</script><h4 id="ELBO"><a href="#ELBO" class="headerlink" title="ELBO"></a>ELBO</h4><script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\log p(X) &\geq -KL(q(W,Z|X)||p(W,Z)) + E_{q(W,Z|X)}[\log p(X|W,Z)]\\
&= -E_{q(Z,W|X)}[\log\frac{q(W)q(Z|W,X)}{p(W)p(Z)}] + E_{q(Z,W|X)}[\log p(X|W,Z)]\\
& = -KL(q(W)||p(W))+ E_{q(Z,W|X)}[-\log \frac{q(Z|W,X)}{p(Z)} + \log p(X|W,Z)]\\
&=-KL(q(W)||p(W)) + E_{q_{enc}(z_0|X)}[-\log \frac{q_{enc}(z_0|X)}{p(z_0)} + \log p(x_0|z_0)]\\
&+ \sum^N_{i=1}E_{q_{ode}(W,z_i|X,z_0)}[-\log \frac{q_{ode}(z_i|W,Z)}{p(z_i)} + \log p(x_i|z_i)]
\end{aligned}
\end{equation}</script><p>其中，先验分布 $p(W,z_0)$是标准高斯分布。</p>
<ul>
<li><p>文章对于损失函数的改进，</p>
<ul>
<li>提出问题<ul>
<li>VAE模型优化ELBO不一定得到正确的inference。</li>
<li>KL项和重构项存在不平衡的矛盾。——-&gt;加权</li>
<li>在此问题中，encoder只是为了得到最初的$z_0$，对于长时预测问题或在小数据或初始数据分布和数据分布不一致时，(26)中动态损失项(第三项)，很容易支配VAE的损失，引起欠拟合———-&gt; 减少编码器分布和ODE流分布之间的距离</li>
</ul>
</li>
<li>方案：<ul>
<li>对于$-KL(q(W)||p(W))$，增加一个权重$\beta=|q|/|W|$,表示因空间维度和权重数目的比。</li>
<li>减少编码器分布和ODE流分布之间的距离</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
L & =-\beta KL(q(W)||p(W))+ E_{q(Z,W|X)}[-\log \frac{q(Z|W,X)}{p(Z)} + \log p(X|W,Z)]\\
&-\gamma E_q(W)[KL(q_{ode}(Z|X) || q_{enc}(Z|W,X)]
\end{aligned}
\end{equation}</script></li>
</ul>
<h2 id="FFJORD-free-form-continuous-dynamics-for-scalable-reversible-generative-models"><a href="#FFJORD-free-form-continuous-dynamics-for-scalable-reversible-generative-models" class="headerlink" title="FFJORD: free form continuous dynamics for scalable reversible generative models"></a>FFJORD: free form continuous dynamics for scalable reversible generative models</h2><p>对于CNF，根据neural ode 中有</p>
<script type="math/tex; mode=display">
\frac{d\log p(\vec z(t))}{dt} = -tr(\frac{df}{dz(t)})</script><p>因此，对于增广状态有，</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
z_0\\
\log p(x) - \log p_{z_0}(z_0) 
\end{bmatrix} = \int_{t_1}^{t_0}
\begin{bmatrix}
f(z(t),t;\theta)\\
-Tr(\frac{\partial f}{\partial z(t)})
\end{bmatrix}dt\\
\begin{bmatrix}
z(t_1)\\
\log p(x)-\log p(z(t_1))
\end{bmatrix} = \begin{bmatrix}
x\\0
\end{bmatrix}</script><p>根据下述重要推论中的第二条，可知</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\log p(z(t_1)) &= \log p(z(t_0)) - \int_{t_0}^{t_1}Tr(\frac{\partial f}{\partial z(t)})dt\\
&= \log p(z(t_0)) - \mathbb{E}_{p(\epsilon)}\big[ \int_{t_0}^{t_1}\epsilon^T \frac{\partial f}{\partial z(t)}\epsilon dt \big]
\end{aligned}
\end{equation}</script><h2 id="How-to-train-your-Neural-ODE"><a href="#How-to-train-your-Neural-ODE" class="headerlink" title="How to train your Neural ODE"></a>How to train your Neural ODE</h2><blockquote>
<p>对于一般的ODE，积分步长相当于堆叠了很多层的layer，本篇文章引入了正则化来缓解这种问题。最终达到用<strong>更简单的动力学方程有更快的收敛速度</strong>，仅<strong>需要更少的离散化步骤求解</strong>。 Neural ODE 可以节省内存开销，但需要很长的时间训练。</p>
</blockquote>
<ul>
<li><p><strong>为什么需要正则化？</strong>从下述图中可看出，两模型输入输出相同，但右图中局部特性不理想，比如局部轨迹变化大，速度不均匀等。左图更加规则。</p>
<p><img src="/2020/09/06/Dynamic-Systems-for-Deep-Learning/train_ODE_1.png" alt="1"></p>
</li>
<li><p><strong>如何measure vector field是否“规则”？</strong>  评估在各点$z(t)$所受的“力”，也就是</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\frac{df(x, t)}{dt} &= \nabla f(z, t) , \dot z + \frac{\partial f(z, t)}{\partial t}\\
&=\nabla f(z,t)f(z, t) + \frac{\partial f(z, t)}{\partial t}
\end{aligned}
\end{equation}</script><p>因此，引入两个正则化项：一个是为了对f正则化，另一个则是对$\nabla f$正则化</p>
<ul>
<li>评估流f行走的距离，也可认为是流模型的动能。</li>
<li>对于vector field中的雅可比行列式进行正则化</li>
</ul>
</li>
<li><p>采用Optimal Transport问题中的Benamou-Brenier方法，引入惩罚项</p>
<p>首先，介绍Optimal  Transport, 假设有两个分布  $p(x), q(x)$，及映射  $z:p(x)-&gt;q(z)$，则应最小化传输代价</p>
<script type="math/tex; mode=display">
M(t) = \int||x-z(x)||^2 p(x)dx</script><p>如果将映射$z(x,T)$写为通过ODE函数f得到的一个映射，则可转化为</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\min_{f, \rho} \int^T_{0}&\int ||f(x, t)||^2 \rho_t(x)dxdt\\
subject~~~to&\\
\frac{\partial \rho_t}{\partial t} &= -div(\rho f)\\
\rho_0(x) &= p\\
\rho_T(x) &= q\\
\end{aligned}
\end{equation}</script><p>此式是(32)式的上界，只有当在最优的时候才会和（32）相等，而后，为了保证$\rho_T(z)=q$，引入KL散度，且恰好可以简化成以下形式($\rho_0 = p_{\theta}$)</p>
<script type="math/tex; mode=display">
KL(\rho_T||q) = -\frac{1}{N} \sum^N_{i=1}\log p_{\theta}(x_i)</script><p>此时(33)变为</p>
<script type="math/tex; mode=display">
J_{\lambda}(f) = \frac{\lambda}{N}\sum^N_{i=1}\int^T_{0}||f(z, t)||^2dt -\frac{1}{N} \sum^N_{i=1}\log p_{\theta}(x_i)</script><p><strong>第一项即为添加的正则项</strong></p>
</li>
<li><p>可发现即使f正则化到很小，如果雅可比矩阵很大，那么（31）依然很大，因此，引入雅可比矩阵的F范数来进行正则化</p>
<script type="math/tex; mode=display">
||\nabla f(z)||^2_F = E_{\epsilon \sim N(0,1)} \epsilon^T \nabla f(z)\nabla f(z)^T\epsilon = E_{\epsilon \sim N(0,1)}||\epsilon^T \nabla f(z)||^2</script></li>
</ul>
<h4 id="最终算法实现"><a href="#最终算法实现" class="headerlink" title="最终算法实现"></a>最终算法实现</h4><p><img src="/2020/09/06/Dynamic-Systems-for-Deep-Learning/train_ODE_2.png" style="zoom:80%;"></p>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>Neural ODE</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Evolutionary Multi-Objective Workflow Scheduling in Cloud</title>
    <url>/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/</url>
    <content><![CDATA[<h2 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea"></a>Main idea</h2><ul>
<li><p><strong>Objective</strong>: optimize both makespan and cost</p>
</li>
<li><p><strong>Main Contributions</strong>:</p>
<ul>
<li><p>突出显示直接应用于云的现有调度算法的挑战，利用真实的云特性制定云工作流调度问题。</p>
<p>使用基于实例的IaaS风格方案制定定价选项，但不指定特定的定价规则</p>
</li>
<li><p>present the EMO(Evolutionary Multi-Objective) algorithm for the modeled workflow scheduling problem.</p>
</li>
</ul>
</li>
</ul>
<h2 id="Challenges-for-Scheduling-Workflows-in-Cloud"><a href="#Challenges-for-Scheduling-Workflows-in-Cloud" class="headerlink" title="Challenges for Scheduling Workflows in Cloud"></a>Challenges for Scheduling Workflows in Cloud</h2><h3 id="云计算和网格计算模型的差别"><a href="#云计算和网格计算模型的差别" class="headerlink" title="云计算和网格计算模型的差别"></a>云计算和网格计算模型的差别</h3><ul>
<li><p>the complex pricing schemes:在云环境下一个任务的cost在调度之前很难精准地预算；</p>
<p>大多数调度的启发式算法还要求知道每个任务的cost来进行优先级排序或者处理器的选择。</p>
</li>
<li><p>the large-size resource pools：在传统异构的环境中，资源池是有限的，所以有些算法是遍历资源池来进行处理器的选择，但是对于云环境不使用。</p>
</li>
<li><p>有一种是把所有种类的instance(VMs)做成一个列表，但是依然太大了。</p>
</li>
</ul>
<h2 id="Workflow-Scheduling-Problem"><a href="#Workflow-Scheduling-Problem" class="headerlink" title="Workflow Scheduling Problem"></a>Workflow Scheduling Problem</h2><h3 id="基本概念定义"><a href="#基本概念定义" class="headerlink" title="基本概念定义"></a>基本概念定义</h3><ul>
<li><strong>DAG</strong> W = (T,D), $T = \{T_0,T_1,…,T_n\}$ is the set of tasks, $D = \{(T_i,T_j)|T_i,T_j\in T\}$ is the set of data or control dependencies.</li>
<li><strong>refertime($T_i$)</strong>: reference execution time of $T_i$</li>
<li><strong>data($T_i,T_j$)</strong> data transfer size from $T_i$ to $T_j$</li>
<li><strong>pred(T_i)</strong>=$\{T_j|(T_j ,T_i)\in D\}$</li>
<li><strong>T_{entry}</strong>: the entry task;<strong>T_{exit}</strong>: exit task </li>
</ul>
<h3 id="云资源管理"><a href="#云资源管理" class="headerlink" title="云资源管理"></a>云资源管理</h3><ul>
<li><p>认为云端的instances是无穷的，用$I=\{I_0,I_1,…\}$来形容所有可用的instances。$P =\{P_0,P_1,…,P_m\}$代表所有的instances的类型，共m种。</p>
</li>
<li><p>用$cu(P_i)$来代表instance type $P_i$的计算单元（compute unit），并且假设<strong>如果一个instacne的CU变为原来的两倍，执行时间会缩短为原来的一半</strong>,故而任务$T_i$的实际运行时间为</p>
<script type="math/tex; mode=display">
Time_{comp}(T_i)=\frac{refertime(T_i)}{cu(P_j)}</script></li>
<li><p>带宽 $bw(P_i)$代表instance type $P_i$的带宽，那么$T_i$和$T_j$之间的communication time (忽略setup delays)为</p>
<script type="math/tex; mode=display">
Time_{comm}(T_i,T_j)=\begin{equation}
\left\{
    \begin{array}{**rc1**}
    \frac{data(T_i,T_j)}{min\{bw(P_p),bw(P_q)\}},~~~p\neq q\\
    0,~~~p=q
    \end{array}
\right.
\end{equation}</script></li>
<li><p><strong>定价模型</strong>：用$M=\{M_0,M_1,…,M_k\}$来表示IaaS提供的pricing options，并且定义了函数<strong>charge</strong>$(M_h,P_j,I_i)$计算用定价模型$M_h$来算type为$P_j$的instance $I_i$.</p>
</li>
</ul>
<h3 id="工作流调度问题定义"><a href="#工作流调度问题定义" class="headerlink" title="工作流调度问题定义"></a>工作流调度问题定义</h3><ul>
<li>已知$W=(T,D)$，并且有IaaS平台$S=(I,P,M)$,那么一个调度问题就是要产生一个或多个下列形式的解 $R = (Ins, Type, Order)$，其中<script type="math/tex; mode=display">
Ins: T\rightarrow I, Ins(T_i) = I_j\\
Type: I\rightarrow P, Type(I_s) = P_t</script><img src="/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/1.png" alt="1"></li>
</ul>
<h2 id="多目标进化算法优化"><a href="#多目标进化算法优化" class="headerlink" title="多目标进化算法优化"></a>多目标进化算法优化</h2><ul>
<li>最主要的贡献就是：present a whole set of the exploration operations, including encoding, population initialization, crossover, and mutation. These operations can work with any exploitation operation (e.g., fitness assignment, selection) in the EMO area </li>
</ul>
<p><img src="/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/2.png" alt="2"></p>
<p>其中$avail(Ins(T_i))$为instance $I_i$可用的时间，他是通过$FT(T_i)$来更新的</p>
<h3 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h3><ul>
<li><p>由于解的形式为$R = (Ins, Type, Order)$,所以染色体分为三段。</p>
<p><strong>order</strong>是所有任务序列的排列，如果i在j前面，则$T_i$会比$T_j$先确定用hosting instance(VM),但是并不意味$T_i$比$T_j$先执行。</p>
<p><strong>task2ins</strong>:是长度为n（假设有n中类型）的映射<em>Ins</em>的向量,其中index代表一个任务，他的值代表选择哪一个instance来执行此任务。例如tak2ins[i]=j就是让$T_i$被安排到一个index为j的instance($I_j$)上。</p>
<p><strong>ins2type</strong>:一个从instance index映射到他们类型的映射，例如ins2type[j] = k表示instance $I_j$的类型为$P_k$</p>
</li>
</ul>
<p><img src="/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/3.png" alt="3"></p>
<h3 id="Crossover"><a href="#Crossover" class="headerlink" title="Crossover"></a>Crossover</h3><p><img src="/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/4.png" alt="4"></p>
<ul>
<li><p>需要注意不能违反依赖关系；This operator will not cause any dependency conflict since the order of any two tasks should have already existed in at least one parent.  </p>
</li>
<li><p>对于task2ins和ins2type则同时进行crossover操作</p>
<ul>
<li><p>随机选择一个切断点，而后将两个父代的task2ins字符串的第一部分进行交换。</p>
<p><strong>注意</strong>：一个任务所在运行的VM的种类也是十分重要的，所以，如果一个任务T，在type为$p_i$的instance I上运行，现在被重新安排到类型为$p_j$的$I^{<em>}$上，那么我们应将$I^{</em>}$的类型修改为$p_i$,但是这样有可能导致其他原本安排在此VM上的任务的所选类型改变，这种情况则将$I^{*}$的类型随机选为$p_i$或者$p_j$;如果没有发生冲突，则对VM的type引入一个小几率的变异操作</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/5.png" alt="5"></p>
<p><img src="/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/8.png" alt="5"></p>
<h3 id="Mutation"><a href="#Mutation" class="headerlink" title="Mutation"></a>Mutation</h3><ul>
<li><p>Mutation of <em>order</em></p>
<p>define all successors of task $T_i$ as</p>
<script type="math/tex; mode=display">
succ(T_i)=\{T_j|(T_i,T_j)\in D\}</script><p><strong>整体思想</strong>：从pos开始向前向后查找，找到可以交换的index的最大范围，然后在这个范围内随机产生一个整数，将pos移到此位置<br><img src="/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/6.png" alt="6"></p>
</li>
<li><p>Mutation of <em>task2ins</em> and <em>ins2type</em> :以一个小概率来对每一个位置产生一个新的有效的值</p>
</li>
</ul>
<h3 id="Initial-Population"><a href="#Initial-Population" class="headerlink" title="Initial Population"></a>Initial Population</h3><ul>
<li><strong>假定</strong>，种群大小为n，那么初始化时，<ul>
<li>一个个体是通过HEFT(异构最早完成时间算法)计算，被认为是最快的调度</li>
<li>一个个体是在执行HEFT时同时产生的，是要产生最便宜的调度</li>
<li>其他n-2个个体是通过<strong>RandTypeOrIns</strong>来产生的。</li>
</ul>
</li>
</ul>
<p><img src="/2018/11/16/Evolutionary-Multi-Objective-Workflow-Scheduling-in-Cloud/7.png" alt="7"></p>
<p><strong>整体思想</strong>：order是按照递增来构造的，然后随机选择一种instance type,将所有的instance的类型设为此类型，对于task2ins，则是有两种方法：第一种是将所有的任务分配到同一个instance(0)上;另一种是对每一个任务产生一个0到n-1之间的随机数。</p>
]]></content>
      <categories>
        <category>Cloud Workflow Scheduling</category>
      </categories>
      <tags>
        <tag>Cloud Workflow Scheduling</tag>
      </tags>
  </entry>
  <entry>
    <title>FirstTest</title>
    <url>/2018/11/02/FirstTest/</url>
    <content><![CDATA[<h1 id="This-is-a-test-on-hexo-plus-github"><a href="#This-is-a-test-on-hexo-plus-github" class="headerlink" title="This is a test on hexo plus github"></a>This is a test on hexo plus github</h1><h2 id="Head-2"><a href="#Head-2" class="headerlink" title="Head 2"></a>Head 2</h2><h3 id="head3"><a href="#head3" class="headerlink" title="head3"></a>head3</h3><ul>
<li><p>Now we can test the support for formulas, $x^2 + 2x+ 1 =\frac{1}{2} g$,</p>
<script type="math/tex; mode=display">
Cov(x,y) = E(xy)-E(x)E(y)</script><script type="math/tex; mode=display">
\begin{bmatrix}
a&b&c\\
d&e&f
\end{bmatrix}</script></li>
</ul>
<ul>
<li><p>Now for  <strong>Bold</strong>, <u>underline</u>, and <em>italic</em></p>
</li>
<li><p>Test on codes</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Test on images </p>
<p><img src="/2018/11/02/FirstTest/FirstTest_1.jpg" alt="FirstTest_1"></p>
</li>
<li><p>Test on tables</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>1</th>
<th>1</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Test on linux…</li>
</ul>
]]></content>
      <categories>
        <category>Entertainment</category>
      </categories>
      <tags>
        <tag>Try4New</tag>
      </tags>
  </entry>
  <entry>
    <title>ICLR 22 submitted paper list</title>
    <url>/2021/11/12/ICLR%2022%20submitted%20papers/</url>
    <content><![CDATA[<h2 id="ICLR-22-submitted-papers-list"><a href="#ICLR-22-submitted-papers-list" class="headerlink" title="ICLR 22 submitted papers list"></a>ICLR 22 submitted papers list<span id="more"></span></h2><h3 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h3><ul>
<li><p><a href="https://openreview.net/forum?id=Ix_mh42xq5w">PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=7sz69eztw9">Context-invariant, multi-variate time series representations</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=VDdDvnwFoyM">TimeVAE: A Variational Auto-Encoder for Multivariate Time Series Generation </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=JpNH4CW_zl">Multivariate Time Series Forecasting with Latent Graph Inference</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=LzQQ89U1qm_">Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=wv6g8fWLX2q">TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=ZncyIXXAB-0">IIT-GAN: Irregular and Intermittent Time-series Synthesis with Generative Adversarial Networks</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=Kwm8I7dU-l5">Graph-Guided Network for Irregularly Sampled Multivariate Time Series</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=45L_dgP48Vd">Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=jM62SQw28f">DeepFIB: Self-Imputation for Time Series Anomaly Detection</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=Jh9VxCkrEZn">Spatiotemporal Representation Learning on Time Series with Dynamic Graph ODEs</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=oVfIKuhqfC">Non-Denoising Forward-Time Diffusions </a></p>
</li>
</ul>
<h3 id="EBM"><a href="#EBM" class="headerlink" title="EBM"></a>EBM</h3><ul>
<li><p><a href="https://openreview.net/forum?id=h4EOymDV3vV">Diffusion-Based Representation Learning</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=Lm8T39vLDTE">Autoregressive Diffusion Models </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=lsQCDXjOl3k">Unconditional Diffusion Guidance</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=_BNiN4IjC5">PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=TIdIXIpzhoI">Progressive Distillation for Fast Sampling of Diffusion Models</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=CzceR82CYc">Score-Based Generative Modeling with Critically-Damped Langevin Diffusion</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=L7wzpQttNO">BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=FuLL40HLCRn">ST-DDPM: Explore Class Clustering for Conditional Diffusion Probabilistic Models </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=0xiJLKH-ufZ">Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=xVGrCe5fCXY">Denoising Diffusion Gamma Models </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=1v1N7Zhmgcx">Maximum Likelihood Training of Parametrized Diffusion Model </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=YmONQIWli--">Gotta Go Fast When Generating Data with Score-Based Models </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=LOz0xDpw4Y">Learning to Efficiently Sample from Diffusion Probabilistic Models </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=aBsCjcPu_tE">SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=qHsuiKXkUb">High Precision Score-based Diffusion Models </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=nioAdKCEdXB">Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=1Zxv7TdLquI">YOUR AUTOREGRESSIVE GENERATIVE MODEL CAN BE BETTER IF YOU TREAT IT AS AN ENERGY-BASED ONE</a></p>
</li>
</ul>
<h3 id="Dynamic-systems"><a href="#Dynamic-systems" class="headerlink" title="Dynamic systems"></a>Dynamic systems</h3><ul>
<li><p><a href="https://openreview.net/forum?id=DIsWHvtU7lF">Composing Partial Differential Equations with Physics-Aware Neural Networks </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=LcF-EEt8cCC">Denoising Likelihood Score Matching for Conditional Score-based Data Generation </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=r9cpyzP-DQ">Learning Efficient and Robust Ordinary Differential Equations via Diffeomorphisms</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=fCG75wd39ze">LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=dtYnHcmQKeM">Physics-Informed Neural Operator for Learning Partial Differential Equations</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=fGEoHDk0C">A framework of deep neural networks via the solution operator of partial differential equations </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=r88Isj2alz">NODEAttack: Adversarial Attack on the Energy Consumption of Neural ODEs </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=wENMvIsxNN">D-CODE: Discovering Closed-form ODEs from Observed Trajectories </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=SFgkP_PZvL">PNODE: A memory-efficient neural ODE framework based on high-level adjoint differentiation</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=c-4HSDAWua5">SketchODE: Learning neural sketch representation in continuous time</a></p>
</li>
</ul>
<h3 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h3><ul>
<li><p><a href="https://openreview.net/forum?id=EMxu-dzvJk">GRAND++: Graph Neural Diffusion with A Source Term </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=XJiajt89Omg">Space-Time Graph Neural Networks </a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ICLR 21 paperlist</title>
    <url>/2021/01/15/ICLR2021/</url>
    <content><![CDATA[<h2 id="ICLR-2021-paper-list"><a href="#ICLR-2021-paper-list" class="headerlink" title="ICLR 2021 paper list "></a>ICLR 2021 paper list <span id="more"></span></h2><h3 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h3><ul>
<li><p><a href="https://openreview.net/forum?id=4c0J6lwQ4_">Multi-Time Attention Networks for Irregularly Sampled Time Series</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=WEHSlH5mOk">Discrete Graph Structure Learning for Forecasting Multiple Time Series</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=8qDwejCuCN">Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=6t_dLShIUyZ">Greedy-GQ with Variance Reduction: Finite-time Analysis and Improved Complexity </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=F3s69XzWOia">Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies </a></p>
</li>
</ul>
<h3 id="Dynamic-systems"><a href="#Dynamic-systems" class="headerlink" title="Dynamic systems"></a>Dynamic systems</h3><ul>
<li><p><a href="https://openreview.net/forum?id=HxzSxSxLOJZ">ResNet After All: Neural ODEs and Their Numerical Solution </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=blfSjHeFM_e">MALI: A memory efficient and reverse accurate integrator for Neural ODEs </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=giit4HdDNa">Go with the flow: Adaptive control for Neural ODEs </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=q-cnWaaoUTH">Conformation-Guided Molecular Representation with Hamiltonian Neural Networks </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=45NZvF1UHam">Identifying Physical Law of Hamiltonian Systems via Meta-Learning </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=aUX5Plaq7Oy">Learning continuous-time PDEs from sparse data with graph neural networks </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=vLaHRtHvfFp">PDE-Driven Spatiotemporal Disentanglement </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=-IXhmY16R3M">Universal approximation power of deep residual neural networks via nonlinear control theory </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=P0p33rgyoE">Variational Intrinsic Control Revisited</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=vyY0jnWG-tK">Physics-aware, probabilistic model order reduction with guaranteed stability </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=Iw4ZGwenbXf">NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=PxTIG12RRHS">Score-Based Generative Modeling through Stochastic Differential Equations</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=xpx9zj7CUlY">Randomized Automatic Differentiation</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=p-NZIuwqhI4">Dynamics of Deep Equilibrium Linear Models</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=_XYzwxPIQu6">Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies </a></p>
</li>
</ul>
<h3 id="Generative-Models"><a href="#Generative-Models" class="headerlink" title="Generative Models"></a>Generative Models</h3><ul>
<li><p><a href="https://openreview.net/forum?id=MBpHUFrcG2x">Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=-GLNZeVDuik">Categorical Normalizing Flows via Continuous Transformations </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=PpshD0AXfA">Generative Time-series Modeling with Fourier Flows </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=MBOyiNnYthd">IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=Zbc-ue9p_rE">Refining Deep Generative Models via Wasserstein Gradient Flows</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=te7PVH1sPxJ">Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=Ig53hpHxS4">Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=iWLByfvUhN">Decoupling Global and Local Representations via Invertible Generative Flows </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=l0V53bErniB">Combining Physics and Machine Learning for Network Flow Estimation</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=HOFxeCutxZR">Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=tYxG_OMs9WE">Property Controllable Variational Autoencoder via Invertible Mutual Dependence </a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Flow based Models</title>
    <url>/2020/09/06/Flow-based-Models/</url>
    <content><![CDATA[<blockquote>
<p>本节将会介绍生成模型中非常典型的一类方法，流模型 。<span id="more"></span></p>
</blockquote>
<h3 id="NICE-Non-linear-Independent-Components-Estimation"><a href="#NICE-Non-linear-Independent-Components-Estimation" class="headerlink" title="NICE: Non-linear Independent Components Estimation"></a>NICE: Non-linear Independent Components Estimation</h3><ul>
<li>对于一个数据分布 x，通过一个可逆映射 f(x), 可以变成另外一个分布y，则两者之间概率密度分布之间的关系为</li>
</ul>
<script type="math/tex; mode=display">
p(x) = p(y)|\det \frac{\partial f(x)}{\partial x}|\\</script><ul>
<li><p>对于函数f有两个要求：</p>
<ul>
<li>可逆，且容易求其逆函数</li>
<li>对应的雅可比行列式容易计算</li>
</ul>
</li>
<li><p>NICE采用加性耦合层</p>
<script type="math/tex; mode=display">
h_1= x_1\\h_2= x_2+m(x_1)</script><p>可以算出，对应的雅可比矩阵是下三角阵，且对角元素为1，故而行列式为1，采用多个耦合层相连，可以增加表达能力。</p>
<p>但是上述存在一定问题，即使多次连接，第一部分仍然是<strong>恒等变换</strong>，故而作者提出交换顺序来进行耦合。</p>
<p><img src="/2020/09/06/Flow-based-Models/NICE.png" alt="NICE"></p>
</li>
<li><p>然而，正是由于模型是可逆的，因此最终z和x的维度是一致的，然而实际上，往往数据具有高维空间的低维流形，因此，引入尺度变换层，对于编码出来的每个维度的特征做尺度变换即</p>
<script type="math/tex; mode=display">
z = s\otimes h^{(n)}</script><p>其中s也是要训练优化的参数，可以识别出该维度的重要性，可以起到压缩流形的作用。</p>
<p>此时行列式变为了 $\Pi_i s_i$</p>
<p>实际上<strong>尺度变换层等价于将先验分布的方差（标准差）也作为训练参数，如果方差足够小，我们就可以认为该维度所表示的流形坍缩为一个点，从而总体流形的维度减1，暗含了降维的可能</strong>。</p>
</li>
<li><p>实验结果：</p>
<p><img src="/2020/09/06/Flow-based-Models/test.png" alt="test"></p>
</li>
</ul>
<h3 id="REALNVP"><a href="#REALNVP" class="headerlink" title="REALNVP"></a>REALNVP</h3><ul>
<li><p>REAL NVP是在NICE基础上进行改进的结果</p>
</li>
<li><p>仿射耦合层</p>
<script type="math/tex; mode=display">
h_1 = x_1\\
h_2 = s(x_1)\otimes x_2 +t(x_1)</script><p>此时，对应的雅可比行列式的值为$\Pi_i s(x_1)_i$, 注意，为了保持可逆性，一般将s各个元素约束为大于0.</p>
</li>
<li><p>在NICE中，是交替进行混合信息流，REALNVP则是采用shuffle的方式，将信息充分耦合</p>
<p><img src="/2020/09/06/Flow-based-Models/REALNVP.png" alt="REALNVP"></p>
</li>
<li><p>引入卷积层，对于图像来说，随意shuffle会打乱空间之间的相关性，因此作者引入以下几种方式来进行分割</p>
<p><img src="/2020/09/06/Flow-based-Models/squeeze.png" alt></p>
</li>
<li><p>多尺度结构： 启发于VGG网络，有点分形的味道</p>
<p><img src="/2020/09/06/Flow-based-Models/scaleMulti.png" alt="scaleMulti" style="zoom:50%;"><img src="/2020/09/06/Flow-based-Models/radial flow.png" alt></p>
</li>
</ul>
<p>可以得到</p>
<script type="math/tex; mode=display">
p(z_1, z_3, z_5)=p(z_1|z_2)p(z_3|z_4)p(z_5)\\</script><p>并假设$\hat z_1, \hat z_3, \hat z_5$服从标准正态分布，相较于直接输出，条件概率输出具有更大的灵活性。</p>
<script type="math/tex; mode=display">
\hat z_1 = \frac{z_1-\mu(z_2)}{\sigma(z_2)}\\
\hat z_3 = \frac{z_3-\mu(z_4)}{\sigma (z_4)}\\
\hat z_5 = \frac{z_5-\mu}{\sigma_z}\</script><h3 id="GLOW"><a href="#GLOW" class="headerlink" title="GLOW"></a>GLOW</h3><ul>
<li><p>GLOW模型是在NICE及REALNVP基础上改的，NICE和REALNVP中采用不同的方式打乱顺序，而注意到<strong>向量的置换操作，可以看作是向量乘一个置换矩阵得到的</strong>，GLOW中<strong>将置换矩阵作为一个可训练的参数矩阵</strong>，并因此引入了<strong>可逆$1\times1$卷积</strong></p>
</li>
<li><p>另一点是引入<strong>ActNorm</strong>， 也就是对于z进行缩放平移变换</p>
<script type="math/tex; mode=display">
\hat z = \frac{z-\mu}{\sigma}</script><p>其中$\mu$和$\sigma$都是训练参数，在<strong>初始化的时候是数据的均值和方差</strong></p>
</li>
</ul>
<h3 id="Variational-inference-with-normalizing-flows"><a href="#Variational-inference-with-normalizing-flows" class="headerlink" title="Variational inference with normalizing flows"></a>Variational inference with normalizing flows</h3><ul>
<li><p>Invertible Linear-Time Transformations（可逆线性变换层）<strong>planar flows</strong></p>
<script type="math/tex; mode=display">
f(z)=z+uh(w^Tz+b)</script><p>其中h为线性变换， 则可知</p>
<script type="math/tex; mode=display">
\psi(z)=h'(w^Tz+b)w\\
\det |\frac{\partial f}{\partial z}| = |\det(I +u\psi(z)^T)|=|1+u^T\psi(z)|</script><p>其中$u^T\psi(z)$是一个数。</p>
<script type="math/tex; mode=display">
z_k= f_k\circ f_{k-1}\circ\cdots\circ f_1(z)\\
\ln q_k(z_k)=\ln q_0(z) -\sum^K_{k=1}\ln |1+u_k^T\psi_k(z_k)|</script></li>
<li><p>改变变换形式，将变换改为将$q_0$绕某一个参考点去变化，</p>
<script type="math/tex; mode=display">
f(z)=z+\beta h(\alpha,r)(z-z_0)\\
\det |\frac{\partial f}{\partial z}|=(1+\beta h +\beta h' r)(1+\beta)^{d-1}</script></li>
<li></li>
</ul>
<p><img src="/2020/09/06/Flow-based-Models/radial flow.png" style="zoom:50%;"></p>
<h3 id="MADE-Masked-Autoencoder-for-Distribution-Estimation"><a href="#MADE-Masked-Autoencoder-for-Distribution-Estimation" class="headerlink" title="MADE: Masked Autoencoder for Distribution Estimation"></a>MADE: Masked Autoencoder for Distribution Estimation</h3><p><img src="/2020/09/06/Flow-based-Models/mad.png" style="zoom:80%;"></p>
<p>参考<a href="https://github.com/karpathy/pytorch-made">https://github.com/karpathy/pytorch-made</a></p>
<h3 id="wavenet-a-generative-model-for-raw-audio"><a href="#wavenet-a-generative-model-for-raw-audio" class="headerlink" title="wavenet: a generative model for raw audio"></a>wavenet: a generative model for raw audio</h3><p><img src="/2020/09/06/Flow-based-Models/wavenet.png" alt></p>
<p>可参考<a href="https://github.com/ibab/tensorflow-wavenet，其中causal">https://github.com/ibab/tensorflow-wavenet，其中causal</a> conv可以表示为</p>
<p><img src="/2020/09/06/Flow-based-Models/wavenet_conv.gif" alt></p>
<h3 id="Improved-Variational-Inference-with-Inverse-Autoregressive-Flow"><a href="#Improved-Variational-Inference-with-Inverse-Autoregressive-Flow" class="headerlink" title="Improved Variational Inference with Inverse Autoregressive Flow"></a>Improved Variational Inference with Inverse Autoregressive Flow</h3><p><img src="/2020/09/06/Flow-based-Models/IAF.png" alt></p>
<ul>
<li><p>相比于normalizing flow中常见的planar nf采用mlp的形式，表达能力欠缺，且在更高维度下需要更长的链转换概率密度，因此不适应于高维空间。</p>
</li>
<li><p>算法结构如下</p>
<p><img src="/2020/09/06/Flow-based-Models/IAF_alg.png" alt></p>
<ul>
<li>其中AutogressiveNN可以选择很多种，<strong>对于MLP类型的，可以选择MADE的形式</strong>；<strong>对于卷积类型的，可采用pixel cnn中的形式</strong>，参考<a href="https://github.com/pclucas14/iaf-vae">https://github.com/pclucas14/iaf-vae</a></li>
</ul>
</li>
</ul>
<h3 id="Sylvestrer-Normalizing-flows-for-Variance-Inference"><a href="#Sylvestrer-Normalizing-flows-for-Variance-Inference" class="headerlink" title="Sylvestrer Normalizing flows for Variance Inference"></a>Sylvestrer Normalizing flows for Variance Inference</h3><ul>
<li><p>首先提出<strong>Sylvester determinant identity</strong>，对于$A\in R^{D\times M}, B\in R^{M\times D}$, 有</p>
<script type="math/tex; mode=display">
\det (I_D+AB) = \det (I_M+BA)</script></li>
<li><p>对于形如（13）的形式，其中$A\in R^{D\times M}, B\in R^{M\times D}, b\in R^{M}, M&lt;D$根据（1）式可以得到(14)，从而降低计算量。</p>
<script type="math/tex; mode=display">
z'= z + Ah(Bz+b)</script><script type="math/tex; mode=display">
\det (\frac{\partial z'}{\partial z}) = \det (I_M + diag (h'(Bz+b)BA))</script></li>
<li><p>需要解决的问题是，(12)式一般是不可逆的，且行列式不好计算，因此引入QR分解，考虑如下特殊形式</p>
<script type="math/tex; mode=display">
z' = z + QR^Ah(R^BQ^Tz=b)\\
\det (\frac{\partial z'}{\partial z}) = \det (I_M + diag(h')R^BR^A)</script><p>下面<strong>证明可逆</strong>：如果$R^A, R^B$是上三角阵，且h是光滑有界的函数，且导数为正，则如果$R^A, R^B$对角元素满足$R^A_{ii}R^B_{ii}&gt;-1/||h’||_{\infty}$，则(15)式可以保证是可逆的</p>
</li>
</ul>
<h3 id="Continuous-Normalizing-Flow"><a href="#Continuous-Normalizing-Flow" class="headerlink" title="Continuous  Normalizing Flow"></a>Continuous  Normalizing Flow</h3><ul>
<li>Neural ODE中提出CNF的概念<script type="math/tex; mode=display">
\frac{d\vec z}{dt}=\vec f(\vec z(t), t)\\
\vec z(0)\sim p_o(\vec z(0))\\
\vec z(T) = \vec z(0) + \int _0^T\vec f(\vec z(t), t)dt\\
\frac{d\log p(\vec z(t))}{dt} = -tr(\frac{df}{dz(t)})</script></li>
</ul>
<h4 id="FFJORD-free-form-continuous-dynamics-for-scalable-reversible-generative-models"><a href="#FFJORD-free-form-continuous-dynamics-for-scalable-reversible-generative-models" class="headerlink" title="FFJORD: free form continuous dynamics for scalable reversible generative models"></a>FFJORD: free form continuous dynamics for scalable reversible generative models</h4><p>对于CNF，根据neural ode 中有</p>
<script type="math/tex; mode=display">
\frac{d\log p(\vec z(t))}{dt} = -tr(\frac{df}{dz(t)})</script><p>因此，对于增广状态有，</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
z_0\\
\log p(x) - \log p_{z_0}(z_0) 
\end{bmatrix} = \int_{t_1}^{t_0}
\begin{bmatrix}
f(z(t),t;\theta)\\
-Tr(\frac{\partial f}{\partial z(t)})
\end{bmatrix}dt\\
\begin{bmatrix}
z(t_1)\\
\log p(x)-\log p(z(t_1))
\end{bmatrix} = \begin{bmatrix}
x\\0
\end{bmatrix}</script><p>根据下述重要推论中的第二条，可知</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\log p(z(t_1)) &= \log p(z(t_0)) - \int_{t_0}^{t_1}Tr(\frac{\partial f}{\partial z(t)})dt\\
&= \log p(z(t_0)) - \mathbb{E}_{p(\epsilon)}\big[ \int_{t_0}^{t_1}\epsilon^T \frac{\partial f}{\partial z(t)}\epsilon dt \big]
\end{aligned}
\end{equation}</script><ul>
<li>上述对$Tr(A)$估计中的variance渐进趋向于 $||A||_F^2$,为了减少Hutchinson估计器的variance,作者提出用降低维度的方式，设 $f=g\circ h(z)$,则有<script type="math/tex; mode=display">
Tr(\frac{\partial f}{\partial z}) = Tr(\frac{\partial g}{\partial h}\frac{\partial h}{\partial z}) = Tr(\frac{\partial h}{\partial z}\frac{\partial g}{\partial h}) = \mathbb{E}_{p(\epsilon)}[\epsilon^T\frac{\partial h}{\partial z}\frac{\partial g}{\partial h}\epsilon]</script>第二项是$D\times D$维的，第三项则是 $H\times H$维的，如果H是很小的，那么我们就可以减少估计器的方差。</li>
</ul>
<h3 id="GRU-ODE-Bayes"><a href="#GRU-ODE-Bayes" class="headerlink" title="GRU-ODE_Bayes"></a>GRU-ODE_Bayes</h3><ul>
<li>特点：<ul>
<li>快速推断潜在随机过程的位置参数</li>
<li>学习变量中的相关性</li>
</ul>
</li>
<li>GRU-ODE包括两部分：<ul>
<li>GRU-ODE：基于GRU的ODE模型，使得隐状态h能够沿时间传播</li>
<li>GRU-Bayes：通过观测数据来更新当前的因状态</li>
</ul>
</li>
</ul>
<h4 id="GRU-ODE-推导"><a href="#GRU-ODE-推导" class="headerlink" title="GRU-ODE 推导"></a>GRU-ODE 推导</h4><ul>
<li><p>对于基本GRU单元，其结构如图所示：</p>
<p><img src="/2020/09/06/Flow-based-Models/GRU.jpg" style="zoom:50%;"></p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
r_t &= \sigma(W_rx_t + U_rh_{t-1}+b_r)\\
z_t &= \sigma(W_zx_t + U_zh_{t-1}+b_z)\\
g_t &= tanh(W_h x_t + U_h(r_t\odot h_{t-1})+b_h)\\
h_t &= z_t\odot h_{t-1} + (1-z_t)\odot g_t\\
\end{aligned}
\end{equation}</script><p>因此，可得隐状态变化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta h_t &= h_t - h_{t-1} = z_t\odot h_{t-1} + (1-z_t)\odot g_t -h_{t-1}\\
&=(1-z_t)\odot (g_t-h_{t-1})
\end{aligned}</script><p>可将隐状态微分方程变为</p>
<script type="math/tex; mode=display">
\frac{dh(t)}{dt} =(1-z(t))\odot (g(t)-h(t))</script></li>
<li><p>GRU-ODE 特性：</p>
<ul>
<li>有界性： 如果初始状态 $h(0)_j\in [-1, 1]$, 则$h(t)_j$则一直在此界内</li>
<li>连续性： GRU-ODE是李普希兹连续的，且K=2</li>
<li>支持各种数值求解器：例如欧拉解法，中值法等</li>
</ul>
</li>
</ul>
<h4 id="GRU-bayes"><a href="#GRU-bayes" class="headerlink" title="GRU-bayes"></a>GRU-bayes</h4><ul>
<li><p>为了用观测值对隐状态进行更新， 并估计$Y(t)$的概率密度函数； 采用标准GRU模块并在[-1,1]内进行变换</p>
</li>
<li><p>定义$f_{prep}$函数对于观测数据进行预处理：（通过隐状态h计算观测值概率密度函数的参数，假设是高斯函数）</p>
<script type="math/tex; mode=display">
\theta = [mean, logvar] = f_{obs}(h(t))\\
error = (y[k]_d - mean)/e^{0.5*logvar}\\
q_d = [y[k]_d, mean, logvar, error]\\
f_{prep} = ReLU(W_dq_d + bias) * m_d\\
h(t_+) = GRU(h(t_-), f_{prep}(y[k], m[k], h(t_-))\\</script><p>其中$h(t_-)$和 $h(t_+)$为GRU-Bayes前后的状态，因此，状态在此时会有一个跳跃。m是mask‘函数，结果中没有观测值的位置直接置0</p>
<p><img src="/2020/09/06/Flow-based-Models/GRU_hidden.png" alt></p>
</li>
<li><p>目标函数： $Loss_{pre} + \lambda Loss_{post}$</p>
<script type="math/tex; mode=display">
Loss_{pre} = -\sum^D_{j=1}m_j\log p(y_j|\theta=f_{obs}(h_-)_j)\\
Loss_{post} = \sum^D_{j=1}m_jD_{KL}(p_{Bayes,j}||p_{post,j})</script><p>其中第一项假设的是高斯分布；第二项损失中$p_{Bayes,j} \propto p_{pre,j}p_{obs,j}$, $p_{pre}$为GRU-Bayes之前的预测分布， $p_{post,j}$表示GRU-Bayes之后的预测分布。</p>
</li>
</ul>
<p><img src="/2020/09/06/Flow-based-Models/GRU_ODE_Bayes.png" alt></p>
<ul>
<li>$p_{Bayes,j} $计算：对于二项式和高斯分布，可以解析表示：<script type="math/tex; mode=display">
\mu_{bayes} = \frac{\sigma_{obs}^2}{\sigma^2_{pre} +  \sigma_{obs}^2}\mu_{pre} + \frac{\sigma_{pre}^2}{\sigma^2_{pre} +  \sigma_{obs}^2}\mu_{obs}\\
\sigma_{bayes}^2 = \frac{\sigma^2_{pre} \sigma^2_{obs}}{\sigma^2_{pre} + \sigma^2_{obs}}</script>实际上，往往观测值的方差$\sigma^2_{obs}&lt;&lt;\sigma^2_{pre}$,此时 $\mu_{bayes}=\mu_{obs}, \sigma_{bayes}^2 = \sigma^2_{obs}$</li>
</ul>
<h3 id="重要推论："><a href="#重要推论：" class="headerlink" title="重要推论："></a>重要推论：</h3><ul>
<li><p>CNF中的结论</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\frac{d\vec z}{dt}=&\vec f(\vec z(t), t)\\
\vec z(0)\sim p_o(\vec z(0))&~~~\vec z(T) = \vec z(0) + \int _0^T\vec f(\vec z(t), t)dt\\
\frac{d\log p(\vec z(t))}{dt} =& -tr(\frac{df}{dz(t)})
\end{aligned}
\end{equation}</script></li>
<li><p>假设p(u)是多元概率分布，均值为0，协方差为单位阵，则对于任意矩阵A，有</p>
<script type="math/tex; mode=display">
Tr(A)=E_{u\sim p(u)}[u^TAu]</script><p>证明如下</p>
<script type="math/tex; mode=display">
u^TAu = \sum_{j=1}^N\sum_{i=1}^Nu_{i}u_{j}A_{ij}</script><script type="math/tex; mode=display">
\begin{equation}
E[u_iu_j] = \left\{
\begin{aligned}
1,i=j\\
0, i\neq j
\end{aligned}
\right.
\end{equation}</script></li>
<li><p>证明恒等式</p>
</li>
</ul>
<script type="math/tex; mode=display">
\det (exp(A)) = exp(Tr(A))\\
  ln (\det B) = Tr(\ln B)</script>]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ICML 21 paperlist</title>
    <url>/2021/06/06/ICML%202021%20accepted%20paper%20list/</url>
    <content><![CDATA[<h2 id="ICML-2021-accepted-paper-list"><a href="#ICML-2021-accepted-paper-list" class="headerlink" title="ICML 2021 accepted paper list"></a>ICML 2021 accepted paper list<span id="more"></span></h2><h3 id="Differential-equations"><a href="#Differential-equations" class="headerlink" title="Differential equations"></a>Differential equations</h3><ul>
<li><strong>Neural Rough Differential Equations for Long Time Series</strong></li>
<li><strong>STRODE: Stochastic Boundary Ordinary Differential Equation</strong></li>
<li><strong>Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics</strong></li>
<li><strong>Inferring Latent Dynamics Underlying Neural Population Activity via Neural Differential Equations</strong></li>
</ul>
<h3 id="Generative-models"><a href="#Generative-models" class="headerlink" title="Generative models"></a>Generative models</h3><ul>
<li><strong>Generative Adversarial Networks for Markovian Temporal Dynamics: Stochastic Continuous Data Generation</strong></li>
<li><strong>Marginalized Stochastic Natural Gradients for Black-Box Variational Inference</strong></li>
<li><strong>Variational Auto-Regressive Gaussian Processes for Continual Learning</strong></li>
<li><strong>Automatic variational inference with cascading flows</strong></li>
<li><strong>Global inducing point variational posteriors for Bayesian neural networks and deep Gaussian processes</strong></li>
<li><strong>Monte Carlo Variational Auto-Encoders</strong></li>
<li><strong>BasisDeVAE: Interpretable Simultaneous Dimensionality Reduction and Feature-Level Clustering with Derivative-Based Variational Autoencoders</strong></li>
<li><strong>n Signal-to-Noise Ratio Issues in Variational Inference for Deep Gaussian Processes</strong></li>
<li><strong>Generative Particle Variational Inference via Estimation of Functional Gradients</strong></li>
<li><strong>Simple and Effective VAE Training with Calibrated Decoders</strong></li>
<li><strong>Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models</strong></li>
<li><strong>On Energy-Based Models with Overparametrized Shallow Neural Networks</strong></li>
<li><strong>Improved Contrastive Divergence Training of Energy-Based Models</strong></li>
<li><strong>Conjugate Energy-Based Models</strong></li>
<li><strong>Adversarial Purification with Score-based Generative Models</strong></li>
</ul>
<h3 id="Dynamic-systems"><a href="#Dynamic-systems" class="headerlink" title="Dynamic systems"></a>Dynamic systems</h3><ul>
<li><strong>Data-driven prediction of general Hamiltonian dynamics via learning exactly-symplectic maps</strong></li>
<li><strong>Task-Optimal Exploration in Linear Dynamical Systems</strong></li>
<li><strong>Better Training using Weight-Constrained Stochastic Dynamics</strong></li>
<li><strong>Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics</strong></li>
<li><strong>Scalable Learning of Independent Cascade Dynamics from Partial Observations</strong></li>
</ul>
<h3 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h3><ul>
<li><strong>Conformal prediction interval for dynamic time-series</strong></li>
<li><strong>Necessary and sufficient conditions for causal feature selection in time series with latent common causes</strong></li>
<li><strong>Approximation Theory of Convolutional Architectures for Time Series Modelling</strong></li>
<li><strong>Whittle Networks: A Deep Likelihood Model for Time Series</strong></li>
<li><strong>Active Learning of Continuous-time Bayesian Networks throughInterventions</strong></li>
<li><strong>Learning Self-Modulating Attention in Continuous Time Space with Applications to Sequential Recommendation</strong></li>
<li><strong>UnICORNN: A recurrent model for learning very long time dependencies</strong></li>
</ul>
<h3 id="anomaly-detection"><a href="#anomaly-detection" class="headerlink" title="anomaly detection"></a>anomaly detection</h3><ul>
<li><strong>Event Outlier Detection in Continuous Time</strong></li>
</ul>
<h3 id="Control"><a href="#Control" class="headerlink" title="Control"></a>Control</h3><ul>
<li><strong>Policy Analysis using Synthetic Controls in Continuous-Time</strong></li>
<li><strong>Online Optimization in Games via Control Theory: Connecting Regret, Passivity and Poincaré Recurrence</strong></li>
<li><strong>Global Convergence of Policy Gradient for Linear-Quadratic Mean-Field Control/Game in Continuous Time</strong></li>
<li><strong>Model-based Reinforcement Learning for Continuous Control with Posterior Sampling</strong></li>
<li><strong>Dropout: Explicit Forms and Capacity Control</strong></li>
<li><strong>Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards</strong></li>
<li><strong>Deep Coherent Exploration For Continuous Control</strong></li>
</ul>
<h3 id="others"><a href="#others" class="headerlink" title="others"></a>others</h3><ul>
<li><p><strong>Continuous-time Model-based Reinforcement Learning</strong></p>
</li>
<li><p><strong>Training Recurrent Neural Networks via Forward Propagation Through Time</strong></p>
</li>
<li><strong>Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series Forecasting</strong></li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ICML 22 accepted paper list</title>
    <url>/2022/06/19/ICML%202022%20accepted%20paper%20list/</url>
    <content><![CDATA[<h2 id="ICML-22-accepted-papers-list"><a href="#ICML-22-accepted-papers-list" class="headerlink" title="ICML 22 accepted papers list"></a>ICML 22 accepted papers list<span id="more"></span></h2><h3 id="EBM"><a href="#EBM" class="headerlink" title="EBM"></a>EBM</h3><ul>
<li><strong>Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance</strong></li>
<li><strong>Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models</strong></li>
<li><p><strong>Latent Diffusion Energy-Based Model for Interpretable Text Modelling</strong></p>
</li>
<li><p><strong>Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation</strong></p>
</li>
<li><strong>Equivariant Diffusion for Molecule Generation in 3D</strong></li>
<li><strong>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</strong></li>
<li><strong>Diffusion Models for Adversarial Purification</strong></li>
<li><strong>Planning with Diffusion for Flexible Behavior Synthesis</strong></li>
<li><strong>Maximum Likelihood Training for Score-based Diffusion ODEs by High Order Denoising Score Matching</strong></li>
<li><strong>Diffusion bridges vector quantized variational autoencoders</strong></li>
<li><strong>Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations</strong></li>
<li><strong>Score-Guided Intermediate Level Optimization: Fast Langevin Mixing for Inverse Problems</strong></li>
<li><strong>Score matching enables causal discovery of nonlinear additive noise models</strong></li>
</ul>
<h3 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h3><ul>
<li><strong>Deep Variational Graph Convolutional Recurrent Network for Multivariate Time Series Anomaly Detection</strong></li>
<li><strong>Learning of Cluster-based Feature Importance for Electronic Health Record Time-series</strong></li>
<li><strong>Modeling Irregular Time Series with Continuous Recurrent Units</strong></li>
<li><strong>Domain Adaptation for Time Series Forecasting via Attention Sharing</strong></li>
<li><strong>Utilizing Expert Features for Contrastive Learning of Time-Series Representations</strong></li>
<li><strong>Reconstructing nonlinear dynamical systems from multi-modal time series</strong></li>
<li><strong>Adaptive Conformal Predictions for Time Series</strong></li>
<li><strong>TACTiS: Transformer-Attentional Copulas for Time Series</strong></li>
<li><strong>Unsupervised Time-Series Representation Learning with Iterative Bilinear Temporal-Spectral Fusion</strong></li>
<li><strong>DSTAGNN: Dynamic Spatial-Temporal Aware Graph Neural Network for Traffic Flow Forecasting</strong></li>
</ul>
<h3 id="Dynamic-systems"><a href="#Dynamic-systems" class="headerlink" title="Dynamic systems"></a>Dynamic systems</h3><h4 id="ODE-amp-PDE-amp-SDE"><a href="#ODE-amp-PDE-amp-SDE" class="headerlink" title="ODE &amp;PDE&amp;SDE"></a>ODE &amp;PDE&amp;SDE</h4><ul>
<li><strong>Continuous-Time Modeling of Counterfactual Outcomes Using Neural Controlled Differential Equations</strong></li>
<li><strong>Composing Partial Differential Equations with Physics-Aware Neural Networks</strong></li>
<li><strong>On Numerical Integration in Neural Ordinary Differential Equations</strong></li>
<li><strong>Neural Laplace: Learning diverse classes of differential equations in the Laplace domain</strong></li>
<li><strong>Learning Efficient and Robust Ordinary Differential Equations via Invertible Neural Networks</strong></li>
<li><strong>Learning to Solve PDE-constrained Inverse Problems with Graph Networks</strong></li>
</ul>
<h4 id="FLOW"><a href="#FLOW" class="headerlink" title="FLOW"></a>FLOW</h4><ul>
<li><strong>Variational Wasserstein gradient flow</strong></li>
<li><strong>Generative Flow Networks for Discrete Probabilistic Modeling</strong></li>
<li><strong>Matching Normalizing Flows and Probability Paths on Manifolds</strong></li>
<li><strong>ButterflyFlow: Building Invertible Layers with Butterfly Matrices</strong></li>
<li><strong>Path-Gradient Estimators for Continuous Normalizing Flows</strong></li>
<li><strong>Marginal Tail-Adaptive Normalizing Flows</strong></li>
<li><strong>Principal Component Flows</strong></li>
<li><strong>Fast Lossless Neural Compression with Integer-Only Discrete Flows</strong></li>
</ul>
<h4 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h4><ul>
<li><strong>Conditional GANs with Auxiliary Discriminative Classifier</strong></li>
<li><strong>Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling</strong></li>
<li><strong>Structure-preserving GANs</strong></li>
<li><strong>A Neural Tangent Kernel Perspective of GANs</strong></li>
</ul>
<h4 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h4><ul>
<li><strong>SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks</strong></li>
<li><strong>Mitigating modality collapse in multimodal VAEs via impartial optimization</strong></li>
<li><strong>SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization</strong></li>
<li><strong>Bit Prioritization in Variational Autoencoders via Progressive Coding</strong></li>
<li><strong>Gaussian Mixture Variational Autoencoder with Contrastive Learning for Multi-Label Classification</strong></li>
<li><strong>Accelerating Bayesian Optimization for Protein Design with Denoising Autoencoders</strong></li>
<li><strong>Diffusion bridges vector quantized variational autoencoders</strong></li>
</ul>
<h3 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h3><ul>
<li><strong>Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning</strong></li>
<li><strong>Local Augmentation for Graph Neural Networks</strong></li>
<li><strong>G-Mixup: Graph Data Augmentation for Graph Classification</strong></li>
<li><strong>p-Laplacian Based Graph Neural Networks</strong></li>
<li><strong>NAFS: A Simple yet Tough-to-beat Baseline for Graph Representation Learning</strong></li>
<li><strong>Rethinking Graph Neural Networks for Anomaly Detection</strong></li>
<li><strong>A New Perspective on the Effects of Spectrum in Graph Neural Networks</strong></li>
<li><strong>G22CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters</strong></li>
<li><strong>Convergence of Invariant Graph Networks</strong></li>
<li><strong>Graph-Coupled Oscillator Networks</strong></li>
<li><strong>Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling</strong></li>
<li><strong>Cross-Space Active Learning on Graph Convolutional Networks</strong></li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Interesting Things</title>
    <url>/2018/11/04/Interesting-Things/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/dog250/article/details/38958637">密码学的基本原理</a></p>
<p><a href="https://yantijin.github.io/JavaEE">Java基本知识点</a></p>
<h3 id="时间序列异常检测"><a href="#时间序列异常检测" class="headerlink" title="时间序列异常检测"></a>时间序列异常检测</h3><ul>
<li><p>优秀的专栏：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/zr9558">数学人生</a></li>
<li><a href="https://zhuanlan.zhihu.com/liutengfei">机器学习和金融风控</a></li>
</ul>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/51053142">基于自编码器的时间序列异常检测</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Interesting Things</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU透传</title>
    <url>/2020/12/29/GPU%E9%80%8F%E4%BC%A0%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="GPU透传教程"><a href="#GPU透传教程" class="headerlink" title="GPU透传教程"></a>GPU透传教程<span id="more"></span></h2><blockquote>
<p>参考<a href="https://medium.com/@MARatsimbazafy/journey-to-deep-learning-nvidia-gpu-passthrough-to-lxc-container-97d0bc474957">链接</a></p>
</blockquote>
<h3 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h3><blockquote>
<p>新建CT，内存32G，存储1024G，选ubuntu18.04的版本，需要注意将非高优先级选项去掉,假设名字为115</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 首先更换软件源，并更新</span><br><span class="line">apt update</span><br><span class="line">apt upgrade</span><br><span class="line">apt install build-essential</span><br></pre></td></tr></table></figure>
<h3 id="Host配置"><a href="#Host配置" class="headerlink" title="Host配置"></a>Host配置</h3><blockquote>
<p>首先查看host中是否已经安装好驱动了，<code>nvidia-smi</code>,如果有则下面host步骤到装驱动部分都不用再弄。</p>
</blockquote>
<ul>
<li><p>apt update</p>
</li>
<li><p>apt upgrade</p>
</li>
<li><p>apt install vim</p>
</li>
<li><p>apt install build-essential</p>
</li>
<li><p>装驱动:</p>
<ul>
<li><p>禁用nouveau：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/modprobe.d/blacklist-nouveau.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure>
<p>重新生成内核initramfs:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">update-initramfs -u</span><br></pre></td></tr></table></figure>
</li>
<li><p>确认内核版本和headers版本一致，如果不一致则进行更新，并安装 kernel-devel</p>
<ul>
<li>apt install kernel-devel</li>
</ul>
</li>
<li><p>装驱动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget -c https://cn.download.nvidia.com/XFree86/Linux-x86_64/450.80.02/NVIDIA-Linux-x86_64-450.80.02.run</span><br><span class="line">sudo sh ./NVIDIA-Linux-x86_64-450.80.02.run (一路默认选项即可)</span><br><span class="line">apt install nvidia-smi</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>配置驱动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim  /etc/modules-load.d/modules.conf</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># /etc/modules: kernel modules to load at boot time.</span><br><span class="line"></span><br><span class="line"># This file contains the names of kernel modules that should be loaded</span><br><span class="line"># at boot time, one per line. Lines beginning with “#” are ignored.</span><br><span class="line">nvidia</span><br><span class="line">nvidia_uvm</span><br></pre></td></tr></table></figure>
<p>保存退出并更新</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">update-initramfs -u</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看<code>/dev/nvidia</code>中是否包含nvidia-ctl nvidia0和nvidia-uvm等文件，若没有</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">usr/bin/nvidia-smi -L &amp;&amp; /bin/chmod 666 /dev/nvidia*</span><br><span class="line">/usr/bin/nvidia-modprobe -c0 -u &amp;&amp; /bin/chmod 0666 /dev/nvidia-uvm*</span><br></pre></td></tr></table></figure>
</li>
<li><p>重启服务器</p>
</li>
<li><p>查看nvidia-smi看是否安装驱动成功，加上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ modprobe nvidia-uvm</span><br><span class="line">$ ls /dev/nvidia* -l</span><br><span class="line">crw-rw-rw- 1 root root 243, 0 Jan 16 02:20 /dev/nvidia-uvm</span><br><span class="line">crw-rw-rw- 1 root root 195, 0 Jan 16 02:16 /dev/nvidia0</span><br><span class="line">crw-rw-rw- 1 root root 195, 255 Jan 16 02:16 /dev/nvidiactl</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置container文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/pve/lxc/115.conf</span><br></pre></td></tr></table></figure>
<p>加入如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lxc.cgroup.devices.allow: c 195:* rwm</span><br><span class="line">lxc.cgroup.devices.allow: c 243:* rwm</span><br><span class="line">lxc.mount.entry: /dev/nvidia0 dev/nvidia0 none bind,optional,create=file</span><br><span class="line">lxc.mount.entry: /dev/nvidiactl dev/nvidiactl none bind,optional,create=file</span><br><span class="line">lxc.mount.entry: /dev/nvidia-uvm dev/nvidia-uvm none bind,optional,create=file</span><br><span class="line">lxc.mount.entry: /dev/nvidia-modeset dev/nvidia-modeset none bind,optional,create=file</span><br><span class="line">lxc.mount.entry: /dev/nvidia-uvm-tools dev/nvidia-uvm-tools none bind,optional,create=file</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：其中195 243是根据上一步中<code>ls /dev/nvidia* -l</code>的输出中的数值确定的</p>
</li>
<li><p>重启前面创建的容器，并确认/dev/中包含了nvidia0, nvidia-ctl, nvidia-uvm这三项</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls /dev/nvidia* -l</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="容器配置"><a href="#容器配置" class="headerlink" title="容器配置"></a>容器配置</h3><ul>
<li><p>安装cuda</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run</span><br><span class="line">sudo sh cuda_10.2.89_440.33.01_linux.run</span><br></pre></td></tr></table></figure>
<p><strong>注意安装过程中不要选cuda中自带的驱动，切记切记</strong>， 成功之后，添加cuda路径到系统</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>添加如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export PATH=/usr/local/cuda-10.2/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>保存，退出</p>
<p>测试是否成功 <code>nvcc -V</code></p>
</li>
<li><p>安装nvidia-utils</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt install nvidia-utils-450</span><br></pre></td></tr></table></figure>
<p>测试透传是否成功</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="安装jupyter-hub"><a href="#安装jupyter-hub" class="headerlink" title="安装jupyter-hub"></a>安装jupyter-hub</h3><blockquote>
<p>参考 <a href="https://tljh.jupyter.org/en/latest/install/custom-server.html">链接</a></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt install python3 python3-dev git curl</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -L https://tljh.jupyter.org/bootstrap.py | sudo -E python3 - --admin root</span><br></pre></td></tr></table></figure>
<p>Press <code>Enter</code> to start the installation process. This will take 5-10 minutes, and will say ‘Done!’ when the installation process is complete.</p>
<h2 id="GPU集群安装教程"><a href="#GPU集群安装教程" class="headerlink" title="GPU集群安装教程"></a>GPU集群安装教程</h2><ul>
<li>前提，几台CT容器均已配置GPU透传成功，只需要其中一个安装jupyter-hub即可,假设为A,其他两台为B1, B2</li>
</ul>
<h3 id="配置ssh免密登录"><a href="#配置ssh免密登录" class="headerlink" title="配置ssh免密登录"></a>配置ssh免密登录</h3><ul>
<li><p>B1, B2上操作：</p>
<p>#1. 修改sshd配置<br>vim /etc/ssh/sshd_config<br>​<br>#2. 改动如下<br>Port 12345<br>PermitRootLogin yes<br>PubkeyAuthentication yes<br>AuthorizedKeysFile      .ssh/authorized_keys .ssh/authorized_keys2<br>​<br>#3. 保存配置，启动sshd<br>/usr/sbin/sshd<br>​<br>#4. 查看ssh是否启动<br>ps -ef | grep ssh</p>
</li>
<li><p>A上操作:</p>
<p>#1. 生成秘钥，一直回车即可，注意生成秘钥位置<br>ssh-keygen -t rsa<br>​<br>#2. 在B上创建.ssh文件夹<br>ssh -p 12345 B的ip （输入密码）</p>
<p>mkdir -p .ssh<br>​退出B返回到A，输入<code>exit</code></p>
<h1 id="3-将公钥添加到B的authorized-keys里，注意A的秘钥路径是否正确"><a href="#3-将公钥添加到B的authorized-keys里，注意A的秘钥路径是否正确" class="headerlink" title="3. 将公钥添加到B的authorized_keys里，注意A的秘钥路径是否正确"></a>3. 将公钥添加到B的authorized_keys里，注意A的秘钥路径是否正确</h1><p>cat .ssh/id_rsa.pub | ssh -p 12345 B ‘cat &gt;&gt; .ssh/authorized_keys’<br>​<br>#测试是否可以免密登录<br>ssh -p 12345 B的ip</p>
</li>
</ul>
<h3 id="安装syncthing，让三台机器有共享文件目录"><a href="#安装syncthing，让三台机器有共享文件目录" class="headerlink" title="安装syncthing，让三台机器有共享文件目录"></a>安装syncthing，让三台机器有共享文件目录</h3><ul>
<li><p>apt install syncthing</p>
</li>
<li><p>更新到最新版本</p>
</li>
<li><pre><code>vim ~/.config/syncthing/config.xml
</code></pre><p>将里面 <code>0.0.0.0:8384</code>改为<code>127.0.0.1:8384</code></p>
<p>将三台机器设置一个同步的目录</p>
</li>
</ul>
<h3 id="在各机器行安装horovod"><a href="#在各机器行安装horovod" class="headerlink" title="在各机器行安装horovod"></a>在各机器行安装horovod</h3><blockquote>
<p>参考<a href="https://zhuanlan.zhihu.com/p/63158504">链接</a></p>
</blockquote>
<ul>
<li>apt install cmake</li>
<li>安装nccl</li>
<li>安装OpenMPI，并配置相关路径</li>
<li>安装horovod</li>
</ul>
]]></content>
      <tags>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title>Irregular Time Series Papers</title>
    <url>/2020/11/15/Irregular-Time-Series-Papers/</url>
    <content><![CDATA[<p>本文将对于非周期采样时序数据的预测问题做一个初步的总结<span id="more"></span><br>&gt;</p>
<blockquote>
<p>RNN对于周期采样的数据是比较合适的，但是对于非周期采样的时序数据是很难处理的。</p>
<ul>
<li>常见trick是将此问题在预处理层面进行解决，即做聚合/插补—-缺点是会损失一部分信息</li>
<li>类似于ODE-RNN的形式，在观测点对于状态进行调整</li>
</ul>
</blockquote>
<h2 id="latent-odes-for-irregular-sampled-time-series"><a href="#latent-odes-for-irregular-sampled-time-series" class="headerlink" title="latent odes for irregular sampled time series"></a>latent odes for irregular sampled time series</h2><blockquote>
<p>本文提出了两种形式的ODE-RNN</p>
</blockquote>
<p><img src="/2020/11/15/Irregular-Time-Series-Papers/ODE_RNN1.jpg" alt></p>
<ul>
<li>第一种与标准RNN的区别仅在于蓝色高亮的部分，即在观测之间采用ODE来模拟数据的变化，而不是像RNN中观测之间的状态是不变的。</li>
</ul>
<p><img src="/2020/11/15/Irregular-Time-Series-Papers/ODE_RNN2.jpg" alt></p>
<ul>
<li><p>第二种整体是一个seq2seq模型。首先通过编码器得到初始隐变量$z_0$，此时采用neural ode来模拟隐藏状态的变化，得到$z_0, z_1, …,z_N$；最后通过解码器对每一个时刻的隐变量进行解码重构。<strong>需要注意的是，为估计初始的$z_0$, ODE-RNN的编码器是时间反序来求解的</strong></p>
<ul>
<li><p>编码器采用的是$ODE-RNN$模型，并假设初始状态$z_0$分布为</p>
<script type="math/tex; mode=display">
q(z_0|\{x_i, t_i\}_{i=0}^N)=\mathcal{N}(\mu_{z_0}, \sigma_{z_0})</script><p>其中， $\mu_{z_0}, \sigma_{z_0}=g(ODE-RNN(\{x_i,t_i\}_{i=0}^N))$, g为神经网络。</p>
</li>
<li><p>对应此种形式的损失函数为：</p>
<script type="math/tex; mode=display">
\text{ELBO}(\theta, \phi) = \mathbb{E}_{z_0\sim q_{\phi(z_0|\{x_i, t_i\}_{i=0}^N)}}[\log p_{\theta}(x_0, ...,x_N)] - \text{KL}[q_{\phi}(z_0|\{x_i,t_i\}_{i=0}^N)||p(z_0)]</script></li>
</ul>
<blockquote>
<p>上述通过隐变量来重构的框架有以下几个优点：</p>
<ul>
<li>显式得将ODE系统的动态特性，观测似然值以及辨识模型分别开来，因此可以有效区分每一部分的作用</li>
<li>采用VAE的框架，能够自然的建模不确定性</li>
<li>对于非周期采样的时序数据建模提供了便捷性</li>
</ul>
</blockquote>
</li>
<li><p>更多的观测意味着更多的信息，也就是说采样的频率在一定程度上能够反映数据的重要性。这是一个典型的counting process，假设他是一个非齐次的泊松过程，对应的强度函数为 $\lambda(z(t))$(即这里假设$\lambda$是z(t)的函数)，则可以推导出</p>
<script type="math/tex; mode=display">
\log p(t_1, t_2, ...,t_N|t_{starrt}, t_{end}) = \sum^N_{i=1}\text{log}\lambda(z(t_i)) - \int^{t_{end}}_{t_{start}}\lambda(z(t))dt</script><blockquote>
<p>推导过程请参考<a href="https://math.stackexchange.com/questions/344487/log-likelihood-of-a-realization-of-a-poisson-process">refer</a></p>
</blockquote>
</li>
</ul>
<h2 id="Learning-Long-Term-Dependencies-in-Irregularly-Sampled-Time-Series"><a href="#Learning-Long-Term-Dependencies-in-Irregularly-Sampled-Time-Series" class="headerlink" title="Learning Long Term Dependencies in Irregularly Sampled Time Series"></a>Learning Long Term Dependencies in Irregularly Sampled Time Series</h2><blockquote>
<p>ODE_RNN无法解决长时依赖问题，根本原因是ODE得到的隐藏状态会产生梯度消失会爆炸现象，而后提出ODE-LSTM的基本框架，讲记忆单元和对应的连续时间态进行显式分离，并实验真证明了有效性。</p>
</blockquote>
<p><img src="/2020/11/15/Irregular-Time-Series-Papers/ODE_LSTM.jpg" alt></p>
<h2 id="GRU-ODE-Bayes-Continuous-Modeling-of-Sporadically-Observed-Time-Series"><a href="#GRU-ODE-Bayes-Continuous-Modeling-of-Sporadically-Observed-Time-Series" class="headerlink" title="GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series"></a>GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series</h2><blockquote>
<p>见 Dynamic systems for Deep Learning <a href="./Dynamic-Systems-for-Deep-Learning.md"></a></p>
</blockquote>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>Irregular Sampled TS</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>NIPS 20 paperlist</title>
    <url>/2020/11/15/NIPS20-paperlist/</url>
    <content><![CDATA[<h2 id="NeuraIPS’20"><a href="#NeuraIPS’20" class="headerlink" title="NeuraIPS’20"></a>NeuraIPS’20<span id="more"></span></h2><h3 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h3><ul>
<li><strong>Probabilistic Time Series Forecasting with Shape and Temporal Diversity</strong></li>
<li><strong>Deep reconstruction of strange attractors from time series</strong></li>
<li><strong>Neural Controlled Differential Equations for Irregular Time Series</strong></li>
<li><strong>Adversarial Sparse Transformer for Time Series Forecasting</strong></li>
<li><strong>Learning Long-Term Dependencies in Irregularly-Sampled Time Series</strong></li>
<li><strong>Benchmarking Deep Learning Interpretability in Time Series Predictions</strong></li>
<li><strong>High-recall causal discovery for autocorrelated time series with latent confounders</strong></li>
<li><strong>Deep Rao-Blackwellised Particle Filters for Time Series Forecasting</strong></li>
<li><strong>Normalizing Kalman Filters for Multivariate Time Series Analysis</strong></li>
<li><strong>Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting</strong></li>
<li><strong>Learning Continuous System Dynamics from Irregularly-Sampled Partial Observations</strong></li>
<li><strong>Gamma-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction</strong></li>
</ul>
<h3 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h3><ul>
<li><strong>Open Graph Benchmark: Datasets for Machine Learning on Graphs</strong></li>
<li><strong>Path Integral Based Convolution and Pooling for Graph Neural Networks</strong></li>
<li><strong>Stochastic Deep Gaussian Processes over Graphs</strong></li>
<li><strong>Scalable Graph Neural Networks via Bidirectional Propagation</strong></li>
<li><strong>Set2Graph: Learning Graphs From Sets</strong></li>
<li><strong>Multipole Graph Neural Operator for Parametric Partial Differential Equations</strong></li>
<li><strong>Parameterized Explainer for Graph Neural Network</strong></li>
<li><strong>Random Walk Graph Neural Networks</strong></li>
<li><strong>Dirichlet Graph Variational Autoencoder</strong></li>
<li><strong>Natural Graph Networks</strong></li>
<li><strong>Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks</strong></li>
<li><strong>Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings</strong></li>
<li><strong>Unsupervised Joint k-node Graph Representations with Compositional Energy-Based Models</strong></li>
<li><strong>PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks</strong></li>
<li><strong>Implicit Graph Neural Networks</strong></li>
<li><p><strong>Towards Deeper Graph Neural Networks with Differentiable Group Normalization</strong></p>
</li>
<li><p><strong>COPT: Coordinated Optimal Transport on Graphs</strong></p>
</li>
<li><strong>Less is More: A Deep Graph Metric Learning Perspective Using Few Proxies</strong></li>
<li><strong>Probabilistic Circuits for Variational Inference in Discrete Graphical Models</strong></li>
<li><strong>Deep Relational Topic Modeling via Graph Poisson Gamma Belief Network</strong></li>
<li><strong>Graph Stochastic Neural Networks for Semi-supervised Learning</strong></li>
<li><strong>Community detection in sparse time-evolving graphs with a dynamical Bethe-Hessian</strong></li>
<li><strong>Graph Random Neural Networks for Semi-Supervised Learning on Graphs</strong></li>
<li><strong>Learning of Discrete Graphical Models with Neural Networks</strong></li>
<li><strong>Building powerful and equivariant graph neural networks with message-passing</strong></li>
<li><strong>Manifold structure in graph embeddings</strong></li>
<li><p><strong>Attribution for Graph Neural Networks</strong></p>
</li>
<li><p><strong>Factorizable Graph Convolutional Networks</strong></p>
</li>
<li><strong>Handling Missing Data with Graph Representation Learning</strong></li>
<li><p><strong>EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning</strong></p>
</li>
<li><p><strong>Design Space for Graph Neural Networks</strong></p>
</li>
<li><strong>Pointer Graph Networks</strong></li>
<li><strong>Reward Propagation Using Graph Convolutional Networks</strong></li>
<li><strong>Subgraph Neural Networks</strong></li>
<li><p><strong>Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</strong></p>
</li>
<li><p><strong>Efficient Learning of Discrete Graphical Models</strong></p>
</li>
<li><strong>Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting</strong></li>
<li><strong>Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks</strong></li>
<li><strong>Scattering GCN: Overcoming Oversmoothness in Graph Convolutional Networks</strong></li>
<li><strong>Pre-Training Graph Neural Networks: A Contrastive Learning Framework with Augmentations</strong></li>
<li><strong>Variational Inference for Graph Convolutional Networks in the Absence of Graph Data and Adversarial Settings</strong></li>
<li><strong>A Novel Approach for Constrained Optimization in Graphical Models</strong></li>
<li><strong>Adversarially-learned Inference via an Ensemble of Discrete Undirected Graphical Models</strong></li>
<li><strong>Nonconvex Sparse Graph Learning under Laplacian-structured Graphical Model</strong></li>
<li><strong>Graph Geometry Interaction Learning</strong></li>
<li><strong>Bandit Samplers for Training Graph Neural Networks</strong></li>
<li><strong>Factor Graph Neural Networks</strong></li>
<li><strong>DiffGCN: Graph Convolutional Networks via Differential Operators and Algebraic Multigrid Pooling</strong></li>
<li><strong>Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting</strong></li>
<li><strong>Curvature Regularization to Prevent Distortion in Graph Embedding</strong></li>
<li><strong>Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs</strong></li>
<li><strong>Higher-Order Spectral Clustering of Directed Graphs</strong></li>
<li><strong>Iterative Deep Graph Learning for Graph NeuralNetworks: Better and Robust Node Embeddings</strong></li>
<li><strong>Graph Information Bottleneck</strong></li>
<li><strong>Rethinking pooling in graph neural networks</strong></li>
</ul>
<h2 id="连续-Flow"><a href="#连续-Flow" class="headerlink" title="连续+Flow"></a>连续+Flow</h2><ul>
<li><strong>Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows</strong></li>
<li><strong>Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics</strong></li>
<li><strong>Riemannian Continuous Normalizing Flows</strong></li>
<li><strong>Continual Deep Learning by Functional Regularisation of Memorable Past</strong></li>
<li><strong>Scalable and Consistent Estimation in Continuous-time Networks of Relational Events</strong></li>
<li><strong>DAGs with No Fears: A Closer Look at Continuous Optimization for Learning Bayesian Networks</strong></li>
<li><strong>Hypersolvers: Toward Fast Continuous-Depth Models</strong></li>
<li><strong>Learning Continuous System Dynamics from Irregularly-Sampled Partial Observations</strong></li>
<li><strong>A mathematical model for automatic differentiation in machine learning</strong></li>
<li><strong>Neural Manifold Ordinary Differential Equations</strong></li>
<li><strong>Multipole Graph Neural Operator for Parametric Partial Differential Equations</strong></li>
<li><strong>Learning Differential Equations that are Fast to Solve</strong></li>
<li><strong>Interpolation technique to speed up gradients propagation in Neural Ordinary Differential Equations</strong></li>
<li><strong>JAX MD: A Framework for Differentiable Physics</strong></li>
<li><strong>Training Generative Adversarial Networks by Solving Ordinary Differential Equations</strong></li>
<li><strong>Numerically Solving Parametric Families of High-Dimensional Kolmogorov Partial Differential Equations via Deep Learning</strong></li>
<li><strong>Improved Variational Bayesian Phylogenetic Inference with Normalizing Flows</strong></li>
<li><strong>Flows for simultaneous manifold learning and density estimation</strong></li>
<li><strong>Wavelet Flow: Fast Training of High Resolution Normalizing Flows</strong></li>
<li><strong>Advances in Black-Box VI: Normalizing Flows, Importance Weighting, and Optimization</strong></li>
<li><strong>Woodbury Transformations for Deep Generative Flows</strong></li>
<li><strong>SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds</strong></li>
<li><p><strong>Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</strong></p>
</li>
<li><p><strong>The Convolution Exponential and Generalized Sylvester Flows</strong></p>
</li>
<li><strong>NanoFlow: scalable normalizing flows with sublinear parameter complexity</strong></li>
<li><strong>Training Normalizing Flows with the Information Bottleneck for Competitive Generative Classification</strong></li>
<li><strong>Log-Likelihood Ratio Minimizing Flows: Towards Robust and Quantifiable Neural Distribution Alignment</strong></li>
<li><strong>Why Normalizing Flows Fail to Detect Out-of-Distribution Data</strong></li>
<li><strong>Stochastic Normalizing Flows</strong></li>
<li><strong>SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows</strong></li>
<li><strong>Closing the Dequantization Gap: PixelCNN as a Single-Layer Flow</strong></li>
<li><strong>SVGD as a kernelized gradient flow of the chi-squared divergence</strong></li>
<li><strong>Gradient Boosted Normalizing Flows</strong></li>
</ul>
<h2 id="ICLR2021投稿"><a href="#ICLR2021投稿" class="headerlink" title="ICLR2021投稿"></a>ICLR2021投稿</h2><h3 id="ODE-Flow-Dyna"><a href="#ODE-Flow-Dyna" class="headerlink" title="ODE + Flow + Dyna"></a>ODE + Flow + Dyna</h3><ul>
<li><p><a href="https://openreview.net/forum?id=DlPnp5_1JMI">Neural Partial Differential Equations </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=Q1jmmQz72M2">Neural Delay Differential Equations </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=c_YKhwEdUcq">A neural method for symbolically solving partial differential equations</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=JFKR3WqwyXR">Neural Jump Ordinary Differential Equation </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=bzVsk7bnGdh">“Hey, that’s not an ODE’”: Faster ODE Adjoints with 12 Lines of Code</a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=dcktlmtcM7">Neural Time-Dependent Partial Differential Equation</a></p>
</li>
<li><h4 id="Score-Based-Generative-Modeling-through-Stochastic-Differential-Equations"><a href="#Score-Based-Generative-Modeling-through-Stochastic-Differential-Equations" class="headerlink" title="Score-Based Generative Modeling through Stochastic Differential Equations "></a><a href="https://openreview.net/forum?id=PxTIG12RRHS">Score-Based Generative Modeling through Stochastic Differential Equations </a></h4></li>
<li><h4 id="Neural-Partial-Differential-Equations-with-Functional-Convolution"><a href="#Neural-Partial-Differential-Equations-with-Functional-Convolution" class="headerlink" title="Neural Partial Differential Equations with Functional Convolution"></a><a href="https://openreview.net/forum?id=D4A-v0kltaX">Neural Partial Differential Equations with Functional Convolution</a></h4></li>
<li><h4 id="Learning-Neural-Event-Functions-for-Ordinary-Differential-Equations"><a href="#Learning-Neural-Event-Functions-for-Ordinary-Differential-Equations" class="headerlink" title="Learning Neural Event Functions for Ordinary Differential Equations"></a><a href="https://openreview.net/forum?id=kW_zpEmMLdP">Learning Neural Event Functions for Ordinary Differential Equations</a></h4></li>
<li><h4 id="Fourier-Neural-Operator-for-Parametric-Partial-Differential-Equations"><a href="#Fourier-Neural-Operator-for-Parametric-Partial-Differential-Equations" class="headerlink" title="Fourier Neural Operator for Parametric Partial Differential Equations "></a><a href="https://openreview.net/forum?id=c8P9NQVtmnO">Fourier Neural Operator for Parametric Partial Differential Equations </a></h4></li>
<li><h4 id="Learning-continuous-time-PDEs-from-sparse-data-with-graph-neural-networks"><a href="#Learning-continuous-time-PDEs-from-sparse-data-with-graph-neural-networks" class="headerlink" title="Learning continuous-time PDEs from sparse data with graph neural networks"></a><a href="https://openreview.net/forum?id=aUX5Plaq7Oy">Learning continuous-time PDEs from sparse data with graph neural networks</a></h4></li>
<li><h4 id="Parameterized-Pseudo-Differential-Operators-for-Graph-Convolutional-Neural-Networks"><a href="#Parameterized-Pseudo-Differential-Operators-for-Graph-Convolutional-Neural-Networks" class="headerlink" title="Parameterized Pseudo-Differential Operators for Graph Convolutional Neural Networks "></a><a href="https://openreview.net/forum?id=Y45i-hDynr">Parameterized Pseudo-Differential Operators for Graph Convolutional Neural Networks </a></h4></li>
<li><h4 id="Neural-CDEs-for-Long-Time-Series-via-the-Log-ODE-Method"><a href="#Neural-CDEs-for-Long-Time-Series-via-the-Log-ODE-Method" class="headerlink" title="Neural CDEs for Long Time Series via the Log-ODE Method "></a><a href="https://openreview.net/forum?id=65MxtdJwEnl">Neural CDEs for Long Time Series via the Log-ODE Method </a></h4></li>
<li><h4 id="Neural-ODE-Processes"><a href="#Neural-ODE-Processes" class="headerlink" title="Neural ODE Processes"></a><a href="https://openreview.net/forum?id=27acGyyI1BY">Neural ODE Processes</a></h4></li>
<li><h4 id="Neural-SDEs-Made-Easy-SDEs-are-Infinite-Dimensional-GANs"><a href="#Neural-SDEs-Made-Easy-SDEs-are-Infinite-Dimensional-GANs" class="headerlink" title="Neural SDEs Made Easy: SDEs are Infinite-Dimensional GANs"></a><a href="https://openreview.net/forum?id=padYzanQNbg">Neural SDEs Made Easy: SDEs are Infinite-Dimensional GANs</a></h4></li>
<li><h4 id="Multiplicative-Filter-Networks"><a href="#Multiplicative-Filter-Networks" class="headerlink" title="Multiplicative Filter Networks"></a><a href="https://openreview.net/forum?id=OmtmcPkkhT">Multiplicative Filter Networks</a></h4></li>
<li><h4 id="ResNet-After-All-Neural-ODEs-and-Their-Numerical-Solution"><a href="#ResNet-After-All-Neural-ODEs-and-Their-Numerical-Solution" class="headerlink" title="ResNet After All: Neural ODEs and Their Numerical Solution "></a><a href="https://openreview.net/forum?id=HxzSxSxLOJZ">ResNet After All: Neural ODEs and Their Numerical Solution </a></h4></li>
<li><h4 id="Go-with-the-flow-Adaptive-control-for-Neural-ODEs"><a href="#Go-with-the-flow-Adaptive-control-for-Neural-ODEs" class="headerlink" title="Go with the flow: Adaptive control for Neural ODEs "></a><a href="https://openreview.net/forum?id=giit4HdDNa">Go with the flow: Adaptive control for Neural ODEs </a></h4></li>
<li><h4 id="MALI-A-memory-efficient-and-reverse-accurate-integrator-for-Neural-ODEs"><a href="#MALI-A-memory-efficient-and-reverse-accurate-integrator-for-Neural-ODEs" class="headerlink" title="MALI: A memory efficient and reverse accurate integrator for Neural ODEs "></a><a href="https://openreview.net/forum?id=blfSjHeFM_e">MALI: A memory efficient and reverse accurate integrator for Neural ODEs </a></h4></li>
<li><h4 id="Deep-Continuous-Networks"><a href="#Deep-Continuous-Networks" class="headerlink" title="Deep Continuous Networks "></a><a href="https://openreview.net/forum?id=L5b6jUonKFB">Deep Continuous Networks </a></h4></li>
<li><h4 id="Learning-Continuous-Time-Dynamics-by-Stochastic-Differential-Networks"><a href="#Learning-Continuous-Time-Dynamics-by-Stochastic-Differential-Networks" class="headerlink" title="Learning Continuous-Time Dynamics by Stochastic Differential Networks "></a><a href="https://openreview.net/forum?id=U850oxFSKmN">Learning Continuous-Time Dynamics by Stochastic Differential Networks </a></h4></li>
<li><h4 id="Implicit-Normalizing-Flows"><a href="#Implicit-Normalizing-Flows" class="headerlink" title="Implicit Normalizing Flows "></a><a href="https://openreview.net/forum?id=8PS8m9oYtNy">Implicit Normalizing Flows </a></h4></li>
<li><h4 id="Directional-graph-networks"><a href="#Directional-graph-networks" class="headerlink" title="Directional graph networks"></a><a href="https://openreview.net/forum?id=FUdBF49WRV1">Directional graph networks</a></h4></li>
</ul>
<h3 id="TS"><a href="#TS" class="headerlink" title="TS"></a>TS</h3><ul>
<li><p><a href="https://openreview.net/forum?id=JVs1OrQgR3A">Time Series Counterfactual Inference with Hidden Confounders </a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=vLaHRtHvfFp">PDE-Driven Spatiotemporal Disentanglement </a></p>
</li>
<li><h4 id="Neural-Spatio-Temporal-Point-Processes"><a href="#Neural-Spatio-Temporal-Point-Processes" class="headerlink" title="Neural Spatio-Temporal Point Processes"></a><a href="https://openreview.net/forum?id=XQQA6-So14">Neural Spatio-Temporal Point Processes</a></h4></li>
<li><h4 id="Anomaly-detection-in-dynamical-systems-from-measured-time-series"><a href="#Anomaly-detection-in-dynamical-systems-from-measured-time-series" class="headerlink" title="Anomaly detection in dynamical systems from measured time series"></a><a href="https://openreview.net/forum?id=Whq-nTgCbNR">Anomaly detection in dynamical systems from measured time series</a></h4></li>
<li><h4 id="Cubic-Spline-Smoothing-Compensation-for-Irregularly-Sampled-Sequences"><a href="#Cubic-Spline-Smoothing-Compensation-for-Irregularly-Sampled-Sequences" class="headerlink" title="Cubic Spline Smoothing Compensation for Irregularly Sampled Sequences "></a><a href="https://openreview.net/forum?id=muu0gF6BW-">Cubic Spline Smoothing Compensation for Irregularly Sampled Sequences </a></h4></li>
<li><h4 id="Generative-Time-series-Modeling-with-Fourier-Flows"><a href="#Generative-Time-series-Modeling-with-Fourier-Flows" class="headerlink" title="Generative Time-series Modeling with Fourier Flows "></a><a href="https://openreview.net/forum?id=PpshD0AXfA">Generative Time-series Modeling with Fourier Flows </a></h4></li>
</ul>
<h3 id="GNN-1"><a href="#GNN-1" class="headerlink" title="GNN"></a>GNN</h3><ul>
<li><h4 id="Generalizing-Graph-Convolutional-Networks"><a href="#Generalizing-Graph-Convolutional-Networks" class="headerlink" title="Generalizing Graph Convolutional Networks "></a><a href="https://openreview.net/forum?id=yBJihVXahXc">Generalizing Graph Convolutional Networks </a></h4></li>
<li><h4 id="Polynomial-Graph-Convolutional-Networks"><a href="#Polynomial-Graph-Convolutional-Networks" class="headerlink" title="Polynomial Graph Convolutional Networks "></a><a href="https://openreview.net/forum?id=uqD-un_Mzd-">Polynomial Graph Convolutional Networks </a></h4></li>
</ul>
<h3 id="MPC"><a href="#MPC" class="headerlink" title="MPC"></a>MPC</h3><ul>
<li><h4 id="Neural-Lyapunov-Model-Predictive-Control"><a href="#Neural-Lyapunov-Model-Predictive-Control" class="headerlink" title="Neural Lyapunov Model Predictive Control"></a><a href="https://openreview.net/forum?id=N5Zacze7uru">Neural Lyapunov Model Predictive Control</a></h4></li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>KDD 21 paperlist</title>
    <url>/2021/06/21/KDD2021%20Accepted%20Paper%20List/</url>
    <content><![CDATA[<h2 id="KDD2021-Accepted-Paper-List"><a href="#KDD2021-Accepted-Paper-List" class="headerlink" title="KDD2021 Accepted Paper List"></a>KDD2021 Accepted Paper List<span id="more"></span></h2><h3 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h3><ul>
<li>Dynamic and Multi-faceted Spatio-temporal Deep Learning for Traffic Speed Forecasting</li>
<li>Forecasting Interaction Order on Temporal Graphs</li>
<li>Quantifying Uncertainty in Deep Spatiotemporal Forecasting</li>
<li>Spatial-Temporal Graph ODE Networks for Traffic Flow Forecasting</li>
<li>ST-Norm: Spatial and Temporal Normalization for Multi-variate Time Series Forecasting</li>
<li>TrajNet: A Trajectory-Based Deep Learning Model for Traffic Prediction</li>
</ul>
<ul>
<li>ELITE : Robust Deep Anomaly Detection with Meta Gradient</li>
<li>Multivariate Time Series Anomaly Detection and Interpretation using Hierarchical Inter-Metric and Temporal Embedding</li>
<li>Practical Approach to Asynchronous Multivariate Time Series Anomaly Detection and Localization.</li>
<li>Time Series Anomaly Detection for Cyber-physical Systems via Neural System Identification and Bayesian Filtering</li>
</ul>
<ul>
<li>Apriori Convolutions for Highly Efficient and Accurate Time Series Classification</li>
</ul>
<ul>
<li>Fast and Accurate Partial Fourier Transform for Time Series Data</li>
<li>Representation Learning of Multivariate Time Series using a Transformer Framework</li>
<li>Statistical models coupling allows for complex local multivariate time series analysis</li>
<li>Causal and Interpretable Rules for Time Series Analysis</li>
</ul>
<h3 id="Graph-Neural-Networks"><a href="#Graph-Neural-Networks" class="headerlink" title="Graph Neural Networks"></a>Graph Neural Networks</h3><ul>
<li>Coupled Graph ODE for Learning Interacting System Dynamics</li>
</ul>
<h3 id="Differential-equations"><a href="#Differential-equations" class="headerlink" title="Differential equations"></a>Differential equations</h3><ul>
<li>ACE-NODE: Attentive Co-Evolving Neural Ordinary Differential Equations</li>
</ul>
<h3 id="Dynamic-systems"><a href="#Dynamic-systems" class="headerlink" title="Dynamic systems"></a>Dynamic systems</h3><ul>
<li>Dynamic Hawkes Processes for Discovering Time-evolving Communities’ States behind Diffusion Processes</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ODE基础</title>
    <url>/2020/09/23/ODE%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>在学习常微分方程之前，我们先了解一些基本的概念；我们在中学的时候都学过解方程（如： <img src="https://www.zhihu.com/equation?tex=x%5E2%2B5%3D0" alt="[公式]"> ），不过那都是函数方程（ <img src="https://www.zhihu.com/equation?tex=f%28x%29%3D0" alt="[公式]"> ，即含有未知数x的方程）。</p>
<p>因此，我们就引出了一个新的概念，什么是微分方程？</p>
<p>（1）微分方程：未知函数及其<strong>导数或微分</strong>的关系式，如： <img src="https://www.zhihu.com/equation?tex=y%27%2B5y%3D3x+%28y%3Df%28x%29%29" alt="[公式]"> ）。</p>
<p>由此，我们便可知道函数方程是关于未知数x的，而微分方程是关于x的导数或微分的；微分方程的英文名叫differential equation，简称D.E.。</p>
<p>我们这一章复习的是常微分方程，什么是常微分方程呢，这个“常”又代表什么呢？</p>
<p>（2）常微分方程：<strong>一个</strong>未知函数及其导数或微分的关系式， 如： <img src="https://www.zhihu.com/equation?tex=f%27%28x%29-7f%28x%29%3D0" alt="[公式]"> 。</p>
<p>这个“常”（Ordinary）表示平常，也就是在一般情况（<strong>理想情况</strong>）下的微分方程，即只有一个未知函数；正是因为如此，所以我们在尚未进行特殊说明的情况下，默认D.E.表示常微分方程。</p>
<p>既然我们已经知道了什么是常微分方程了，那么它的解又是什么呢？（有问题，就应该有答案啊）</p>
<p>（3）解：能使D.E的关系式恒成立的函数，形如 <img src="https://www.zhihu.com/equation?tex=y%3Df%28x%29" alt="[公式]"> 。</p>
<p>先回顾一下我们熟悉的函数方程，它的解是什么？是满足函数关系式的未知数，也就是 <img src="https://www.zhihu.com/equation?tex=x%3DC" alt="[公式]"> （C一般为常数）；不难推出D.E.的解也是要满足关系式，是长成 <img src="https://www.zhihu.com/equation?tex=y%3Df%28x%29" alt="[公式]"> 这个样子。</p>
<p>（4）通解：带有常数C的解，如 ：<img src="https://www.zhihu.com/equation?tex=y%3DC_1x%5E2%2BC_2e%5Ex" alt="[公式]"> （有两项）。</p>
<p>（5）通积分：隐函数形式的通解，如： <img src="https://www.zhihu.com/equation?tex=f%28y%29+%3D+C_1x%5E2+%2B+C_2e%5Ex" alt="[公式]"> 。</p>
<p>注：某D.E.通解的项和该D.E.的阶相同，即二阶D.E.的通解就有两项，下文会介绍什么是阶。</p>
<p>① <img src="https://www.zhihu.com/equation?tex=%28y%27%29%5E2+%2B+p%28x%29y+%3D+0" alt="[公式]"> 和② <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+f%28x%29" alt="[公式]"> 有什么区别呢？ </p>
<p>（5）阶：D.E.中的<strong>最高阶导数</strong>或微分称为这个D.E的阶，如：①是1阶，②是2阶。</p>
<p>（6）齐次：D.E.中不含未知函数和常量，如：①是齐次，②是非齐。</p>
<p>（7）线性：D.E中的导数或微分<strong>只有一次</strong>，如：①是二次，②是一次。</p>
<p>注：“线性”这个概念在代数上一般都是指一次，如： <img src="https://www.zhihu.com/equation?tex=y%3D3x" alt="[公式]"> 就是线性的，而 <img src="https://www.zhihu.com/equation?tex=y%3Dx%5E2" alt="[公式]"> 是非线性的；从几何上看也很好理解， <img src="https://www.zhihu.com/equation?tex=y%3D3x" alt="[公式]"> 是一条直线，而 <img src="https://www.zhihu.com/equation?tex=y%3Dx%5E2" alt="[公式]"> 是一条曲线。</p>
<p>（8）定解条件：其实就是一些已知信息，能用来确定解中的常数C。</p>
<p>（9）初始条件：定解条件中最常见的一种，给出初始条件的问题称为初值问题，也叫<strong>柯西问题</strong>。</p>
<p>（10）特解：常数C确定后的通解，称为特解，比如由定解条件确定常数C。</p>
<p>最后，我们来看一下D.E.的两种表现形式。</p>
<p>（11）显函数形式： <img src="https://www.zhihu.com/equation?tex=y%5E%7B%28n%29%7D+%3D+f%28x%2Cy%2Cy%27%2Cy%27%27%2C...%2Cy%5E%7B%28n-1%29%7D%29" alt="[公式]"> </p>
<p>（12）隐函数形式： <img src="https://www.zhihu.com/equation?tex=f%28x%2Cy%2Cy%27%2Cy%27%27%2C...%2Cy%5E%7B%28n%29%7D%29+%3D+0" alt="[公式]"> </p>
<h2 id="二、求解D-E"><a href="#二、求解D-E" class="headerlink" title="二、求解D.E."></a>二、求解D.E.</h2><p>好，我们现在已经了解了有关常微分方程的基本概念，那么我们应该如何去求一个常微分方程的解呢？（一般是求通解，在特定条件下会是求特解）</p>
<p>为了能快速的求解D.E.，通常把D.E分为以下几个类型。</p>
<p>（1）<strong>可分离变量型</strong>：形如 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bdy%7D%7Bdx%7D+%3D+p%28y%29q%28x%29" alt="[公式]"> </p>
<p>首先，分离变量： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bp%28y%29%7Ddy+%3D+q%28x%29dx" alt="[公式]"> </p>
<p>然后，两边积分： <img src="https://www.zhihu.com/equation?tex=%5Cint%5Cfrac%7B1%7D%7Bp%28y%29%7Ddy+%3D+%5Cint+q%28x%29dx+%2BC" alt="[公式]"> （<strong>注意</strong>：自变量一端加常数C，而不定积分的结果就不要加C了）</p>
<p>最后，计算结果。</p>
<p>（2）<strong>齐次型</strong>：形如 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bdy%7D%7Bdx%7D+%3D+f%28%5Cfrac+yx%29" alt="[公式]"> </p>
<p>首先，换元法：设 <img src="https://www.zhihu.com/equation?tex=u%3D%5Cfrac+yx" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=y%3Dxu%2C+%5Cfrac%7Bdy%7D%7Bdx%7D+%3D+%28xu%29%27+%3D+u%2Bx%5Cfrac%7Bdu%7D%7Bdx%7D" alt="[公式]"> </p>
<p>然后，原式就变成： <img src="https://www.zhihu.com/equation?tex=u%2Bx%5Cfrac%7Bdu%7D%7Bdx%7D+%3D+f%28u%29" alt="[公式]"> </p>
<p>现在就和分离变量型一样了，如法炮制。</p>
<p>就先，分离变量： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bf%28u%29-u%7Ddu+%3D+%5Cfrac+1x+dx" alt="[公式]"> </p>
<p>接着，两边积分： <img src="https://www.zhihu.com/equation?tex=%5Cint%5Cfrac%7B1%7D%7Bf%28u%29-u%7Ddu+%3D+%5Cint%5Cfrac+1x+dx+%2B+C" alt="[公式]"> </p>
<p>最后，计算结果。</p>
<p>（3）<strong>一阶齐次线性型</strong>：形如 <img src="https://www.zhihu.com/equation?tex=y%27+%2B+p%28x%29y+%3D+0" alt="[公式]"> </p>
<p>直接套用公式： <img src="https://www.zhihu.com/equation?tex=y%3DCe%5E%7B-%5Cint+p%28x%29dx%7D" alt="[公式]"> </p>
<p>下面我们来推导这个公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=y%27+%2B+p%28x%29y+%3D+0%5CRightarrow+%5Cfrac%7Bdy%7D%7Bdx%7D%3D-p%28x%29y%5CRightarrow+%5Cfrac+1ydy+%3D+-p%28x%29dx%5CRightarrow+%5Cint%5Cfrac+1ydy+%3D+%5Cint+-p%28x%29dx%2BC_1" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5CRightarrow+ln%7Cy%7C+%3D+%5Cint+-p%28x%29dx%2BC_1%5CRightarrow+%7Cy%7C+%3D+e%5E%7BC_1%7De%5E%5Cint+-p%28x%29dx%5CRightarrow+y+%3D+%5Cpm+e%5E%7BC_1%7De%5E%5Cint+-p%28x%29dx" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5CRightarrow+y+%3D+Ce%5E%5Cint+-p%28x%29dx" alt="[公式]"> </p>
<p>（4）<strong>一阶非齐线性型</strong>：形如 <img src="https://www.zhihu.com/equation?tex=y%27+%2B+p%28x%29y+%3D+q%28x%29" alt="[公式]"> </p>
<p>直接套用公式： <img src="https://www.zhihu.com/equation?tex=y%3D%28%5Cint+q%28x%29e%5E%7B%5Cint+p%28x%29dx%7D%2BC%29e%5E%7B-%5Cint+p%28x%29dx%7D" alt="[公式]"> </p>
<p>这里就不推导了，麻烦。</p>
<p>上面我们求解的都是一阶D.E.，下面我们来看看高阶D.E如何求解（主要以二阶为例）。</p>
<p>（5）<strong>可降阶型（双缺）</strong>：形如 <img src="https://www.zhihu.com/equation?tex=y%5E%7B%28n%29%7D+%3D+f%28x%29" alt="[公式]"> </p>
<p>由基本概念中“某D.E.通解的项和该D.E.的阶相同”可知， <img src="https://www.zhihu.com/equation?tex=y%5E%7B%28n%29%7D+%3D+f%28x%29" alt="[公式]"> 的解为：</p>
<p><img src="https://www.zhihu.com/equation?tex=y%27+%3D+f%28x%29%5CRightarrow+y+%3D+%5Cint+f%28x%29dx+%2BC" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=y%27%27+%3D+f%28x%29%5CRightarrow+y%27%3D+%5Cint+f%28x%29dx+%2B+C_1%5CRightarrow+y+%3D+%5Ciint+f%28x%29dx%5E2+%2BC_1x+%2B+C_2%5C" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=y%27%27%27+%3D+f%28x%29%5CRightarrow+y%27%27%3D+%5Cint+f%28x%29dx+%2B+C_1%5CRightarrow+y%27+%3D+%5Ciint+f%28x%29dx%5E2+%2BC_1x+%2B+C_2%5CRightarrow+y+%3D+%5Ciiint+f%28x%29dx%5E3+%2BC_1x%5E2+%2B+C_2x+%2B+C_3" alt="[公式]"> </p>
<p>以此类推：</p>
<p><img src="https://www.zhihu.com/equation?tex=y%5E%7B%28n%29%7D+%3D+f%28x%29%5CRightarrow+y+%3D+%5Ciiint...%5Cint+f%28x%29dx%5En+%2BC_1x%5E%7Bn-1%7D+%2B+C_2x%5E%7Bn-2%7D+%2B+...+%2BC_%7Bn-3%7Dx%5E2+%2B+C_%7Bn-2%7Dx+%2B+C_%7Bn-1%7D" alt="[公式]"> </p>
<p>（6）<strong>二阶可降阶型（缺y）</strong>：形如 <img src="https://www.zhihu.com/equation?tex=y%27%27+%3D+f%28x%2Cy%27%29" alt="[公式]"> </p>
<p>首先，换元法：设 <img src="https://www.zhihu.com/equation?tex=p+%3D+y%27" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=y%27%27+%3D+p%27+%3D+%5Cfrac%7Bdp%7D%7Bdx%7D" alt="[公式]"> </p>
<p>然后，原式就变成： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bdp%7D%7Bdx%7D+%3D+f%28x%2Cp%29" alt="[公式]"> </p>
<p>这就化成了一阶线性D.E，最后直接套公式计算结果就好了。</p>
<p>（7）<strong>二阶可降阶型（缺x）</strong>：形如 <img src="https://www.zhihu.com/equation?tex=y%27%27+%3D+f%28y%2Cy%27%29" alt="[公式]"> </p>
<p>首先，换元法：设 <img src="https://www.zhihu.com/equation?tex=p+%3D+y%27" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=y%27%27+%3D+p%27+%3D+%5Cfrac%7Bdp%7D%7Bdx%7D" alt="[公式]"> </p>
<p>然后，原式就变成： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bdp%7D%7Bdx%7D+%3D+f%28y%2Cp%29" alt="[公式]"> </p>
<p>但是这样就出现了三个字母（p，y，x），我们不希望有三个字母，因为只有两个字母的话，我们就可以如法炮制，变成一阶线性D.E.，然后直接套公式计算了。</p>
<p>所以，我们把 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bdp%7D%7Bdx%7D+%3D+%5Cfrac%7Bdp%7D%7Bdy%7D+%5Cfrac%7Bdy%7D%7Bdx%7D+%3D+p%5Cfrac%7Bdp%7D%7Bdy%7D" alt="[公式]"> </p>
<p>于是，原式就变成： <img src="https://www.zhihu.com/equation?tex=p%5Cfrac%7Bdp%7D%7Bdy%7D+%3D+f%28y%2Cp%29" alt="[公式]"> </p>
<p>这样就只有两个字母了，哈哈可以套公式直接算了。</p>
<p>（8）<strong>二阶常系数齐次线性型</strong>：形如 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+py%27+%2Bqy+%3D+0" alt="[公式]"> </p>
<p>首先，写出特征方程： <img src="https://www.zhihu.com/equation?tex=%5Clambda+%5E2+%2B+p%5Clambda+%2B+q+%3D+0" alt="[公式]"> </p>
<p>然后，看有几个根（用求根公式）： <img src="https://www.zhihu.com/equation?tex=%5CDelta+%3D+b%5E2+-+4ac+%3D+p%5E2+-+4q" alt="[公式]"> </p>
<p>当Δ&gt;0，即 <img src="https://www.zhihu.com/equation?tex=%5Clambda+_1+%5Cne+%5Clambda+_2" alt="[公式]"> ,则： <img src="https://www.zhihu.com/equation?tex=y+%3D+C_1e%5E%7B%5Clambda+_1+x%7D+%2B+C_2e%5E%7B%5Clambda+_2+x%7D" alt="[公式]"> </p>
<p>当Δ=0，即 <img src="https://www.zhihu.com/equation?tex=%5Clambda+_1+%3D+%5Clambda+_2" alt="[公式]"> ,则： <img src="https://www.zhihu.com/equation?tex=y+%3D+%28C_1%2BC_2x%29e%5E%7B%5Clambda+_1+x%7D" alt="[公式]"> </p>
<p>当Δ&lt;0，存在共轭复根 <img src="https://www.zhihu.com/equation?tex=%5Calpha+%5Cpm+i%5Cbeta" alt="[公式]"> ，则： <img src="https://www.zhihu.com/equation?tex=y+%3D+e%5E%7B%5Calpha+x%7D%28C_1+cos+%5Cbeta+x+%2B+C_2+sin+%5Cbeta+x%29" alt="[公式]"> </p>
<p>（在中学的时候啊，对于Δ&lt;0的情况，我们就直接说该方程无解，但如果引入虚数的概念，则有一对共轭复根为解）</p>
<p>拓展：这个特征方程是怎么来的呢？（特征方程的来源）</p>
<p>由方程的结构可知， <img src="https://www.zhihu.com/equation?tex=y%27%27%2Cy%27%2Cy" alt="[公式]"> 必须是同类函数，才能满足<img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+py%27+%2Bqy+%3D+0" alt="[公式]"> </p>
<p>不难猜出， <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 是指数函数，即 <img src="https://www.zhihu.com/equation?tex=y+%3D+e%5E%7Brx%7D%2C+y%27+%3D+re%5E%7Brx%7D%2C+y%27%27+%3D+r%5E2e%5E%7Brx%7D" alt="[公式]"> </p>
<p>于是，原式等于 <img src="https://www.zhihu.com/equation?tex=r%5E2e%5E%7Brx%7D%2B+pre%5E%7Brx%7D+%2Bqe%5E%7Brx%7D+%3D+0%5CRightarrow+e%5E%7Brx%7D%28r%5E2%2B+pr+%2Bq%29+%3D+0" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B5e%5E%7Brx%7D%5Cne0+%E2%88%B4+r%5E2%2B+pr+%2Bq+%3D+0" alt="[公式]"> </p>
<p>故可设，特征方程 <img src="https://www.zhihu.com/equation?tex=%5Clambda%5E2%2B+p%5Clambda+%2Bq+%3D+0" alt="[公式]"> （其实就是把 <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]"> 写成 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> ，因为习惯上用<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]">来描述特征方程）。</p>
<p>那么特征方程后面所对应的用来求出D..通解的公式又是怎么来的呢？</p>
<p>由于，这个特征方程<img src="https://www.zhihu.com/equation?tex=%5Clambda%5E2%2B+p%5Clambda+%2Bq+%3D+0" alt="[公式]">是一个一元二次方程（<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]">为未知数）。</p>
<p>根据求根公式 <img src="https://www.zhihu.com/equation?tex=%5CDelta+%3D+b%5E2-4ac" alt="[公式]"> 可知，这个特征方程的解有三种情况。</p>
<p>我就以第一种情况为例（因为后两种情况推导起来很麻烦）：</p>
<p>当 <img src="https://www.zhihu.com/equation?tex=%5CDelta+%3E0" alt="[公式]"> ，有两个不同根 <img src="https://www.zhihu.com/equation?tex=%5Clambda_1%5Cne+%5Clambda_2" alt="[公式]"> ，将其分别代入<img src="https://www.zhihu.com/equation?tex=y+%3D+e%5E%7Brx%7D" alt="[公式]"> </p>
<p>得： <img src="https://www.zhihu.com/equation?tex=y_1+%3D+e%5E%7B%5Clambda_1x+%7D" alt="[公式]"> 和<img src="https://www.zhihu.com/equation?tex=y_2+%3D+e%5E%7B%5Clambda_2x+%7D" alt="[公式]">（两个特解）</p>
<p>由基本概念中“某D.E.通解的项和该D.E.的阶相同”可知， <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+py%27+%2Bqy+%3D+0" alt="[公式]"> 的通解应有两项，不妨设为 <img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]"> 。</p>
<p>再根据下文“解的性质中线性无关那块的概念”可知，如果 <img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]"> 要是 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+py%27+%2Bqy+%3D+0" alt="[公式]"> 的解，则必须满足 <img src="https://www.zhihu.com/equation?tex=Cy_1+%5Cne+y_2" alt="[公式]"> （即二者线性无关）。</p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B5%5Clambda_1+%5Cne+%5Clambda_2+%E2%88%B4%5Cfrac%7By_1%7D%7By_2%7D+%3D+%5Cfrac%7Be%5E%7B%5Clambda_1x+%7D%7D%7Be%5E%7B%5Clambda_2x+%7D%7D+%5Cne+C" alt="[公式]"> （C为常数）则二者线性无关。</p>
<p>故，<img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2+%3D+C_1e%5E%7B%5Clambda_2x+%7D+%2B+C_1e%5E%7B%5Clambda_2x+%7D" alt="[公式]"> </p>
<p>第二种情况：当 <img src="https://www.zhihu.com/equation?tex=%5CDelta+%3D0" alt="[公式]"> ，有一个二重根 <img src="https://www.zhihu.com/equation?tex=%5Clambda_1%3D+%5Clambda_2" alt="[公式]">，可以通过设一个线性无关的另一个特解来推导。</p>
<p>第三种情况：当 <img src="https://www.zhihu.com/equation?tex=%5CDelta+%3C0" alt="[公式]"> ，存在共轭复根 <img src="https://www.zhihu.com/equation?tex=%5Calpha+%5Cpm+i%5Cbeta" alt="[公式]">，要注意复数的运算规则。</p>
<p>（9）<strong>二阶常系数非齐线性型①</strong>：形如 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+py%27+%2Bqy+%3D+P_n%28x%29e%5E%7Bkx%7D" alt="[公式]"> </p>
<p>（ <img src="https://www.zhihu.com/equation?tex=+P_n" alt="[公式]"> 是多项式的意思，洋屁名polynomial）</p>
<p>它的解其实就是：二阶常系数齐次线性型的通解 + 一个自己的特解。</p>
<p>例题： <img src="https://www.zhihu.com/equation?tex=y%27%27+-+3y%27+%2B+2y+%3D+%282x%2B1%29e%5E%7Bx%7D" alt="[公式]"> </p>
<p>第一步，求齐次的通解：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Clambda+%5E2+-3%5Clambda+%2B2+%3D+0+%5CRightarrow%5CDelta+%3D+1+%3E+0%2C+%5Clambda+_1+%3D+1%2C+%5Clambda+_2+%3D+2%5CRightarrow+y+%3D+C_1e%5E%7Bx%7D+%2B+C_2e%5E%7B2x%7D" alt="[公式]"> </p>
<p>第二步，找一个非齐的特解（按通解的模样，假设一个特解）：</p>
<p><img src="https://www.zhihu.com/equation?tex=y_0%28x%29+%3D+%28ax+%2B+b%29e%5E%7B%28x%29%7D" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B5k+%3D+1+%3D+%5Clambda+_1+%5Cne+%5Clambda+_2+%E2%88%B4" alt="[公式]"> 特解乘以x，是<img src="https://www.zhihu.com/equation?tex=y_0+%3D+x%28ax+%2B+b%29e%5E%7Bx%7D+%3D+%28ax%5E2+%2B+bx%29e%5E%7Bx%7D" alt="[公式]"> </p>
<p>如果<img src="https://www.zhihu.com/equation?tex=k+%5Cne+%5Clambda+_1+%5Cne+%5Clambda+_2+" alt="[公式]">则特解是 <img src="https://www.zhihu.com/equation?tex=%28ax+%2B+b%29e%5E%7Bx%7D+" alt="[公式]"> </p>
<p>如果<img src="https://www.zhihu.com/equation?tex=k+%3D+%5Clambda+_1+%3D+%5Clambda+_2" alt="[公式]"> 则特解乘以 <img src="https://www.zhihu.com/equation?tex=x%5E2" alt="[公式]"> </p>
<p> 由于<img src="https://www.zhihu.com/equation?tex=y_0+%3D+%28ax%5E2+%2B+bx%29e%5E%7Bx%7D" alt="[公式]"> 则：</p>
<p><img src="https://www.zhihu.com/equation?tex=y%27_0+%3D+%28ax%5E2+%2B+bx%29e%5E%7Bx%7D+%2B+%282ax%2B+b%29e%5E%7Bx%7D+%3D+%5Bax%5E2+%2B+%282a%2Bb%29x+%2B+b%5De%5E%7Bx%7D" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=y%27%27_0+%3D+2ae%5Ex+%2B+%28ax%5E2+%2B+bx%29e%5E%7Bx%7D+%2B+2%282ax%2B+b%29e%5E%7Bx%7D+%3D+%5Bax%5E2+%2B+%284a%2Bb%29x+%2B+2a%2Bb%5De%5Ex" alt="[公式]"> </p>
<p>代入原式，得 <img src="https://www.zhihu.com/equation?tex=+%5B3ax%5E2+%2B+%286a%2B3b%29x+%2B+2a%2B2b%5De%5Ex+%3D+%282x-1%29e%5Ex" alt="[公式]"> </p>
<p>求出： <img src="https://www.zhihu.com/equation?tex=a+%3D+-1%2C+b%3D-1" alt="[公式]"> </p>
<p>则原式的解就是： <img src="https://www.zhihu.com/equation?tex=y+%3D+C_1e%5E%7Bx%7D+%2B+C_2e%5E%7B2x%7D+%2B+%28-x%5E2+%2B+-x%29e%5E%7Bx%7D" alt="[公式]"> </p>
<p>拓展：这里代入原式，是有一定技巧的（ <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 为特解里的 <img src="https://www.zhihu.com/equation?tex=P_n%28x%29" alt="[公式]"> ）</p>
<p>如果<img src="https://www.zhihu.com/equation?tex=k+%3D+%5Clambda+_1+%3D+%5Clambda+_2+" alt="[公式]">则公式为 <img src="https://www.zhihu.com/equation?tex=Q%27%27+%3D+P_n%28x%29" alt="[公式]"> </p>
<p>如果<img src="https://www.zhihu.com/equation?tex=k+%3D+%5Clambda+_1+%5Cne+%5Clambda+_2+" alt="[公式]">则公式为 <img src="https://www.zhihu.com/equation?tex=Q%27%27+%2B+%282k-p%29Q%27+%3D+P_n%28x%29" alt="[公式]"> </p>
<p>如果<img src="https://www.zhihu.com/equation?tex=k+%5Cne+%5Clambda+_1+%5Cne+%5Clambda+_2+" alt="[公式]">，那就没有技巧了。</p>
<p>以上题为例：</p>
<p>由于<img src="https://www.zhihu.com/equation?tex=k+%3D+1+%3D+%5Clambda+_1+%5Cne+%5Clambda+_2+" alt="[公式]"> ，得到<img src="https://www.zhihu.com/equation?tex=y_0+%3D+x%28ax+%2B+b%29e%5E%7Bx%7D+%3D+%28ax%5E2+%2B+bx%29e%5E%7Bx%7D" alt="[公式]">这个特解</p>
<p>代入公式： <img src="https://www.zhihu.com/equation?tex=2a+%2B+%282-3%29%282ax%2Bb%29+%3D+%282x-1%29%5CRightarrow+-2ax+%2B+2a-b+%3D+%282x-1%29" alt="[公式]"> </p>
<p>解得： <img src="https://www.zhihu.com/equation?tex=a+%3D+-1%2C+b+%3D+-1" alt="[公式]"> （是不是快很多）</p>
<h2 id="三、解的性质"><a href="#三、解的性质" class="headerlink" title="三、解的性质"></a>三、解的性质</h2><p>（1）<strong>线性组成</strong>： <img src="https://www.zhihu.com/equation?tex=y_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=y_2" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+0" alt="[公式]"> 的解，则 <img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]"> 也是它的解。</p>
<p>证：把 <img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]"> 代入 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29+" alt="[公式]"> 得：</p>
<p><img src="https://www.zhihu.com/equation?tex=C_1+y%27%27_1+%2B+C_2+y%27%27_2+%2B+p%28x%29%28C_1+y%27_1+%2B+C_2+y%27_2+%29+%2B+q%28x%29%28C_1+y_1+%2B+C_2+y_2+%29" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5CRightarrow+C_1+%28y%27%27_1+%2B+p%28x%29y%27_1+%2B+q%28x%29y_1%29+%2B+C_2+%28y%27%27_2+%2B+p%28x%29y%27_2+%2B+q%28x%29y_2%29+" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B5y_1%2C+y_2" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29+%3D+0" alt="[公式]">的解</p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B4y%27%27_1+%2B+p%28x%29y%27_1+%2B+q%28x%29y_1+%3D+y%27%27_2+%2B+p%28x%29y%27_2+%2B+q%28x%29y_2+%3D+0" alt="[公式]"> </p>
<p>则 <img src="https://www.zhihu.com/equation?tex=C_1+%28y%27%27_1+%2B+p%28x%29y%27_1+%2B+q%28x%29y_1%29+%2B+C_2+%28y%27%27_2+%2B+p%28x%29y%27_2+%2B+q%28x%29y_2%29+%3D+0" alt="[公式]"> </p>
<p>Q.E.D.嘻嘻</p>
<p>同理其实我们可以推出，在 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+f%28x%29" alt="[公式]"> 的情况。</p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B5y_1%2C+y_2" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29+%3D+f%28x%29" alt="[公式]">的解</p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B4y%27%27_1+%2B+p%28x%29y%27_1+%2B+q%28x%29y_1+%3D+y%27%27_2+%2B+p%28x%29y%27_2+%2B+q%28x%29y_2+%3D+f%28x%29" alt="[公式]"> </p>
<p>则<img src="https://www.zhihu.com/equation?tex=C_1+%28y%27%27_1+%2B+p%28x%29y%27_1+%2B+q%28x%29y_1%29+%2B+C_2+%28y%27%27_2+%2B+p%28x%29y%27_2+%2B+q%28x%29y_2%29+%3D+%28C_1+%2B+C_2%29f%28x%29" alt="[公式]"> </p>
<p>当 <img src="https://www.zhihu.com/equation?tex=C_1+%2B+C_2+%3D1" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]"> 是<img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+f%28x%29" alt="[公式]">的解</p>
<p>当 <img src="https://www.zhihu.com/equation?tex=C_1+%2B+C_2+%3D0" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]"> 是<img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+0" alt="[公式]">的解、</p>
<p>否则， <img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]"> 是<img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+%28C_1+%2B+C_2%29f%28x%29" alt="[公式]">的解</p>
<p>（2）<strong>线性无关</strong>：<img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]">是<img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29+%3D+0" alt="[公式]"> 的解，充要条件是<img src="https://www.zhihu.com/equation?tex=y_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=y_2" alt="[公式]">是线性无关的。</p>
<p>（所谓线性无关，就是指 <img src="https://www.zhihu.com/equation?tex=y_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=y_2" alt="[公式]"> 谁都不能表示谁，即<img src="https://www.zhihu.com/equation?tex=Cy_1+%5Cne+y_2" alt="[公式]"> （C为任意常数））</p>
<p>证：</p>
<p>充分性：因为<img src="https://www.zhihu.com/equation?tex=y_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=y_2" alt="[公式]">线性无关的，根据基本概念中关于通解的定义，不难看出<img src="https://www.zhihu.com/equation?tex=C_1+y_1+%2B+C_2+y_2" alt="[公式]">是<img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29+%3D+0" alt="[公式]"> 的解</p>
<p>必要性（反证法）：设<img src="https://www.zhihu.com/equation?tex=y_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=y_2" alt="[公式]">线性相关，则</p>
<p><img src="https://www.zhihu.com/equation?tex=Cy_1+%3D+y_2%5CRightarrow+C_1+y_1+%3D+C_2+y_2%5CRightarrow+y+%3D+C_1+y_1+%2B+C_2+y_2+%3D+C_1+y_1" alt="[公式]"> </p>
<p>这与基本概念中“某D.E.通解的项和该D.E.的阶相同”矛盾，则<img src="https://www.zhihu.com/equation?tex=y_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=y_2" alt="[公式]">线性无关。</p>
<p>Q.E.D.O(∩_∩)O哈哈~</p>
<p>（3）<strong>叠加原理</strong>：如果 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+f%28x%29+%3D+f_1%28x%29+%2B+f_2%28x%29" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+f_1%28x%29" alt="[公式]"> 的解为 <img src="https://www.zhihu.com/equation?tex=y_1" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+f_2%28x%29" alt="[公式]"> 的解为 <img src="https://www.zhihu.com/equation?tex=y_2" alt="[公式]"> </p>
<p>则 <img src="https://www.zhihu.com/equation?tex=y_1%2B2+y_1" alt="[公式]"> 便是 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+%3D+f%28x%29" alt="[公式]"> 的通解</p>
<p>证：把 <img src="https://www.zhihu.com/equation?tex=y_1%2By_1" alt="[公式]"> 代入 <img src="https://www.zhihu.com/equation?tex=y%27%27+%2B+p%28x%29y%27+%2B+q%28x%29y+" alt="[公式]"> ，得出</p>
<p><img src="https://www.zhihu.com/equation?tex=%28y%27%27_1%2B+y%27%27_1%29+%2B+p%28x%29%28y%27_1%2B+y%27_1%29+%2B+q%28x%29%28y_1%2B+y_1%29" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5CRightarrow+y%27%27_1%2B+p%28x%29y%27_1+%2B+q%28x%29y_1+%2B+y%27%27_2%2B+p%28x%29y%27_2+%2B+q%28x%29y_2" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B5y%27%27_1%2B+p%28x%29y%27_1+%2B+q%28x%29y_1+%3D+f_1%28x%29%2C+y%27%27_2%2B+p%28x%29y%27_2+%2B+q%28x%29y_2+%3D+f_2%28x%29" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%B4+y%27%27_1%2B+p%28x%29y%27_1+%2B+q%28x%29y_1+%2B+y%27%27_2%2B+p%28x%29y%27_2+%2B+q%28x%29y_2+%3D+f_1%28x%29+%2B+f_2%28x%29+%3D+f%28x%29" alt="[公式]"></p>
<blockquote>
<p><strong>本文转载自<a href="https://zhuanlan.zhihu.com/p/38573924">（回忆大学所学）常微分方程</a></strong></p>
</blockquote>
]]></content>
      <categories>
        <category>math</category>
        <category>ODE</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>ODE</tag>
      </tags>
  </entry>
  <entry>
    <title>NeurIPS 2021  paper list</title>
    <url>/2021/11/07/NeurIPS%202021%20accepted%20paper%20list/</url>
    <content><![CDATA[<h2 id="NeurIPS-2021-accepted-paper-list"><a href="#NeurIPS-2021-accepted-paper-list" class="headerlink" title="NeurIPS 2021 accepted paper list"></a>NeurIPS 2021 accepted paper list<span id="more"></span></h2><h3 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h3><ul>
<li><strong>Online false discovery rate control for anomaly detection in time series</strong></li>
<li><strong>Conformal Time-series Forecasting</strong></li>
<li><strong>Probabilistic Forecasting: A Level-Set Approach</strong></li>
<li><strong>Topological Attention for Time Series Forecasting</strong></li>
<li><strong>Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</strong></li>
<li><strong>MixSeq: Connecting Macroscopic Time Series Forecasting with Microscopic Time Series Data</strong></li>
<li><strong>MobTCast: Leveraging Auxiliary Trajectory Forecasting for Human Mobility Prediction</strong></li>
<li><strong>Collaborative Uncertainty in Multi-Agent Trajectory Forecasting</strong></li>
<li><strong>Dynamical Wasserstein Barycenters for Time-series Modeling</strong></li>
</ul>
<h3 id="Dynamic-systems"><a href="#Dynamic-systems" class="headerlink" title="Dynamic systems"></a>Dynamic systems</h3><ul>
<li><strong>Heavy Ball Neural Ordinary Differential Equations</strong></li>
<li><strong>A Probabilistic State Space Model for Joint Inference from Differential Equations and Data</strong></li>
<li><strong>On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)</strong></li>
<li><strong>PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations</strong></li>
<li><strong>Multiwavelet-based Operator Learning for Differential Equations</strong></li>
<li><strong>Imitating Deep Learning Dynamics via Locally Elastic Stochastic Differential Equations</strong></li>
<li><strong>Dynamic Resolution Network</strong></li>
<li><strong>SyMetric: Measuring the Quality of Learnt Hamiltonian Dynamics Inferred from Vision</strong></li>
<li><strong>Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems</strong></li>
<li><strong>Hamiltonian Dynamics with Non-Newtonian Momentum for Rapid Sampling</strong></li>
<li><strong>Online Control of Unknown Time-Varying Dynamical Systems</strong></li>
<li><strong>Learning Stable Deep Dynamics Models for Partially Observed or Delayed Dynamical Systems</strong></li>
<li><strong>Neural Hybrid Automata: Learning Dynamics With Multiple Modes and Stochastic Transitions</strong></li>
<li><strong>Compositional Modeling of Nonlinear Dynamical Systems with ODE-based Random Features</strong></li>
</ul>
<h3 id="EBMS-and-applications"><a href="#EBMS-and-applications" class="headerlink" title="EBMS and applications"></a>EBMS and applications</h3><ul>
<li><strong>Predicting Molecular Conformation via Dynamic Graph Score Matching</strong></li>
<li><strong>Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions</strong></li>
<li><strong>Diffusion Models Beat GANs on Image Synthesis</strong></li>
<li><strong>Local Hyper-Flow Diffusion</strong></li>
<li><strong>ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis</strong></li>
<li><strong>Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling</strong></li>
<li><strong>CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation</strong></li>
<li><strong>Structured Denoising Diffusion Models in Discrete State-Spaces</strong></li>
<li><strong>D2C: Diffusion-Decoding Models for Few-Shot Conditional Generation</strong></li>
<li><strong>Maximum Likelihood Training of Score-Based Diffusion Models</strong></li>
<li><strong>On Density Estimation with Diffusion Models</strong></li>
<li><strong>Diffusion Normalizing Flow</strong></li>
<li><strong>A Variational Perspective on Diffusion-Based Generative Models and Score Matching</strong></li>
<li><strong>Arbitrary Conditional Distributions with Energy</strong></li>
<li><strong>Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent</strong></li>
<li><strong>Bounds all around: training energy-based models with bidirectional bounds</strong></li>
<li><strong>Controllable and Compositional Generation with Latent-Space Energy-Based Models</strong></li>
<li><strong>Perturb-and-max-product: Sampling and learning in discrete energy-based models</strong></li>
<li><strong>Score-based Generative Neural Networks for Large-Scale Optimal Transport</strong></li>
<li><strong>Noise2Score: Tweedie’s Approach to Self-Supervised Image Denoising without Clean Images</strong></li>
<li><strong>Score-based Generative Modeling in Latent Space</strong></li>
</ul>
<h3 id="others"><a href="#others" class="headerlink" title="others"></a>others</h3><ul>
<li><strong>Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems</strong></li>
<li><strong>Dissecting the Diffusion Process in Linear Graph Convolutional Networks</strong></li>
<li><strong>Beltrami Flow and Neural Diffusion on Graphs</strong></li>
<li><strong>Adaptive Diffusion in Graph Neural Networks</strong></li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ODE数值解</title>
    <url>/2020/09/24/ODE%E6%95%B0%E5%80%BC%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>转载自  <strong>李森科在zhihu</strong> <a href="https://zhuanlan.zhihu.com/p/70255604">链接</a> <span id="more"></span></p>
</blockquote>
<h2 id="第一部分-常微分方程的数值解法"><a href="#第一部分-常微分方程的数值解法" class="headerlink" title="第一部分 常微分方程的数值解法"></a>第一部分 常微分方程的数值解法</h2><p>常微分方程数值解法主要分为两大部分，初值问题与边值问题的数值方法。本部分只讨论初值问题。</p>
<h2 id="第一章-常微分方程初值问题"><a href="#第一章-常微分方程初值问题" class="headerlink" title="第一章 常微分方程初值问题"></a>第一章 常微分方程初值问题</h2><p>本章讨论一阶常微分方程初值问题 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D++%5Cfrac%7Bdu%28t%29%7D%7Bdt%7D%3Df%28t%2Cu%28t%29%29%5C%5C+u%28t_0%29%3Du_0%5C%5C+%5Cend%7Bcases%7D%5C+++%281.0.1%29" alt="[公式]"> 的数值求解，其中 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=u" alt="[公式]"> 的已知函数， <img src="https://www.zhihu.com/equation?tex=u_0" alt="[公式]">为给定的初值。在基本假设下，初值问题 <img src="https://www.zhihu.com/equation?tex=%281.0.1%29" alt="[公式]"> 在区间 <img src="https://www.zhihu.com/equation?tex=%5Bt_0%2CT%5D" alt="[公式]"> 上有唯一解 <img src="https://www.zhihu.com/equation?tex=u%28t%29" alt="[公式]"> ，并且 <img src="https://www.zhihu.com/equation?tex=u%28t%29" alt="[公式]"> 是连续可微的。</p>
<p>数值解法是一种离散化方法，利用这种方法，可以在一系列离散点 <img src="https://www.zhihu.com/equation?tex=t_1%2C...%2Ct_N" alt="[公式]"> 上求出未知函数 <img src="https://www.zhihu.com/equation?tex=u%28t%29" alt="[公式]"> 的近似值 <img src="https://www.zhihu.com/equation?tex=u_1%2C...%2Cu_N" alt="[公式]"> ，这个近似值称为初值问题的一个数值解。</p>
<p>自变量 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 的离散值 <img src="https://www.zhihu.com/equation?tex=t_1%2C...%2Ct_N" alt="[公式]"> 是事先取定的，称之为节点，通常取成等距的，即 <img src="https://www.zhihu.com/equation?tex=t_1%3Dt_0%2Bh%2C...%2Ct_N%3Dt_0%2BNh" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=h%3E0" alt="[公式]"> 称为步长。</p>
<h2 id="1-1-基本概念-Euler法与梯形法"><a href="#1-1-基本概念-Euler法与梯形法" class="headerlink" title="1.1 基本概念 Euler法与梯形法"></a>1.1 基本概念 Euler法与梯形法</h2><p><strong>1.1.1 Euler法介绍</strong></p>
<p>本节通过Euler法及梯形法的讨论，说明常微分方程数值解法中的一些基本概念与主要研究的问题。</p>
<p>首先得到初值问题 <img src="https://www.zhihu.com/equation?tex=%281.0.1%29" alt="[公式]"> 等价的积分形式 <img src="https://www.zhihu.com/equation?tex=u%28t%2Bh%29%3Du%28t%29%2B%5Cint%5E%7Bt%2Bh%7D_%7Bt%7Df%28%5Ctau%2Cu%28%5Ctau%29%29d%5Ctau+%5C+%281.1.2%29" alt="[公式]"> </p>
<p>设 <img src="https://www.zhihu.com/equation?tex=t_1%3Dt_0%2Bh" alt="[公式]"> ，使用左矩形公式计算一步 <img src="https://www.zhihu.com/equation?tex=u_1%3Du_0%2Bhf%28t_0%2Cu_0%29" alt="[公式]"> ，类似地利用 <img src="https://www.zhihu.com/equation?tex=u_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=f%28t_1%2Cu_1%29" alt="[公式]"> 计算出 <img src="https://www.zhihu.com/equation?tex=u%28t_2%29" alt="[公式]"> 的近似值，那么可以得到Euler法的计算公式  <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7Du_0%3Du_0%EF%BC%8C%5C%5C+u_%7Bn%2B1%7D%3Du_n%2Bhf%28t_n%2Cu_n%29%5C+%281.1.1%29%2Cn%5Cgeq0%5C%5C+%5Cend%7Bcases%7D%5C+" alt="[公式]"> </p>
<p>Euler法的几何意义是十分清楚的：在这种方法中，实际上是用一条过 <img src="https://www.zhihu.com/equation?tex=%28t_0%2Cu_0%29" alt="[公式]"> 的折现来近似替代过 <img src="https://www.zhihu.com/equation?tex=%28t_0%2Cu_0%29" alt="[公式]"> 的积分曲线，因此这种方法又称为折线法。</p>
<p><strong>1.1.2 收敛性，截断误差，稳定性问题的简单介绍</strong></p>
<p>为考察由Euler法提供的数值解是否具有实用价值，首先应该知道，当步长 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 取得充分小时，所得数值解 <img src="https://www.zhihu.com/equation?tex=u_n" alt="[公式]"> 能否足够精确地逼近初值问题 <img src="https://www.zhihu.com/equation?tex=%281.0.1%29" alt="[公式]"> 的真解 <img src="https://www.zhihu.com/equation?tex=u%28t_n%29" alt="[公式]"> ，这就是收敛性问题。</p>
<p>其次，还必须估计数值解与真解之间的误差，以便于在实际计算中根据精度要求确定计算方案。在Euler法中产生的误差称为截断误差。</p>
<p>其次，计算过程中还会由于数值误差的舍入产生舍入误差。由于Euler法是一种步进方法，只有当最初产生的误差在以后各步的计算中不会无限制扩大时，即只有当 <img src="https://www.zhihu.com/equation?tex=%281.1.1%29" alt="[公式]"> 的解对初值具有某种连续相依性质时，方法才具有实用价值。这种性质称为稳定性问题。</p>
<p><strong>1.1.3 Euler法的截断误差估计</strong></p>
<p>首先讨论Euler法的截断误差估计及收敛性问题。初值问题 <img src="https://www.zhihu.com/equation?tex=%281.0.1%29" alt="[公式]"> 可以写成等价的积分形式 <img src="https://www.zhihu.com/equation?tex=u%28t%2Bh%29%3Du%28t%29%2B%5Cint%5E%7Bt%2Bh%7D_%7Bt%7Df%28%5Ctau%2Cu%28%5Ctau%29%29d%5Ctau%5C+%281.1.2%29" alt="[公式]"> </p>
<p>在上式中，令 <img src="https://www.zhihu.com/equation?tex=t%3Dt_n" alt="[公式]"> ，并用左矩形公式计算右端积分，有</p>
<p><img src="https://www.zhihu.com/equation?tex=u%28t_%7Bn%2B1%7D%29%3Du%28t_n%29%2B%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7Df%28%5Ctau%2Cu%28%5Ctau%29%29d%5Ctau" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3Du%28t_n%29%2B%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%5Bf%28%5Ctau%2Cu%28%5Ctau%29-f%28t_n%2Cu%28t_n%29%29%5Dd%5Ctau%2B%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7Df%28t_n%2Cu%28t_n%29%29d%5Ctau" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3Du%28t_n%29%2Bhf%28t_n%2Cu%28t_n%29%29%2B%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%5Bf%28%5Ctau%2Cu%28%5Ctau%29-f%28t_n%2Cu%28t_n%29%29%5Dd%5Ctau" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3Du%28t_n%29%2Bhf%28t_n%2Cu%28t_n%29%29%2BR_n%5C+%281.1.3%29" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=R_n" alt="[公式]"> 称为Euler法的局部截断误差，它表示当 <img src="https://www.zhihu.com/equation?tex=u_n%3Du%28t_n%29" alt="[公式]"> 为精确值时，利用公式 <img src="https://www.zhihu.com/equation?tex=%281.1.1%29" alt="[公式]"> 计算 <img src="https://www.zhihu.com/equation?tex=u%28t_n%2Bh%29" alt="[公式]"> 的误差。在逐步计算的过程中，局部截断误差会传播和积累，因此还必须对这种误差的积累与传播作出估计。</p>
<p>设 <img src="https://www.zhihu.com/equation?tex=u_n" alt="[公式]"> 是在无舍入误差情况下用 <img src="https://www.zhihu.com/equation?tex=%281.1.1%29" alt="[公式]"> 计算出的数值解，而 <img src="https://www.zhihu.com/equation?tex=u%28t_n%29" alt="[公式]"> 是真解， <img src="https://www.zhihu.com/equation?tex=%5Cepsilon_n%3Du%28t_n%29-u_n" alt="[公式]"> 为Euler法 <img src="https://www.zhihu.com/equation?tex=%281.1.1%29" alt="[公式]"> 的整体截断误差。</p>
<p>为估计 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon_n" alt="[公式]"> ，先给出 <img src="https://www.zhihu.com/equation?tex=R_n" alt="[公式]"> 的上界。从式 <img src="https://www.zhihu.com/equation?tex=%281.1.3%29" alt="[公式]"> 知，</p>
<p><img src="https://www.zhihu.com/equation?tex=R_n%3D%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%5Bf%28%5Ctau%2Cu%28%5Ctau%29-f%28t_n%2Cu%28t_n%29%29%5Dd%5Ctau" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3D%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%5Bf%28%5Ctau%2Cu%28%5Ctau%29-f%28t_n%2Cu%28%5Ctau%29%29%5Dd%5Ctau%2B%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%5Bf%28t_n%2Cu%28%5Ctau%29-f%28t_n%2Cu%28t_n%29%29%5Dd%5Ctau" alt="[公式]"> </p>
<p>若函数 <img src="https://www.zhihu.com/equation?tex=f%28t%2Cu%29" alt="[公式]"> 关于 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 也满足Lipschitz条件，以 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 记相应的Lipschitz常数，则由上式推出 <img src="https://www.zhihu.com/equation?tex=%7CR_n%7C%5Cleq%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%5CBig%7Cf%28%5Ctau%2Cu%28%5Ctau%29-f%28t_n%2Cu%28%5Ctau%29%29%5CBig%7Cd%5Ctau%2B%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%5CBig%7Cf%28t_n%2Cu%28%5Ctau%29-f%28t_n%2Cu%28t_n%29%29%5CBig%7Cd%5Ctau" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%5Cleq+K%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%7C%5Ctau-t_n%7Cd%5Ctau%2BL%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%7Cu%28%5Ctau%29-u%28t_n%29%7Cd%5Ctau" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%5Cleq+K%5Cint%5E%7Bh%7D_%7B0%7D%7Cu%7Cdu%2BhL%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7D%7C%5Cfrac%7Bu%28%5Ctau%29-u%28t_n%29%7D%7Bh%7D%7Cd%5Ctau" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%5Cleq+K%5Cint%5E%7Bh%7D_%7B0%7Dudu%2BhL%5Cint%5E%7Bt_%7Bn%2B1%7D%7D_%7Bt_n%7DMd%5Ctau%3D%5Cfrac%7B1%7D%7B2%7Dh%5E2K%2Bh%5E2LM%5Cleq+h%5E2%28K%2BLM%29" alt="[公式]"> </p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=M%3D%5Cmax_%7Bt_0%5Cleq+t%5Cleq+T%7D%7Cu%27%28t%29%7C%3D%5Cmax_%7Bt_0%5Cleq+t%5Cleq+T%7D%7Cf%28t%2Cu%28t%29%29%7C" alt="[公式]"> ，于是 <img src="https://www.zhihu.com/equation?tex=%7CR_n%7C%5Cleq+R%3Dh%5E2%28K%2BLM%29" alt="[公式]"> </p>
<p>用式 <img src="https://www.zhihu.com/equation?tex=%281.1.3%29" alt="[公式]"> 减去式 <img src="https://www.zhihu.com/equation?tex=%281.1.1%29" alt="[公式]"> ，得到整体截断误差所满足的方程</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cepsilon_%7Bn%2B1%7D%3D%5Cepsilon_%7Bn%7D%2Bh%5Bf%28t_n%2Cu%28t_n%29%29-f%28t_n%2Cu_n%29%5D%2BR_n" alt="[公式]"> ，进一步可以得到</p>
<p><img src="https://www.zhihu.com/equation?tex=%7C%5Cepsilon_%7Bn%2B1%7D%7C%5Cleq%7C%5Cepsilon_%7Bn%7D%7C%2Bh%7Cf%28t_n%2Cu%28t_n%29%29-f%28t_n%2Cu_n%29%7C%2BR+" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%5Cleq%7C%5Cepsilon_n%7C%2BhL%7C%5Cepsilon_n%7C%2BR" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3D%281%2BhL%29%7C%5Cepsilon_n%7C%2BR%5Cleq%281%2B%28T-t_0%29L%29%7C%5Cxi_n%7C%2BR" alt="[公式]"> </p>
<p>这是一个递推公式，我们需要得到对任意 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 均成立的上界。下面的引理能够做到这一点。</p>
<p><strong>引理1.1.1</strong> 设数列 <img src="https://www.zhihu.com/equation?tex=%5Cxi_i" alt="[公式]"> 满足不等式 <img src="https://www.zhihu.com/equation?tex=%7C%5Cxi_%7Bi%2B1%7D%7C%5Cleq%281%2B%5Cdelta%29%7C%5Cxi_i%7C%2BB%2C" alt="[公式]"> ， 其中<img src="https://www.zhihu.com/equation?tex=%5C+%5Cdelta%3E0%2C%5C+B%5Cgeq0%2C%5C+i%3D0%2C1%2C2%2C..." alt="[公式]"> ，则有 <img src="https://www.zhihu.com/equation?tex=%7C%5Cxi_i%7C%5Cleq+e%5E%7Bi%5Cdelta%7D%7C%5Cxi_0%7C%2B%5Cfrac%7Be%5E%7Bi%5Cdelta%7D-1%7D%7B%5Cdelta%7DB%2C%5C+i%3E1" alt="[公式]"> 。</p>
<p>由此我们可以得到整体截断误差的估计 <img src="https://www.zhihu.com/equation?tex=%7C%5Cepsilon_n%7C%5Cleq+e%5E%7BL%28T-t_0%29%7D%7C%5Cepsilon_0%7C%2B%5Cfrac%7Bh%7D%7B2%7D%28e%5E%7BL%28T-t_0%29%7D-1%29%28%5Cfrac%7BLM%2BK%7D%7BL%7D%29%5C+%281.1.5%29" alt="[公式]"> </p>
<p>从 <img src="https://www.zhihu.com/equation?tex=u_0%3Du%28t_0%29" alt="[公式]"> 及式 <img src="https://www.zhihu.com/equation?tex=%281.1.5%29" alt="[公式]"> ，又知 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon_n%3DO%28h%29%EF%BC%8Ch%5Crightarrow0" alt="[公式]"> ，即Euler法的整体阶段误差与 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 同阶，此时称Euler法为一阶方法。它的直观意义是，当 <img src="https://www.zhihu.com/equation?tex=u%28t%29" alt="[公式]"> 是一次多项式时，Euler法是精确的。</p>
<p><strong>1.1.4 Euler法的稳定性</strong></p>
<p>这里考虑仅初值 <img src="https://www.zhihu.com/equation?tex=u_0" alt="[公式]"> 有误差，而其他计算步骤无误差的简单情况。</p>
<p>…</p>
<p>上述结论所表述的Euler法的性质，及所谓关于初值的稳定性：当初始误差充分小时，以后各步的误差也可充分小。</p>
<p><strong>1.1.5 梯形法</strong></p>
<p>由式 <img src="https://www.zhihu.com/equation?tex=%281.1.2%29" alt="[公式]"> ，取 <img src="https://www.zhihu.com/equation?tex=t%3Dt_n" alt="[公式]"> ，并用梯形求积公式计算右端积分，舍去每一步梯形求积公式的余项，可以得到梯形法的计算公式 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+u_0%3Du_0%5C%5C+u_%7Bn%2B1%7D%3Du_n%2B%5Cfrac%7Bh%7D%7B2%7D%5Bf_n%2Bf_%7Bn%2B1%7D%5D%2Cn%5Cgeq0%5C%5C+%5Cend%7Bcases%7D+" alt="[公式]"> </p>
<p>对于梯形法，可类似地建立其截断误差估计式及收敛性，且 <img src="https://www.zhihu.com/equation?tex=%7Cu%28t_n%29-u_n%7C%3DO%28h%5E2%29%2Ch%5Crightarrow0" alt="[公式]"> ，此时称梯形法为二阶方法。</p>
<p>这里要注意梯形法与Euler法的一个重要区别，即式 <img src="https://www.zhihu.com/equation?tex=%281.1.8%29" alt="[公式]"> 只给出一个关于 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D" alt="[公式]"> 的（非线性）方程，而Euler法给出的是 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D" alt="[公式]"> 的显式表达式，因此Euler法是一个显式方法，而梯形法是隐式方法。</p>
<p>当梯形法求 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D" alt="[公式]"> 时，可利用求解非线性方程的各种方法，最简单的是下述迭代方法： <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D%5E%7B%28m%2B1%29%7D%3Du_n%2B%5Cfrac%7Bh%7D%7B2%7D%5Bf_n%2Bf%28t_%7Bn%2B1%7D%2Cu_%7Bn%2B1%7D%5E%7B%28m%29%7D%29%5D%2Cm%5Cgeq0" alt="[公式]"> 。</p>
<p>实际计算时，初始近似 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D%5E%7B%280%29%7D" alt="[公式]"> 可用Euler法求出，然后再使用迭代法，这便是预测校正格式（又称为改进Euler法）：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+u_%7Bn%2B1%7D%5E%7B%280%29%7D%3Du_n%2Bhf%28t_%7Bn%7D%2Cu_n%29%5C%5C+u_%7Bn%2B1%7D%5E%7B%28m%2B1%29%7D%3Du_n%2B%5Cfrac%7Bh%7D%7B2%7D%5Bf_n%2Bf%28t_%7Bn%2B1%7D%2Cu_%7Bn%2B1%7D%5E%7B%28m%29%7D%29%5D%2Cm%5Cgeq0%5C%5C+%5Cend%7Bcases%7D" alt="[公式]"> </p>
<p><strong>1.1.6 总结</strong></p>
<p>通过上述两个方法的讨论可知，数值方法的研究有如下基本问题：计算公式的构造，方法的收敛性，截断误差估计，稳定性及方法的实际应用等。</p>
<h2 id="1-2-Runge-Kutta方法及一般单步方法"><a href="#1-2-Runge-Kutta方法及一般单步方法" class="headerlink" title="1.2 Runge-Kutta方法及一般单步方法"></a><strong>1.2 Runge-Kutta方法及一般单步方法</strong></h2><p>前节介绍的两个方法精度是很低的。本节从提高局部截断误差阶入手，来构造具有较高精度的方法。</p>
<p><strong>1.2.1 Taylor级数法</strong></p>
<p>设初值问题 <img src="https://www.zhihu.com/equation?tex=%281.0.1%29" alt="[公式]">  的解 <img src="https://www.zhihu.com/equation?tex=u%28t%29" alt="[公式]"> 具有 <img src="https://www.zhihu.com/equation?tex=q%2B1" alt="[公式]"> 次连续导数。考虑 <img src="https://www.zhihu.com/equation?tex=u%28t%29" alt="[公式]"> 在 <img src="https://www.zhihu.com/equation?tex=t%3Dt_0" alt="[公式]"> 处的Taylor展开式，可得局部截断误差 <img src="https://www.zhihu.com/equation?tex=u%28t_0%2Bh%29-u_1%3DO%28h%5E%7Bq%2B1%7D%29" alt="[公式]"> 。</p>
<p>…</p>
<p>这种方法称为Taylor级数法。虽然它的局部截断误差的阶可任意高，但各阶导数 <img src="https://www.zhihu.com/equation?tex=u%5E%7B%28i%29%7D%28t%29" alt="[公式]"> 的计算量非常大，不实用。现在介绍一类既可以达到较高精度，又可避免高阶导数计算的方法——Runge-Kutta方法。</p>
<p><strong>1.2.2 RK方法</strong></p>
<p>根据公式 <img src="https://www.zhihu.com/equation?tex=%281.1.2%29" alt="[公式]"> 及积分中值定理 <img src="https://www.zhihu.com/equation?tex=%5Cint%5E%7Bt%2Bh%7D_%7Bt%7Df%28%5Ctau%2Cu%28%5Ctau%29%29d%5Ctau+%3Dhf%28t_n%2B%5Ctheta+h%2Cu%28t_n%2B%5Ctheta+h%29%29" alt="[公式]"> ，可得 <img src="https://www.zhihu.com/equation?tex=u%28t_n%2Bh%29%3Du%28t_n%29%2Bhf%28t_n%2B%5Ctheta+h%2Cu%28t_n%2B%5Ctheta+h%29%29+%5C+%281.2.3%29" alt="[公式]"> 。</p>
<p>但 <img src="https://www.zhihu.com/equation?tex=f%28t_n%2B%5Ctheta+h%2Cu%28t_n%2B%5Ctheta+h%29%29" alt="[公式]">是无法计算的。我们用 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 在位于 <img src="https://www.zhihu.com/equation?tex=%5Bt_n%2Ct_n%2Bh%5D" alt="[公式]"> 上的若干个点（例如 <img src="https://www.zhihu.com/equation?tex=s" alt="[公式]"> 个点）值的线性组合来近似它，并使之有尽可能高的精度。具体来讲，就是用下列公式代替式 <img src="https://www.zhihu.com/equation?tex=%281.2.3%29" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+u_%7Bn%2B1%7D%3Du%28t_n%29%2Bh%5Csum%5E%7Bs%7D_%7Bi%3D1%7DW_iK_i%5C+%281.2.4%29%5C%5C+++%5Cbegin%7Bcases%7D+K_1%3Df%28t_n%2Cu%28t_n%29%29%EF%BC%8C%5C%5C+K_i%3Df%28t_n%2Bh%5Calpha_i%2Cu%28t_n%29%2Bh%5Csum%5E%7Bi-1%7D_%7Bj%3D1%7D%5Cbeta_%7Bij%7DK_i%29%5C+%5C%5C+%5Cend%7Bcases%7D%281.2.5%29%5C%5C+%5Cend%7Bequation%7D%5C+" alt="[公式]"> </p>
<p>现在去选定系数 <img src="https://www.zhihu.com/equation?tex=%5Calpha_i%2C%5Cbeta_%7Bij%7D%28i%5Cgeq2%29" alt="[公式]"> ，以及 <img src="https://www.zhihu.com/equation?tex=W_i%28i%5Cgeq1%29" alt="[公式]"> ，使RK方法的整体截断误差具有尽可能高的阶 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> ，这个阶数 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 当然与级数 <img src="https://www.zhihu.com/equation?tex=s" alt="[公式]"> 有关，可记为 <img src="https://www.zhihu.com/equation?tex=q%28s%29" alt="[公式]"> 。</p>
<p>构造RK方法的基本步骤是，选定上述系数，使 <img src="https://www.zhihu.com/equation?tex=u%28t_n%2Bh%29-u%28t_n%29" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=h%5Csum%5E%7Bs%7D_%7Bi%3D1%7DW_iK_i" alt="[公式]"> 两者的Taylor展开式中含 <img src="https://www.zhihu.com/equation?tex=h%5Er%28r%3D1%2C...%2Cq%29" alt="[公式]"> 的项的系数对尽可能大的 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 相等。</p>
<p>书上考虑的是 <img src="https://www.zhihu.com/equation?tex=s%3D4" alt="[公式]"> 的情况，在这里我们只考虑 <img src="https://www.zhihu.com/equation?tex=s%3D2" alt="[公式]"> 的情况。</p>
<p>-————————————————————————————————-</p>
<p>由一元函数Taylor展开可得</p>
<p><img src="https://www.zhihu.com/equation?tex=u%28t_n%2Bh%29-u%28t_n%29%3Du%27%28t_n%29h%2Bu%27%27%28t_n%29%5Cfrac%7Bh%5E2%7D%7B2%7D%2BO%28h%5E3%29" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3Df%28t_n%2Cu%28t_n%29%29h%2B%5CBig%5Bf_t%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2Bf_u%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29f%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%5CBig%5D%5Cfrac%7Bh%5E2%7D%7B2%7D%2BO%28h%5E3%29" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3Dhf%28t_n%2Cu%28t_n%29%29%2B%5Cfrac%7Bh%5E2%7D%7B2%7Df_t%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2B%5Cfrac%7Bh%5E2%7D%7B2%7Df_u%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29f%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2BO%28h%5E3%29" alt="[公式]"> </p>
<p>由二元函数的Taylor展开可得</p>
<p><img src="https://www.zhihu.com/equation?tex=K_2%3Df%28t_n%2Ba_2h%2Cu%28t_n%29%2B%5Cbeta_%7B21%7DK_1h%29" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%3Df%28t_n%2Cu%28t_n%29%29%2Bf_t%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%5Calpha_2h%2Bf_u%28t_n%2Cu%28t_n%29%29%5Cbeta_%7B21%7DK_1h" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%2BO%28h%5E2%29" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%3Df%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2Bf_t%28t_n%2Cu%28t_n%29%29%5Calpha_2h%2Bf%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29f_u%28t_n%2Cu%28t_n%29%29%5Cbeta_%7B21%7Dh" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%2BO%28h%5E2%29%5C+%28RK.2%29" alt="[公式]"> </p>
<p>将 <img src="https://www.zhihu.com/equation?tex=%28RK.2%29" alt="[公式]"> 代入 <img src="https://www.zhihu.com/equation?tex=%281.2.4%29" alt="[公式]"> 可得 </p>
<p><img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D-u%28t_n%29%3Dh%28W_1K_1%2BW_2K_2%29%3DhW_1K_1%2BhW_2K_2" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=%3DhW_1f%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2BhW_2%5CBig%28f%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2Bf_t%28t_n%2Cu%28t_n%29%29%5Calpha_2h" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%2Bf%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29f_u%28t_n%2Cu%28t_n%29%29%5Cbeta_%7B21%7Dh%2BO%28h%5E2%29%5CBig%29" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=%3DhW_1f%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2BhW_2f%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2Bh%5E2W_2%5Calpha_2f_t%28t_n%2Cu%28t_n%29%29" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=%2Bh%5E2W_2%5Cbeta_%7B12%7Df%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29f_u%28t_n%2Cu%28t_n%29%29%2BO%28h%5E3%29%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%3D%28W_1%2BW_2%29hf%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29%2BW_2%5Calpha_2h%5E2f_t%28t_n%2Cu%28t_n%29%29" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%2BW_2%5Cbeta_%7B12%7Dh%5E2f%5Cbig%28t_n%2Cu%28t_n%29%5Cbig%29f_u%28t_n%2Cu%28t_n%29%29%2BO%28h%5E3%29" alt="[公式]"> </p>
<p>比较 <img src="https://www.zhihu.com/equation?tex=%28RK.1%29" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=%28RK.3%29" alt="[公式]"> ，使 <img src="https://www.zhihu.com/equation?tex=h%5Er%28r%3D1%2C2%29" alt="[公式]"> 的系数相等，得</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+W_1%2BW_2%3D1%5C%5C+W_2%5Calpha_2%3D%5Cfrac%7B1%7D%7B2%7D%5C%5C+W_2%5Cbeta_%7B21%7D%3D%5Cfrac%7B1%7D%7B2%7D%5C%5C+%5Cend%7Bcases%7D" alt="[公式]"> ，即 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+W_1%2BW_2%3D1%5C%5C+W_2%5Calpha_2%3D%5Cfrac%7B1%7D%7B2%7D%5C%5C+%5Cbeta_%7B21%7D%3D%5Calpha_2%5C%5C+%5Cend%7Bcases%7D" alt="[公式]"> </p>
<p>取不同的 <img src="https://www.zhihu.com/equation?tex=%5Calpha_2" alt="[公式]"> ，可以得到不同的2级RK公式。</p>
<p>（以上 <img src="https://www.zhihu.com/equation?tex=s%3D2" alt="[公式]"> 情况的推导与书本无关）</p>
<p>-————————————————————————————————-</p>
<p>…</p>
<p>含 <img src="https://www.zhihu.com/equation?tex=h%5Er%28r%3D1%2C2%2C3%2C4%29" alt="[公式]"> 项的系数对应相等，且一般不能再使含 <img src="https://www.zhihu.com/equation?tex=h%5E5" alt="[公式]">项的系数相等，故所得方法的局部截断误差为 <img src="https://www.zhihu.com/equation?tex=O%28h%5E5%29" alt="[公式]"> ，即方法是四阶的。</p>
<p>…</p>
<p>从这些公式的讨论得知，当 <img src="https://www.zhihu.com/equation?tex=s%5Cleq4" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=s" alt="[公式]"> 级RK方法的阶 <img src="https://www.zhihu.com/equation?tex=q%28s%29%3Ds" alt="[公式]"> ；当 <img src="https://www.zhihu.com/equation?tex=s%3D5%2C6%2C7" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=q%28s%29%3Ds-1" alt="[公式]"> ；当 <img src="https://www.zhihu.com/equation?tex=s%3D8%2C9" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=q%28s%29%3Ds-2" alt="[公式]"> ； <img src="https://www.zhihu.com/equation?tex=q%2810%29%5Cleq8" alt="[公式]"> 。可见，对于四级以上的公式，计算函数值的计算量增加较快，而精度即阶数提高较慢，因此在实际问题中比较常用的是四级RK公式。</p>
<p><strong>1.2.3 单步法的相容性与收敛性</strong></p>
<p>无论是Euler法还是各级RK法，他们都是在已知 <img src="https://www.zhihu.com/equation?tex=u_n" alt="[公式]"> 的条件下算出 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D" alt="[公式]"> ，因此称为显式单步方法，简称为单步方法。这类方法的一般形式为 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D%3Du_n%2Bh%5Cvarphi%28t_n%2Cu_n%2Ch%29%5C+%281.2.28%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Cvarphi%28t%2Cu%2Ch%29" alt="[公式]"> 称为方法 <img src="https://www.zhihu.com/equation?tex=%281.2.28%29" alt="[公式]"> 的增量函数，它是依赖于 <img src="https://www.zhihu.com/equation?tex=u" alt="[公式]"> 及 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 的非线性函数（此处为简单计，略写了 <img src="https://www.zhihu.com/equation?tex=%5Cvarphi" alt="[公式]"> 对 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 的依赖关系）。</p>
<p>在前面的讨论中，我们一再提到了每个具体方法的阶，现对单步方法 <img src="https://www.zhihu.com/equation?tex=%281.2.8%29" alt="[公式]"> 给出阶的一般定义。</p>
<p>定义1.2.1 单步方法 <img src="https://www.zhihu.com/equation?tex=%281.2.28%29" alt="[公式]"> 称为是 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 阶的，如果对于真解 <img src="https://www.zhihu.com/equation?tex=u%28t%29" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 是使关系式 <img src="https://www.zhihu.com/equation?tex=u%28t%2Bh%29-u%28t%29%3Dh%5Cvarphi%28t%2Cu%28t%29%2Ch%29%2BO%28h%5E%7Bq%2B1%7D%29" alt="[公式]"> 成立的最大整数。</p>
<p>…</p>
<h2 id="1-3-线性多步方法"><a href="#1-3-线性多步方法" class="headerlink" title="1.3 线性多步方法"></a><strong>1.3 线性多步方法</strong></h2><p>在前面所讨论的单步方法中，为提高截断误差的阶，每个时间布必须增加计算右端 <img src="https://www.zhihu.com/equation?tex=f%28t%2Cu%29" alt="[公式]"> 的次数。当 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 的结构比较复杂时，计算量会比较大。现在指出另一个提高截断误差阶的办法：构造这样的方法，在计算公式中 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D-u_%7Bn%7D" alt="[公式]"> 不仅依赖于 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%7D" alt="[公式]"> ，且也直接依赖于 <img src="https://www.zhihu.com/equation?tex=u_%7Bn-1%7D%2Cn_%7Bn-2%7D" alt="[公式]"> 等已算出的值，但每进一步，只计算一次 <img src="https://www.zhihu.com/equation?tex=f%28t%2Cu%29" alt="[公式]"> 的值。这样的方法称为多步方法。记 <img src="https://www.zhihu.com/equation?tex=f_n%3Df%28t_n%2Cu_n%29" alt="[公式]"> ，若函数值 <img src="https://www.zhihu.com/equation?tex=f_n%2Cf_%7Bn-1%7D" alt="[公式]"> 等以线性组合的形式出现于公式中，则称方法为线性多步方法。</p>
<p><strong>1.3.1 数值积分法</strong></p>
<p><strong>仍然考虑初值问题</strong> <img src="https://www.zhihu.com/equation?tex=%281.0.1%29" alt="[公式]"> 。易知 <img src="https://www.zhihu.com/equation?tex=u%28t_%7Bn%2Bk%7D%29-u%28t_%7Bn-j%7D%29%3D%5Cint%5E%7Bt_%7Bn%2Bk%7D%7D_%7Bt_%7Bn-j%7D%7Df%28t%2Cu%28t%29%29dt%5C+%281.3.1%29" alt="[公式]"></p>
<p>我们用被积函数 <img src="https://www.zhihu.com/equation?tex=f%28t%2Cu%28t%29%29" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 次Lagrange插值多项式 <img src="https://www.zhihu.com/equation?tex=P_q%28t%29%3D%5Csum%5E%7Bq%7D_%7Bi%3D0%7Df%28t_%7Bn-i%7D%2Cu%28t_%7Bn-i%7D%29%29L_i%28t%29" alt="[公式]"> 来近似替代 <img src="https://www.zhihu.com/equation?tex=%281.3.1%29" alt="[公式]"> 中的被积函数，其中<img src="https://www.zhihu.com/equation?tex=L_i%28t%29%3D%5Cprod_%7Bl%3D0%2Cl%5Cneq+i%7D%5E%7Bq%7D%5Cfrac%7Bt-t_%7Bn-l%7D%7D%7Bt_%7Bn-i%7D-t_%7Bn-l%7D%7D%3D%5Cprod_%7Bl%3D0%2Cl%5Cneq+i%7D%5E%7Bq%7D%5Cfrac%7Bt-t_n%2Bt_n-t_%7Bn-l%7D%7D%7Bt_%7Bn-i%7D-t_%7Bn-l%7D%7D" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3D%5Cprod_%7Bl%3D0%2Cl%5Cneq+i%7D%5E%7Bq%7D%5Cfrac%7Bt-t_n%2Blh%7D%7B%28-i%2Bl%29h%7D" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3D%5Cprod_%7Bl%3D0%2Cl%5Cneq+i%7D%5E%7Bq%7D%5Cfrac%7B%5Cfrac%7Bt-t_n%7D%7Bh%7D%2Bl%7D%7B%28-i%2Bl%29%7D" alt="[公式]"> ，而有下面的恒等式成立<img src="https://www.zhihu.com/equation?tex=%5Cint%5E%7Bt_%7Bn%2Bk%7D%7D_%7Bt_%7Bn-j%7D%7DL_i%28t%29dt%3D%5Cprod_%7Bl%3D0%2Cl%5Cneq+i%7D%5E%7Bq%7D%5Cint%5E%7Bt_%7Bn%7D%2Bkh%7D_%7Bt_%7Bn%7D-jh%7D%5Cfrac%7B%5Cfrac%7Bt-t_n%7D%7Bh%7D%2Bl%7D%7B-i%2Bl%7Ddt+%3Dh%5Cprod_%7Bl%3D0%2Cl%5Cneq+i%7D%5E%7Bq%7D%5Cint%5E%7Bk%7D_%7B-j%7D%5Cfrac%7Bs%2Bl%7D%7B-i%2Bl%7Dds%2C%5C+" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=i%3D0%2C1%2C...%2Cq" alt="[公式]"> ，插值节点是 <img src="https://www.zhihu.com/equation?tex=t_%7Bn-q%7D%2C...%2Ct_n" alt="[公式]"> 。</p>
<p>于是得到近似公式</p>
<p><img src="https://www.zhihu.com/equation?tex=u%28t_%7Bn%2Bk%7D%29-u%28t_%7Bn-j%7D%29%5Capprox%5Csum%5E%7Bq%7D_%7Bi%3D0%7Df%5Cbig%28t_%7Bn-i%7D%2Cu%28t_%7Bn-i%7D%29%5Cbig%29%5Cint%5E%7Bt_%7Bn%2Bk%7D%7D_%7Bt_%7Bn-j%7D%7DL_i%28t%29dt" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3Dh%5Csum%5E%7Bq%7D_%7Bi%3D0%7D%5CBigg%28%5Cprod_%7Bl%3D0%2Cl%5Cneq+i%7D%5E%7Bq%7D%5Cint%5E%7Bk%7D_%7B-j%7D%5Cfrac%7Bs%2Bl%7D%7B%28-i%2Bl%29%7Dds%5CBigg%29f%5Cbig%28t_%7Bn-i%7D%2Cu%28t_%7Bn-i%7D%29%5Cbig%29" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3Dh%5Csum%5E%7Bq%7D_%7Bi%3D0%7D%5Cbeta_%7Bqi%7Df%5Cbig%28t_%7Bn-i%7D%2Cu%28t_%7Bn-i%7D%29%5Cbig%29%5C+%281.3.2%29" alt="[公式]"> </p>
<p>在式 <img src="https://www.zhihu.com/equation?tex=%281.3.2%29" alt="[公式]"> 中，用 <img src="https://www.zhihu.com/equation?tex=u_n" alt="[公式]"> 代替 <img src="https://www.zhihu.com/equation?tex=u%28t_n%29" alt="[公式]"> ，仍用 <img src="https://www.zhihu.com/equation?tex=f_n" alt="[公式]"> 表示 <img src="https://www.zhihu.com/equation?tex=f%28t_n%2Cu_n%29" alt="[公式]"> ，用等号代替 <img src="https://www.zhihu.com/equation?tex=%5Capprox" alt="[公式]"> ，则得到线性多步法公式 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2Bk%7D%3Du_%7Bn-j%7D%2Bh%5Csum%5E%7Bq%7D_%7Bi%3D0%7D%5Cbeta_%7Bqi%7Df_%7Bn-i%7D%5C+%281.3.4%29" alt="[公式]"> </p>
<p>对 <img src="https://www.zhihu.com/equation?tex=k%2Cj" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 的不同选择，得到不同类型的具体公式。对 <img src="https://www.zhihu.com/equation?tex=k%3D1%2Cj%3D0" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=q%3D0%2C1%2C..." alt="[公式]"> ，我们得到Adams显式方法</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+u_%7Bn%2B1%7D%3Du_n%2Bh%28%5Cbeta_%7Bq0%7Df_n%2B%5Cbeta_%7Bq1%7Df_%7Bn-1%7D%2B...%2B%5Cbeta_%7Bqq%7Df_%7Bn-q%7D%29%5C%5C+%5Cbeta_%7Bqi%7D%3D%5Cint%5E%7B1%7D_%7B0%7D%5Cprod%5E%7Bq%7D_%7Bl%3D0%2Cl%5Cneq+1%7D%5Cfrac%7Bs%2Bl%7D%7B-i%2Bl%7Dds%2C%5C+i%3D0%2C1%2C...%2Cq%5C%5C+%5Cend%7Bcases%7D%5C+%281.3.5%29" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=q%2B1" alt="[公式]"> 步Adams显式方法和 <img src="https://www.zhihu.com/equation?tex=q" alt="[公式]"> 步Admas隐式方法的局部截断误差都与 <img src="https://www.zhihu.com/equation?tex=h%5E%7Bq%2B2%7D" alt="[公式]"> 同阶，即他们是<img src="https://www.zhihu.com/equation?tex=q%2B1" alt="[公式]"> 阶方法。</p>
<p><strong>1.3.2 待定系数法</strong></p>
<p>线性 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 步方法的一般形式为 <img src="https://www.zhihu.com/equation?tex=%5Csum%5E%7Bk%7D_%7Bj%3D0%7D%5Calpha_ju_%7Bn%2Bj%7D%3Dh%5Csum%5E%7Bk%7D_%7Bj%3D0%7D%5Cbeta_jf_%7Bn%2Bj%7D" alt="[公式]"> ，这里 <img src="https://www.zhihu.com/equation?tex=%5Calpha_j" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Cbeta_j" alt="[公式]"> 为实常数（注意这个 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 先被提了出来），且最大步长系数 <img src="https://www.zhihu.com/equation?tex=%5Calpha_k%5Cneq0" alt="[公式]"> ，<img src="https://www.zhihu.com/equation?tex=%7C%5Calpha_0%7C%2B%7C%5Cbeta_0%7C%3E0" alt="[公式]"> （其他系数都有可能为0）。为讨论方便，改为以 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2Bk%7D" alt="[公式]"> 为未知量， <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2Bk-1%7D%2C...%2Cu_n" alt="[公式]"> 为已知量。我们用待定系数法来构造这种公式。</p>
<p>…</p>
<h2 id="1-6-数值稳定性"><a href="#1-6-数值稳定性" class="headerlink" title="1.6 数值稳定性"></a><strong>1.6 数值稳定性</strong></h2><p>前面各节，对一个数值方法进行理论分析时，引进了相容性，收敛性等有关概念。此时，我们作出过共同的假设，即 <img src="https://www.zhihu.com/equation?tex=h%5Crightarrow0" alt="[公式]"> 。这些概念在数值分析中虽然很重要，但还没有充分考虑到实际计算过程的主要特征：步长 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 由于计算工具等条件的限制，不可能任意取小，更不要说趋于零。另一方面，由于每计算一步，总会产生舍入误差，而且这些舍入误差还要步步传播下去，步数增多就有可能使误差累积到影响数值解的精度，甚至于淹没了所求的近似解。因此，从实际计算的角度来看，反而要求步长 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 尽可能大些。对有限的步长 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> ，考察一个方法对舍入误差的敏感性，这就是数值稳定性问题。这不但与计算公式有关，而且与微分方程本身的性质有关。</p>
<h3 id="1-6-1-线性多步方法的绝对稳定性"><a href="#1-6-1-线性多步方法的绝对稳定性" class="headerlink" title="1.6.1 线性多步方法的绝对稳定性"></a><strong>1.6.1 线性多步方法的绝对稳定性</strong></h3><p>首先考察一个数值例子。</p>
<p>…</p>
<p>从这个数值表看到，当用有限步长 <img src="https://www.zhihu.com/equation?tex=h%3D0.1" alt="[公式]"> 进行 <img src="https://www.zhihu.com/equation?tex=40" alt="[公式]"> 余步的计算之后，所得数值解与真解之值偏离相当大，已无法作为近似解提供使用。但我们知道，Milne方法 <img src="https://www.zhihu.com/equation?tex=%281.6.1%29" alt="[公式]"> 是收敛的二步四阶方法，在所有的二步方法中精度阶最高，应该给出较好的数值计算结果。为什么会出现这种反常的不稳定现象？一个方法具有什么性质才能避免上述不稳定现象？这正是本节要讨论的问题。</p>
<p>首先假设所考虑的线性多步方法是收敛的。对于一般的线性 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 步方法 <img src="https://www.zhihu.com/equation?tex=%5Csum%5E%7Bk%7D_%7Bj%3D0%7D%5Calpha_ju_%7Bn%2Bj%7D%3Dh%5Csum%5E%7Bk%7D_%7Bj%3D0%7D%5Cbeta_jf_%7Bn%2Bj%7D%5C+%281.6.2%29" alt="[公式]"> </p>
<p>…</p>
<p>为了对 <img src="https://www.zhihu.com/equation?tex=e_n" alt="[公式]"> 的性质进行分析，我们作两个简化问题的假设：…；二是 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+u%7D%3D%5Cmu%3D%5Crm+const." alt="[公式]"> ，即此时所考虑的是模型方程 <img src="https://www.zhihu.com/equation?tex=u%27%3D%5Cmu+u" alt="[公式]"> 的求解问题。</p>
<p>…</p>
<p>记 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7Bh%7D%3D%5Cmu+h" alt="[公式]"> ，用 <img src="https://www.zhihu.com/equation?tex=%5Cxi_j%3D%5Cxi_j%28%5Coverline%7Bh%7D%29" alt="[公式]"> 表示稳定多项式 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28%5Clambda%3B%5Coverline%7Bh%7D%29%5Cequiv%5Crho%28%5Clambda%29-%5Coverline%7Bh%7D%5Csigma%28%5Clambda%29%5C+%281.6.7%29" alt="[公式]"> 的零点，其中 <img src="https://www.zhihu.com/equation?tex=%5Crho%28%5Clambda%29%3D%5Clambda%5Ek%2B%5Csum%5E%7Bk-1%7D_%7Bj%3D0%7D%5Calpha_j%5Clambda%5Ej%2C%5C+%5Csigma%28%5Clambda%29%3D%5Csum%5E%7Bk%7D_%7Bj%3D0%7D%5Cbeta_j%5Clambda%5Ej" alt="[公式]"> 。</p>
<p>…</p>
<p>我们感兴趣的不是对 <img src="https://www.zhihu.com/equation?tex=e_n" alt="[公式]"> 大小的估计，而是判定当 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 增大时， <img src="https://www.zhihu.com/equation?tex=e_n" alt="[公式]"> 是随着增大还是减小或是振荡的问题。若 <img src="https://www.zhihu.com/equation?tex=e_n" alt="[公式]"> 是在减小，那就是说每步计算所产生的舍入误差对以后计算结果的影响在减弱，即可受到控制。据此，我们引进绝对稳定性的概念。</p>
<p><strong>定义1.6.1</strong> 若对于给定的 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7Bh%7D%3D%5Cmu+h" alt="[公式]"> ，稳定多项式 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28%5Clambda%3B%5Coverline%7Bh%7D%29" alt="[公式]"> 的所有根 <img src="https://www.zhihu.com/equation?tex=%5Cxi_j" alt="[公式]"> 都满足 <img src="https://www.zhihu.com/equation?tex=%7C%5Cxi_j%7C%3C1%2Cj%3D1%2C...%2Ck" alt="[公式]"> ，则称方法 <img src="https://www.zhihu.com/equation?tex=%281.6.2%29" alt="[公式]"> 关于 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7Bh%7D" alt="[公式]"> 是绝对稳定的；若对实轴上的某区间（或复平面上某区域）内的<strong>任意</strong> <img src="https://www.zhihu.com/equation?tex=%5Coverline%7Bh%7D" alt="[公式]"> ，方法都是绝对稳定的，则称此区间（或区域）为绝对稳定区间（或区域）。</p>
<p><strong>Euler法</strong></p>
<p>计算公式为 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D%3Du_n%2Bhf_n" alt="[公式]"> 。对模型方程 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bdu%7D%7Bdt%7D%3Df%3D%5Cmu+u" alt="[公式]"> ，可化为 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D-u_n%3Dh%5Cmu+u_n" alt="[公式]"> </p>
<p>这是线性 <img src="https://www.zhihu.com/equation?tex=1" alt="[公式]"> 步方法， <img src="https://www.zhihu.com/equation?tex=k%3D1" alt="[公式]">， <img src="https://www.zhihu.com/equation?tex=%5Calpha_0%3D-1%2C%5Calpha_1%3D1%2C" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%5Cbeta_0%3D%5Cfrac%7B%5Cmu%7D%7Bh%7D%2C%5Cbeta_1%3D0" alt="[公式]"> ，<img src="https://www.zhihu.com/equation?tex=%5Crho%28%5Clambda%29%3D%5Clambda%2B%5Calpha_0%3D%5Clambda-1%2C%5Csigma%28%5Clambda%29%3D%5Cfrac%7B%5Cmu%7D%7Bh%7D" alt="[公式]"></p>
<p>稳定多项式为 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28%5Clambda%3B%5Coverline%7Bh%7D%29%3D%5Crho%28%5Clambda%29-%5Coverline%7Bh%7D%5Csigma%28%5Clambda%29%3D%5Clambda-1-%5Coverline%7Bh%7D%5Cfrac%7B%5Cmu%7D%7Bh%7D%3D%5Clambda-%281%2B%5Coverline%7Bh%7D%29" alt="[公式]"> </p>
<p>绝对稳定区域满足 <img src="https://www.zhihu.com/equation?tex=%7C1%2B%5Coverline%7Bh%7D%7C%3C1" alt="[公式]"> 。当 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 为复常数时，上式表示复平面上以 <img src="https://www.zhihu.com/equation?tex=-1" alt="[公式]"> 为中心的单位圆域内部。当 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]">  为实常数时，绝对稳定区间为实轴上的线段 <img src="https://www.zhihu.com/equation?tex=%28-2%2C0%29" alt="[公式]"> 。</p>
<p><strong>向后Euler法</strong></p>
<p>计算公式为 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D%3Du_n%2Bhf_%7Bn%2B1%7D" alt="[公式]"> 。稳定多项式 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28%5Clambda%3B%5Coverline%7Bh%7D%29%3D%281-%5Coverline%7Bh%7D%29%5Clambda-1" alt="[公式]"> 。绝对稳定区间满足 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B%7C1-%5Coverline%7Bh%7D%7C%7D%3C1" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%7C1-%5Coverline%7Bh%7D%7C%3E1" alt="[公式]"> ，即方法的绝对稳定区域为复平面上以 <img src="https://www.zhihu.com/equation?tex=%2B1" alt="[公式]"> 为中心的单位圆域外部。特别地，当 <img src="https://www.zhihu.com/equation?tex=Re%5C+%5Cmu%3C0" alt="[公式]"> 时，方法绝对稳定。若 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 为实常数，绝对稳定区间为 <img src="https://www.zhihu.com/equation?tex=%28-%5Cinfty%2C0%29%5Ccup%282%2C%5Cinfty%29" alt="[公式]"> 。</p>
<p><strong>梯形法</strong></p>
<p>计算公式为 <img src="https://www.zhihu.com/equation?tex=u_%7Bn%2B1%7D%3Du_n%2B%5Cfrac%7B1%7D%7B2%7D%28f_n%2Bf_%7Bn%2B1%7D%29" alt="[公式]"> </p>
<p>稳定多项式为 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28%5Clambda%3B%5Coverline%7Bh%7D%29%3D%281-%5Cfrac%7B1%7D%7B2%7D%5Coverline%7Bh%7D%29%5Clambda-%281%2B%5Cfrac%7B1%7D%7B2%7D%5Coverline%7Bh%7D%29" alt="[公式]"> </p>
<p>绝对稳定区间满足 <img src="https://www.zhihu.com/equation?tex=%7C%5Cxi_1%7C%3D%5CBigg%7C%5Cfrac%7B%281%2B%5Cfrac%7B1%7D%7B2%7D%5Coverline%7Bh%7D%29%7D%7B%281-%5Cfrac%7B1%7D%7B2%7D%5Coverline%7Bh%7D%29%7D%5CBigg%7C%3C1" alt="[公式]"> 。当 <img src="https://www.zhihu.com/equation?tex=Re%5C+%5Cmu%3C0" alt="[公式]"> 时，总有 <img src="https://www.zhihu.com/equation?tex=%7C%5Cxi_1%7C%3C1" alt="[公式]"> ，故方法的绝对稳定区间为左半复平面 <img src="https://www.zhihu.com/equation?tex=Re%5C+%5Cmu%3C0" alt="[公式]"> ，而绝对稳定区间为 <img src="https://www.zhihu.com/equation?tex=%28-%5Cinfty%2C0%29" alt="[公式]"> 。</p>
<p><strong>改进Euler法</strong></p>
<p>计算公式为 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+u_%7Bn%2B1%7D%5E%7B%280%29%7D%3Du_n%2Bhf%28t_%7Bn%7D%2Cu_n%29%5C%5C+u_%7Bn%2B1%7D%5E%7B%28m%2B1%29%7D%3Du_n%2B%5Cfrac%7Bh%7D%7B2%7D%5Bf_n%2Bf%28t_%7Bn%2B1%7D%2Cu_%7Bn%2B1%7D%5E%7B%28m%29%7D%29%5D%2Cm%5Cgeq0%5C%5C+%5Cend%7Bcases%7D" alt="[公式]"> </p>
<p>稳定多项式为 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28%5Clambda%3B%5Coverline%7Bh%7D%29%3D%5Clambda-%281%2B%5Coverline%7Bh%7D%2B%5Coverline%7Bh%7D%5E2%29" alt="[公式]"> </p>
<p>绝对稳定区间满足 <img src="https://www.zhihu.com/equation?tex=%7C%5Cxi_1%7C%3D%7C1%2B%5Coverline%7Bh%7D%2B%5Cfrac%7B1%7D%7B2%7D%5Coverline%7Bh%7D%5E2%7C%3D%7C%28%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Coverline%7Bh%7D%2B%5Csqrt%7B2%7D%29%5E2-1%7C%3C1" alt="[公式]"> ，即 <img src="https://www.zhihu.com/equation?tex=%28%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Coverline%7Bh%7D%2B%5Csqrt%7B2%7D%29%5E2%5Cleq2%2C+%5Coverline%7Bh%7D%5Cin+C" alt="[公式]"> ，这是一个复椭圆。绝对稳定区间为 <img src="https://www.zhihu.com/equation?tex=%28-2%2C0%29" alt="[公式]"> 。</p>
<p><strong>二级二阶RK方法</strong></p>
<p>绝对稳定区间为 <img src="https://www.zhihu.com/equation?tex=%28-2%2C0%29" alt="[公式]"> 。</p>
<p><strong>四级四阶RK方法</strong></p>
<p>绝对稳定区间为 <img src="https://www.zhihu.com/equation?tex=%28-2.78%2C0%29" alt="[公式]"> 。</p>
<p>通过本节的讨论可看到，从绝对稳定性的要求来看，对于 <img src="https://www.zhihu.com/equation?tex=Re%5C+%5Cmu%3E0" alt="[公式]"> 的方程，没有什么可靠的算法，因为误差函数随着计算步数 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 在增长。若从另一个角度来讨论问题，由于方法的收敛性，此时数值解必然随着真解的增长而增长，但只要误差函数的增长速度低于数值解的增长速度，而不影响数值解的精度，所得的计算结果也具有使用价值，这就是所谓的相对稳定性问题。</p>
]]></content>
      <categories>
        <category>math</category>
        <category>ODE</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>NeurIPS 22 accepted paper list</title>
    <url>/2022/09/17/NeurIPS22%20accepted%20paper%20list/</url>
    <content><![CDATA[<h2 id="NeurIPS-22-accepted-papers-list"><a href="#NeurIPS-22-accepted-papers-list" class="headerlink" title="NeurIPS 22 accepted papers list"></a>NeurIPS 22 accepted papers list<span id="more"></span></h2><h3 id="EBM"><a href="#EBM" class="headerlink" title="EBM"></a>EBM</h3><ul>
<li>Diffusion Models as Plug-and-Play Priors</li>
<li>video diffusion models</li>
<li>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</li>
<li>On analyzing generative and denoising capabilities of diffusion-based generative models</li>
<li>BinauralGrad: A two stage conditional diffusion probabilistic model for binaural audio synthesis</li>
<li>Thompson sampling efficiently learns to control diffusion process</li>
<li>Generative Time Series Forecasting with Diffusion, Denoise and Disentanglement</li>
<li>CARD: Classification and regression diffusion models</li>
<li>Improve diffusion models for inverse problems using manifold constraints</li>
<li>Elucidating the design space of diffusion-based generative models</li>
<li>Score-based diffusion meets annealed importance sampling</li>
<li>Deep equilibrium approaches to diffusion models</li>
<li>Flexible diffusion modeling of long videos</li>
<li>Conditional diffusion process for inverse halftoning</li>
<li>Diffusion-LM improves controllable text generation</li>
<li>Riemannian diffusion models</li>
<li>Denoising diffusion restoration models</li>
<li>DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps.</li>
<li>First hitting diffusion models</li>
<li>GENIE: High -order denoising diffusion solvers.</li>
<li>Antigen-Specific antibody design and optimization with diffusion-based generative models</li>
<li>Unsupervised representation learning from pre-trained diffusion probabilistic models.</li>
<li>Exponential family model-based reinforcement learning via score matching</li>
<li>Convergence for score-based generative modeling with polynomial complexity.</li>
<li>Score-based models detect manifolds</li>
<li>Concrete score matching: generalized score matching for discrete data.</li>
<li>Score-based generative modeling secretly minimizes the Wasserstein distance.</li>
<li>Wavelet score-based generative modeling</li>
<li>Riemannian score-based generative modeling.</li>
<li>End-to-end stochastic programming with energy-based model</li>
<li>Adaptive multi-stage density ratio estimation for learning latent space energy-based model</li>
<li>EGSDE: Unpaired image-to-image translation via energy-guided stochastic differential equations.</li>
<li>A continuous time framework for discrete denoising models.</li>
</ul>
<h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><ul>
<li>Masked GANs are robust generation learners.</li>
<li>Improving GANs via adversarial learning in latent spaces.</li>
<li>Amortized projection optimization for sliced wasserstein generative models.</li>
</ul>
<h3 id="Dynamic-systems"><a href="#Dynamic-systems" class="headerlink" title="Dynamic systems"></a>Dynamic systems</h3><h4 id="ODE"><a href="#ODE" class="headerlink" title="ODE"></a>ODE</h4><ul>
<li>Neural differential equations for learning to program neural nets though continuous learning rules.</li>
<li>Do residual neural networks discretize neural ordinary differential equations?</li>
<li>Constraining Gaussian processes to systems of linear ordinary differential equations.</li>
<li>Imrpoving neural ordinary equations with Nesterov’s accelerated gradient method</li>
</ul>
<h4 id="SDE"><a href="#SDE" class="headerlink" title="SDE"></a>SDE</h4><ul>
<li>Learning white noises in neural stochastic differential equations</li>
<li>Riemannian neural SDE: learning stochastic representation on manifolds.</li>
</ul>
<h4 id="PDE"><a href="#PDE" class="headerlink" title="PDE"></a>PDE</h4><ul>
<li>Neural Stochastic PDEs: Resolution invariant learning of continuous spatiotemporal dynamics</li>
</ul>
<h4 id="Control"><a href="#Control" class="headerlink" title="Control"></a>Control</h4><ul>
<li>Neural stochastic control</li>
<li>Markov Chain Score Ascent: A unifying framework of variational inference with Markovian gradients.</li>
</ul>
<h3 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h3><ul>
<li>Self-supervised contrastive pre-training for time seires via time-frequency consistency</li>
<li>BILCO: An efficient algorithm for joint alignment of time series.</li>
<li>GT-GAN: General purpose time series synthesis with generative adversarial networks.</li>
<li>Generative Time Series Forecasting with Diffusion, Denoise and Disentanglement</li>
<li>Non-stationary transformers: rethinking the staionarity in time series forecasting</li>
<li>Time dimension dances with simplicial complexes: Zigzag filtration curve based supra-hodge convolution networks for time-series forecasting</li>
<li>FILM: Frequency improved legendre memory model for long-term time series forecasting.</li>
<li>Learning latent seasonal-trend representations for time series forecasting.</li>
<li>SCINet: Time series modeling and forecasting with sample convolution and interaction.</li>
<li>WaveBound: Dynamically bounding error for stable time series forecasting.</li>
<li>Causal disentanglement for time series.</li>
<li>Dynamic sparse network for time series classification: Learning what to “See”</li>
<li>Efficient learning of nonlinear prediction models with time-series privileged information.</li>
<li>Multivariate time-series forecasting with temporal polynomial graph neural networks.</li>
<li>Dynamic graph neural networks under spatio-temporal distribution shift.</li>
<li>AutoST: Towards the universal modeling of spatio-temporal sequences.</li>
<li>Neural Stochastic PDEs: Resolution invariant learning of continuous spatio-temporal dynamics</li>
<li>Variational context adjustment for temporal event prediction under distribution shifts.</li>
<li>Practical adversarial attacks on spatio-temporal traffic forecasting models.</li>
<li>Quo Vadis: Is trajectory forecasting the key towards long-term multi-objective tracking</li>
<li>Contact-aware human motion forecasting.</li>
<li>Forecasting Human Trajectory from scene history</li>
<li>Motion forecasting transformer with global intention localization and local movement refinement.</li>
<li>Representing spatial trajectories as distributions.</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Process Mining based on the Markov Transition Matrix</title>
    <url>/2018/11/03/Process-Mining-based-on-the-Markov-Transition-Matrix/</url>
    <content><![CDATA[<h3 id="Main-Thoughts"><a href="#Main-Thoughts" class="headerlink" title="Main Thoughts"></a>Main Thoughts</h3><ul>
<li>First analyses the workflow log transition probabilities between activities to build the Markov transition matrix;</li>
<li>Then mines the process logic relationship by defining a set of rules of logical relational</li>
<li>At last design the process mining algorithm to establish the actual structure relationship between the activities in order to reconstruct the workflow.  </li>
</ul>
<h3 id="Methods-to-find-incomplete-logs"><a href="#Methods-to-find-incomplete-logs" class="headerlink" title="Methods to find incomplete logs"></a>Methods to find incomplete logs</h3><ul>
<li>List the set of the end events of the log, if an instance’s log end event does not belong to the set</li>
<li>If a task only has the start event without a corresponding end event</li>
</ul>
<h3 id="Symbol-Description"><a href="#Symbol-Description" class="headerlink" title="Symbol Description"></a>Symbol Description</h3><div class="table-container">
<table>
<thead>
<tr>
<th>symbol</th>
<th>meaning</th>
<th>symbol</th>
<th>meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>n</td>
<td>execution times of workflow</td>
<td>$X = \{x_0,x_1…x_k\}$</td>
<td>set of activities</td>
</tr>
<tr>
<td>P</td>
<td>workflow transition matrix</td>
<td>$X_s$</td>
<td>start process</td>
</tr>
<tr>
<td>$X_E$</td>
<td>end process</td>
<td>&gt;</td>
<td>sequence relationship</td>
</tr>
<tr>
<td>$\rightarrow$</td>
<td>casual relationship</td>
<td>#</td>
<td>selection relationship</td>
</tr>
<tr>
<td>//</td>
<td>synchronous relationship</td>
<td>$\diamond$</td>
<td>circle relationship</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Build-Markov-Transition-Matrix"><a href="#Build-Markov-Transition-Matrix" class="headerlink" title="Build Markov Transition Matrix"></a>Build Markov Transition Matrix</h3><ul>
<li>$p_{ij} = m_{ij}/n$, $m_{ij}$ means the one step transition times from activity $x_i$ to $x_j$ in the n workflow instances</li>
</ul>
<h3 id="Logic-Relationship-Mining-Rules"><a href="#Logic-Relationship-Mining-Rules" class="headerlink" title="Logic Relationship Mining Rules"></a>Logic Relationship Mining Rules</h3><ul>
<li><p>(1) <strong>Start of the process</strong>:  $\exists x_j \in X$, and $\forall x_i~\in~X$, if $p_{ij} = 0$, then $X_s = x_j$</p>
</li>
<li><p>(2) <strong>End of the process</strong>: $\exists x_i \in X$, and $\forall x_j \in X$, if $p_{ij} = 0$, then $X_E = xi$</p>
</li>
<li><p>(3) <strong>Sequence relationship</strong>: $\forall x_i,~x_j \in X$, if $p_{ij} \neq 0$, then $x_i &gt;x_j$</p>
</li>
<li><p>(4) <strong>Casual relationship</strong>: $\forall x_i,~x_j \in X$, if $p_{ij} = 1$, then$x_i \rightarrow x_j$</p>
</li>
<li><p>(5) <strong>Synchronous relationship</strong>:</p>
<ul>
<li><strong>And-Split</strong>: if $p_{i \eta_1} = p_{i\eta_2} = … = p_{i\eta_k}= \frac{1}{k}\neq 0$, and $x_i \neq x_{\eta_k}$, then $\{x_{\eta_1}//x_{\eta_2}//..//x_{\eta_k},x_i &gt; x_{\eta_k}\}$, $x_i$ is the <em>And-Split</em> activity node</li>
<li><strong>And-Join</strong>: if $p_{\eta_1 i} = p_{\eta_2 i}= …=p_{\eta_k} = \frac{1}{k} \neq 0$, and $x_i \neq x_{\eta_k}$, then  $\{x_{\eta_1}//x_{\eta_2}//..//x_{\eta_k}, x_{\eta_k}&gt;x_i \}$, $x_i$ is the <em>And-Join</em> activity node</li>
</ul>
</li>
<li><p>(6) <strong>Selection relationship</strong>:</p>
<ul>
<li><strong>OR-Split</strong>: $\exists x_i, x_{\eta_k} \in X, p_{i\eta_k} &gt;0$, if $\sum_k p_{ij} = 1$, then $\{x_{\eta_1}$# $x_{\eta_2}$# ..# $x_{\eta_k},x_i &gt; x_{\eta_k}\}$, $x_i$ is the <em>OR-Split</em> activity node</li>
<li><strong>OR-Join</strong>:  $\exists x_i, x_{\eta_k} \in X, p_{i\eta_k} &gt;0$, if $\sum_k p_{\eta_ki} = 1$, then $\{x_{\eta_1}$#$x_{\eta_2}$#..#$x_{\eta_k},x_{\eta_k} &gt; x_i\}$, $x_i$ is the <em>OR-Join</em> activity node</li>
</ul>
</li>
<li><p>(7) <strong>Self-circulation relationship</strong>: $\exists x_i \in X, x_{\eta_k} \ in X - \{x_i\}$, if $p_{ii} = p(p \neq 1)$, then $\{\diamond x_i; x_i &gt; x_{\eta_k}\}$</p>
</li>
<li><p>(8) <strong>Multi-step cycle relationship</strong>: </p>
<ul>
<li>$\forall x_i, x_{\eta_k} \in X, x_i \neq X_S, x_i \neq X_E, X_{re} \neq \emptyset$, if  $p_{ij} = 0, p_{ji} &gt; 0~and~\exists k(k\leq n-1)$, which makes $p_{ij}^{(k)} &gt; 0 (p_{ij}^{(k)}\in p^k)$, then there is a <em>multi cycle</em> between $x_i$ and $x_j$, and $x_j &gt; x_i$; $X_{re}$ = $X_{re} +\{x_i,x_j\}$; and do <strong>Refunction()</strong>.</li>
</ul>
<p><img src="/2018/11/03/Process-Mining-based-on-the-Markov-Transition-Matrix/processMining_1.png" alt="processMining_1"></p>
<p>​    While ($k \geq 2$) the activities in $X_{re}$ has cycle relationship, i.e. $x_j \diamond x_i \diamond x_{\eta_1} \diamond …\diamond x_{\eta_{k-3}}$</p>
<ul>
<li><p><strong>The start $X_{rS}$ and end $X_{rE}$ of the multi-step cycle</strong>:</p>
<p>If $\exists x_{\eta_k}\in X_{re}, x_i \in X-X_{re}$, and $p_{i\eta_k} &gt; 0 $, then $\{X_{rS} = X_{\eta_k};x_i &gt; X_{rS}\}$</p>
<p>If $\exists x_{\eta_k}\in X_{re}, x_i \in X-X_{re}$, and $p_{\eta_k i} &gt; 0 $, then $\{X_{rE} = X_{\eta_k};X_{rE}&gt;x_i \}$</p>
</li>
</ul>
<h3 id="Design-of-Process-Mining-Algorithm-Based-on-the-Rules"><a href="#Design-of-Process-Mining-Algorithm-Based-on-the-Rules" class="headerlink" title="Design of Process Mining Algorithm Based on the  Rules"></a>Design of Process Mining Algorithm Based on the  Rules</h3><ul>
<li><strong>INPUT</strong>: Markov matrix P, activities set X</li>
<li><strong>OUTPUT</strong>: $W_{and} = \{(x_{\eta_i},x_{\eta_2},…)|x_{\eta_1}//x_{\eta_2}//…\}$, $W = \{(x_i,x_j)|x_i\rightarrow x_j~or~x_i &gt;x_j~or~ x_i \diamond x_j\}$,$W_{select} = \{(x_{\eta_i},x_{\eta_2},…)|$$x_{\eta_1}$#$x_{\eta_2}$#$ …\}$</li>
</ul>
<p>​</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Process Mining</category>
        <category>Markov Transition Matrix</category>
      </categories>
      <tags>
        <tag>Process Mining</tag>
        <tag>Markov Transition Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title>TS Anomaly Detection</title>
    <url>/2020/09/12/TS-Anomaly-Detection/</url>
    <content><![CDATA[<blockquote>
<p>PaperList for TS anomaly detection <span id="more"></span></p>
</blockquote>
<h2 id="Survey-Paper"><a href="#Survey-Paper" class="headerlink" title="Survey Paper"></a>Survey Paper</h2><ul>
<li>Deep Learning for Anomaly Detection: A Survey  | <strong>[arXiv’ 19]</strong> |<a href="https://arxiv.org/pdf/1901.03407.pdf"><code>[pdf]</code></a></li>
<li>Anomalous Instance Detection in Deep Learning: A Survey | <strong>[arXiv’ 20]</strong> |<a href="https://arxiv.org/pdf/2003.06979.pdf"><code>[pdf]</code></a></li>
</ul>
<h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><ul>
<li><a href="#time-series-anomaly-detection">Time-series anomaly detection</a></li>
</ul>
<h2 id="Time-series-anomaly-detection-need-to-survey-more"><a href="#Time-series-anomaly-detection-need-to-survey-more" class="headerlink" title="Time-series anomaly detection (need to survey more..)"></a>Time-series anomaly detection <strong>(need to survey more..)</strong></h2><ul>
<li>Anomaly Detection of Time Series  | <strong>[Thesis’ 10]</strong> |<a href="https://conservancy.umn.edu/bitstream/handle/11299/92985/Cheboli_Deepthi_May2010.pdf?sequence=1"><code>[pdf]</code></a></li>
<li>Long short term memory networks for anomaly detection in time series | <strong>[ESANN’ 15]</strong> |<a href="https://www.researchgate.net/publication/304782562_Long_Short_Term_Memory_Networks_for_Anomaly_Detection_in_Time_Series"><code>[pdf]</code></a><ul>
<li>LSTM-Based System-Call Language Modeling and Robust Ensemble Method for Designing Host-Based Intrusion Detection Systems | <strong>[arXiv’ 16]</strong> |   <a href="https://arxiv.org/pdf/1611.01726.pdf"><code>[pdf]</code></a></li>
</ul>
</li>
<li>Time Series Anomaly Detection; Detection of anomalous drops with limited features and sparse examples in noisy highly periodic data | <strong>[arXiv’ 17]</strong> |   <a href="https://arxiv.org/ftp/arxiv/papers/1708/1708.03665.pdf"><code>[pdf]</code></a></li>
<li>Anomaly Detection in Multivariate Non-stationary Time Series for Automatic DBMS Diagnosis | <strong>[ICMLA’ 17]</strong> | <a href="https://arxiv.org/abs/1708.02635"><code>[pdf]</code></a></li>
<li>Truth Will Out: Departure-Based Process-Level Detection of Stealthy Attacks on Control Systems  | <strong>[ACM CCS ‘18]</strong> | <a href="https://research.chalmers.se/publication/507989/file/507989_Fulltext.pdf"><code>[pdf]</code></a></li>
<li>Time-Series Anomaly Detection Service at Microsoft  | <strong>[KDD’ 19]</strong> |  <a href="https://arxiv.org/pdf/1906.03821v1.pdf"><code>[pdf]</code></a></li>
<li>Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network  | <strong>[KDD’ 19]</strong> |  <a href="https://dl.acm.org/doi/10.1145/3292500.3330672"><code>[pdf]</code></a></li>
<li>A Systematic Evaluation of Deep Anomaly Detection Methods for Time Series | <strong>Under Review</strong> | <a href="https://github.com/KDD-OpenSource/DeepADoTS"><code>[code]</code></a></li>
<li>BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time | <strong>[IJCAI 19]</strong> | <a href="https://www.ijcai.org/Proceedings/2019/0616.pdf"><code>[pdf]</code></a></li>
<li>MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams  | <strong>[AAAI’ 20]</strong> |  <a href="https://www.comp.nus.edu.sg/~sbhatia/assets/pdf/midas.pdf"><code>[pdf]</code></a> | <a href="https://github.com/bhatiasiddharth/MIDAS"><code>[code]</code></a></li>
</ul>
<ul>
<li>Unsupervised Anomaly Detection for Intricate KPIs via Adversarial Training of VAE</li>
<li>DEEP AUTOENCODING GAUSSIAN MIXTURE MODEL FOR UNSUPERVISED ANOMALY DETECTION <a href="https://github.com/RomainSabathe/dagmm">code</a></li>
<li>A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data <a href="https://github.com/LZhengyu/MSCRED">code</a></li>
<li>Sequential Anomaly Detection using Inverse Reinforcement Learning</li>
</ul>
<blockquote>
<p>参考 <a href="https://github.com/yzhao062/anomaly-detection-resources">链接</a></p>
</blockquote>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>Applications</tag>
      </tags>
  </entry>
  <entry>
    <title>VIM快捷键配置</title>
    <url>/2018/12/04/VIM%E5%BF%AB%E6%8D%B7%E9%94%AE%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h3 id="VIM常用快捷键设置"><a href="#VIM常用快捷键设置" class="headerlink" title="VIM常用快捷键设置"></a>VIM常用快捷键设置</h3><ul>
<li>,nn:打开文件树<br>​    打开文件树之后，在新的分栏打开光标文件按s，在新的tab打开用t</li>
<li>空格：查找</li>
<li>gt：切换tab</li>
<li><c-h j k l>切换分栏</c-h></li>
<li>shift+3: 查找当前光标位置的字符</li>
<li>调整分栏占比：<br>:vertical res 50  # 纵向分栏<br>:res 50  # 横向分栏</li>
</ul>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Interesting Things</tag>
      </tags>
  </entry>
  <entry>
    <title>Yading: fast clustering of large-scale time series data</title>
    <url>/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li><p>包括三个步骤：</p>
<ul>
<li>(1)从数据集中采样;</li>
<li>(2)在采样的数据集上面进行聚类算法;</li>
<li>(3)将数据集中其他的数据根据采样数据集的类别进行分类</li>
</ul>
</li>
<li><p>现在的聚类算法可以大致分为两类：根据<strong>相似性度量</strong>是否是直接用在输入的数据集上的还是用在从数据集中提取的特征上的</p>
<ul>
<li>第一类： Golay et al 研究了三种时间序列的相似性度量，欧氏距离，两个基于互相关性的距离； Liao et all 采用DTW(Dynamic Time Warping)动态时间弯曲和遗传聚类来对时间序列进行分类</li>
<li>第二类：是基于时间序列是根据内在的模型和概率分布的假设来进行聚类的。比如ARMA(Auto-Aggressive Integrated Moving Average)算法,高斯混合算法，缺点是是模型的学习的计算复杂度很高</li>
</ul>
</li>
<li><p>相似性度量：</p>
<ul>
<li><p>(1)Lp范数： 其中L1范数对于冲击噪声是鲁棒的；但是对于$p\geq 3$来说会遇到维数灾问题</p>
</li>
<li><p>(2)Pearson’s correlation：取值范围是[-1,1],两个变量之间的<strong>皮尔逊相关系数</strong>定义为两个变量之间的协方差和标准差的商：</p>
<script type="math/tex; mode=display">
\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_x,\sigma_y}</script><p>上式是<strong>总体</strong>皮尔逊相关系数，下面是样本皮尔逊相关系数，一般用r表示：</p>
<script type="math/tex; mode=display">
r = \frac{\sum^{n}_{i=1}(X_i-\bar X)(Y_i-\bar Y)}{\sqrt{\sum^{n}_{i=1}(X_i-\bar X)^2}\sqrt{\sum^n_{i=1}(Y_i-\bar Y)^2}}=\frac{1}{n-1}\sum^n_{i=1}(\frac{X_i-\bar X}{\sigma_x})(\frac{Y_i-\bar Y}{\sigma_Y})</script></li>
<li><p>(3)DTW(动态时间弯曲距离)：</p>
<p><img src="/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/2.png" alt="2"></p>
</li>
</ul>
</li>
<li><p>Data Reduction:分为降维或者减少时间序列的数量(size)</p>
<ul>
<li><p>离散傅里叶变换(DFT)用于在频域中表示时间序列；离散小波变换用于提供额外的信息；奇异值分解(SVD)和分段聚合近似（PAA）</p>
<p><img src="/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/1.jpg" alt="1"></p>
<p><img src="/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/3.jpg" alt="1"></p>
</li>
<li><p>采样可以减少时间序列数据集的大小：随机采样/有偏采样</p>
</li>
</ul>
</li>
</ul>
<h2 id="算法详解"><a href="#算法详解" class="headerlink" title="算法详解"></a>算法详解</h2><h3 id="Data-Reduction"><a href="#Data-Reduction" class="headerlink" title="Data Reduction"></a>Data Reduction</h3><ul>
<li><p>N个时间序列，$T_{N\times D}:=\{T_1,T_2,…,T_N\}$,每个时间序列是一个D维的序列，现在就是要确定采样序列$T_{s\times d}，s\leq N,d\leq D$</p>
</li>
<li><p>提出了关于s选择的上下界的理论证明，可以引导我们选择合适的s：</p>
<p>假设所有$T_i\in T_{N\times D}$属于k个已知的分类，$n_i$代表第i个分组中时间序列的个数，$p_i = \frac{n_i}{N}$代表第i个分组的占比，$p_i’=\frac{n_i’}{s}$代表采样数据集中第i个分组的占比，则$|p_i-p_i’|$代表了数据集和采样数据集中的占比偏差。现在定义s的上界$s_l$和下界$s_u$。</p>
<p>对于给定的$\epsilon$和置信水平$1-\alpha$</p>
<p>(1)对于$p_i&lt; \epsilon$的类来说，对于$s&lt;s_l$不能保证有足够多的instance数量，直观上解释就是下界越低，那么越有可能找到小型的类</p>
<p>(2)对于$s&gt;s_u$来说，最大的占比偏差应该是在$\epsilon$之内的，直观上，根据大数定律，大于某一个阈值后，样本的分布和总体的分布是一致的</p>
<ul>
<li><p><strong>Lemma 1(lower bound)</strong>:m是采样数据及group i中最小的instance数量，给定置信水平$1-\alpha$，那么采样的size $s\geq \frac{m+z_{\alpha}(\frac{z_{\alpha}}{2}+\sqrt{m+\frac{z_{\alpha}^2}{4}})}{p_i}$满足$P(n_i’\geq m)&gt;1-\alpha$,这里$z_{\alpha/2} = P(Z&gt;z_{\alpha/2})=\alpha/2$其中Z是标准正态分布</p>
<p>注意：m的确定取决于用在采样数据集上的聚类算法</p>
<p><strong>证明</strong>：由于是随机采样，那么一个样本属于第i类的概率为$p_i$,由于每个样本都是独立的，那么在所有采样中，采样属于第i个类的样本服从二项分布$n_i’\sim B (s,p_i)$,当s足够大的时候，二项分布$B(s,p_i) \sim N(sp_i,sp_i(1-p_i))$，此时$n_i’$近似服从$N(sp_i,sp_i(1-p_i))$,那么对于事件$\{n_i’\geq m\}\leftrightarrow \{ \frac{n_i’-sp_i}{\sigma}\geq \frac{m-sp_i}{\sigma}\}\leftrightarrow \{Z\geq \frac{m-sp_i}{\sigma}\}$,故而</p>
<script type="math/tex; mode=display">
P(n_i'\geq m)>1-\alpha \leftrightarrow P(Z\geq \frac{m-sp_i}{\sigma})>1-\alpha \leftrightarrow \frac{m-sp_i}{\sigma}\leq -z_{\alpha},\\
where~~~ m\leq sp_i,~~~\sigma = \sqrt{sp_i(1-p_i)}</script><p>那么根据上面的不等式</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\frac{(m-sp_i)^2}{sp_i(1-p_i)} &\geq& z_\alpha^2\\
m^2+s^2p_i^2-2msp_i&\geq& sp_iz_{\alpha}^2-sp_i^2z_{\alpha}^2\\
\frac{m^2}{s}+sp_i^2-2mp_i&\geq& p_iz_\alpha^2-p_i^2z_\alpha^2\\
p_i^2-2p_i\frac{m+z_{\alpha}^2/2}{s+z_\alpha^2}&\geq& -\frac{m^2}{s(s+z_{\alpha}^2)}\\
(p_i-\frac{m+z_{\alpha}^2/2}{s+z_\alpha^2})^2&\geq&(\frac{m+z_{\alpha}^2/2}{s+z_\alpha^2})^2-\frac{m^2}{s(s+z_\alpha^2)},m\leq sp_i
\end{aligned}
\end{equation}</script><p>由于$z_\alpha$经常落在[0,3]范围内，也就是说$s &gt;&gt;z_\alpha^2\leftrightarrow s+z_{\alpha}^2\approx s   $</p>
<p>因此，得到$s\geq \frac{m+z_{\alpha}(\frac{z_\alpha}{2}+\sqrt{m+\frac{z_\alpha^2}{4}})}{p_i}$</p>
</li>
<li><p><strong>Lemma 2(upper bound)</strong>:对于给定的$\epsilon\in [0,1]$,置信水平为$1-\alpha$,那么$s\geq \frac{z_{\alpha/2}^2}{4\epsilon^2}$能够满足$P(|p_i-p_i’| &lt; \epsilon) &gt; 1-\alpha, 1\leq i\leq k$</p>
<p>定理2说明<strong>s仅取决于给定的阈值和置信水平，和输入数据集的大小无关</strong></p>
<p><strong>Proof</strong>:由于$p_i’ = \frac{n_i’}{s}\sim N(p_i,\frac{p_i(1-p_i)}{s})\leftrightarrow p_i’-p_i\sim N(p_i,\frac{p_i(1-p_i)}{s})$,故而</p>
<script type="math/tex; mode=display">
\{|p_i-p_i'|< \epsilon\}\leftrightarrow \{|Y|<\epsilon\}\leftrightarrow \{|Z|<\frac{\epsilon}{\sigma}\}\\
where~~\sigma =\sqrt{\frac{p_i(1-p_i)}{s}},\\
So~~when~~\frac{\epsilon}{\sigma}>z_{\alpha/2}, ~~P(|Z|<\frac{\epsilon}{\sigma})>1-\alpha\\
s\geq \frac{z_{\alpha/2}^2}{p_i(1-p_i)\epsilon^2}\geq \frac{z_{\alpha/2}^2}{4\epsilon^2}</script></li>
</ul>
</li>
</ul>
<h3 id="降维度"><a href="#降维度" class="headerlink" title="降维度"></a>降维度</h3><ul>
<li><p>采用分段聚合近似(PAA)，从$T_i := (t_{i1},t_{i2},…,t_{iD})$转换为$T_i’ := (\tau_{i1},…,\tau_{id})$公式如下</p>
<script type="math/tex; mode=display">
\tau_{ij} = \frac{d}{D}\sum^{\frac{D}{d}j}_{k = \frac{D}{d}(j-1)+1}t_{ik}</script><p>一个关键的步骤就是要确定d的大小，根据<strong>香农采样定理</strong>，如果最高频率为B，那么采样频率为2B时可以无损复原，文章中提出了<strong>基于自相关性的方法来大概确定时间序列的频率上界</strong>，首先根据自相关曲线$g_i$的第一个局部最小点确定临界频率，为$g_i(y) = \sum^{D-y}_{j=1}t_{ij}t_{i(j+y)}$,其中y是滞后。如果对于某一个$y’$,$g_i$存在一个极小值，如果$g_i(y’)&lt;0$,$y’$与典型半衰期有关，在此情况下，我们把$1/y_i’$叫做$T_i$的临界频率。$y_i’$越小，代表着他的临界频率越大，找到所有的临界频率之后，我们按照降序进行排列，选择一个较高的百分比，如80%，来对所有的时间序列的上界进行估计，不用小的临界频率值的原因是降低额外噪声的影响。</p>
</li>
<li><p>自相关曲线可以通过快速傅里叶变换求取</p>
</li>
</ul>
<script type="math/tex; mode=display">
F_g(f) = FFT[T_i]\\
S(f)=F_g(f)F_g^{*}(f)\\
g(y) = IFFT[S(f)]</script><p><img src="/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/4.jpg" alt="4"></p>
<h3 id="基于密度的聚类"><a href="#基于密度的聚类" class="headerlink" title="基于密度的聚类"></a>基于密度的聚类</h3><ul>
<li><p>采用L1范数作为距离相似性度量（因为对于冲击噪声的鲁棒性和计算简便性）</p>
</li>
<li><p>对于数据集的分布没有假设</p>
</li>
<li><p>用PCA来对数据集降维到两个维度的子空间上来观察它的空间分布D</p>
</li>
<li><p>实际的时间序列有相位的变化，随机的噪声，影响聚类的效果</p>
</li>
<li><p>如果两个时间序列相位差别很小，那么他的L1范数也很小，随着数据集size的增大，对于有很大相位差的两个时间序列，基于密度的聚类算法可以保证将两个时间序列聚类，因为他们可以通过一系列的中间的有小相位扰动的时间序列实例相连</p>
</li>
<li><p><strong>给出了关于相位扰动和噪声理论上的边界，找到了数据size n，相位扰动上界</strong>$\Delta$<strong>以及事件：时间序列之间相位差别小于</strong>$\Delta$<strong>的可以被聚类在一起 发生的概率P之间的关系，从而</strong>$\Delta$<strong>可以根据P和n来确定</strong></p>
<p>定义时间序列$T(a) = \{f(a+b),…f(a+mb)\}$,a是初始相位，b是时间序列采样的间隔，m是时间序列的长度。假设T(a)是根据f(t)来采样得到的，这里假设f(t)是一个解析函数，另外一个时间序列为$T(a-\delta) = \{f(a+b-\delta),..,f(a+mb-\delta)\}$,假设现在有n个时间序列$T(a-\delta_i)$,其中$\delta_i \in [0,\Delta]$,假设这些时间序列之间是相互独立的，$\delta_i$是在$[0,\Delta]$之间均匀分布的，现在定义基于密度的聚类算法中的距离阈值为$\epsilon$，定义下面事件为</p>
<script type="math/tex; mode=display">
E_n := \{T(a-\delta_i),i=1,2,...n. belong~to~same~cluster\}</script><p><strong>Lemma 3:</strong>$P(E_n)\geq 1-n(1-\frac{\epsilon}{mMk\Delta})^n$</p>
<p><strong>证明</strong>：(1)首先证明$\exist M,s.t., L_1(T(a),T(a-\delta)) := \sum^m_{i=1}|f(a+ib)-f(a+ib-\delta|\leq mM\delta$，意义就是只要相位误差足够小，那么他们两个之间的$L_1$距离就可以足够小。</p>
<p>(2)把区间$[0,\Delta]$分为若干个小区间，长度为$\frac{\epsilon}{mMk}$,下面分情况进行讨论， 如果每个小区间对应最少一个时间序列，那么$L_1(T(a),its~KNN)\leq  mM\times \frac{\epsilon}{mMk}\times k =\epsilon$,此时，所有的时间序列都是密度相连的，他们可以被聚类到一起。</p>
<p>(3)定义事件$U_j = \{j^{th} ~bucket~is~empty\}$，那么</p>
<script type="math/tex; mode=display">
P(at~least~one~bucket~is~empty)=P(\cup_j U_j)\leq \sum_jP(U_j)=n(1-\frac{\epsilon}{mMk\Delta})^n</script><p>因为每个bucket的长度为$\frac{\epsilon}{mMk}$,那么对于$\Delta$区域之内bucket的数量为$\frac{\Delta}{\frac{\epsilon}{mMk}}$，</p>
<p><strong>这个地方存疑，我觉得是</strong>$C_n^1(1-\frac{\epsilon}{mMk\Delta})^{n -1}\frac{\epsilon}{mMk\Delta}$.</p>
<p>注意到事件：没有空的bucket  是事件$E_n$的子集，因此</p>
<script type="math/tex; mode=display">
P(E_n) \geq P(no~empty~bucket) = 1-p(\cup_j U_j)\geq 1-n(1-\frac{\epsilon}{mMk\Delta})^n</script></li>
</ul>
<ul>
<li><p>密度估计：思想是画出$k_{dis}$图来，根据斜率来进行density radius的选择，定义经验分布函数为</p>
<script type="math/tex; mode=display">
EDF_k(r):=\frac{|\{objeccts~whose~k_{dis}~\leq r\}|}{N}</script><p><strong> Lemma 4</strong>: $EDF_k(r)\approx \sum^N_{m=k+1}P(E_{m,r})$，其中$P(E_{m,r}) = C_{N-1}^{m-1}P_r^{m-1}(1-P_r)^{N-m}, P_r = \frac{V_r}{V}=\frac{c_d\times r^d}{V}$,$V_r$是在d维$L_p$空间上的超球面的体积，例如在欧氏空间上， $c_d = \frac{\pi^{d/2}}{\Gamma(\frac{d}{2}+1)}$,(注意 $\Gamma(n) = (n-1)!$)，注意到$k_{dis}$图中的Y轴拐点在$EDF_k\sim r$图中对应的是x轴拐点</p>
<p><strong>Lemma 5</strong>：$k_{dis}$图上面Y轴的拐点是density radius</p>
<p><img src="/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/5.jpg" alt="5"></p>
</li>
</ul>
<p><img src="/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/6.jpg" alt="6"></p>
<h3 id="分配"><a href="#分配" class="headerlink" title="分配"></a>分配</h3><ul>
<li><p>在采样数据集上聚类完成之后，需要对数据集中其他没有标签的数据进行分类</p>
<p>分配过程需要计算每一对有标记和无标记数据之间的距离，这样它的时间复杂度是O(Nsd)，文章中提出了一种修建策略，如下图所示，如果一个无标签数据a距离一个有标签数据b的距离dis大于$\epsilon$,那么对于距离b为$\epsilon$的有标签数据与a点的距离来说，根据三角不等式,$distance -(distance-\epsilon)&lt;new_dis$,也就是说也是大于$\epsilon$的</p>
<p><img src="/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/7.jpg" alt="7"></p>
<p>并且设计了一个叫做 Sorted Neighbor Graph(SNG)的数据结构来实现上述的修剪策略。在样本集进行聚类的时候，如果点b是一个核心点，那么把他加进SNG，并且把b点和样本集中其他实例之间的距离按照降序排列，在这过程中用到了<strong>快速排序</strong>的方法，因此SNG的时间复杂度为$O(s^2logs)$.</p>
<p><img src="/2018/12/21/Yading-fast-clustering-of-large-scale-time-series-data/8.jpg" alt="8"></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm Library</category>
        <category>Clustering Algorithm</category>
      </categories>
      <tags>
        <tag>Clustering Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>WorkflowSim Structure</title>
    <url>/2018/11/15/WorkflowSim-Structure/</url>
    <content><![CDATA[<h2 id="Characteristic"><a href="#Characteristic" class="headerlink" title="Characteristic"></a>Characteristic</h2><ul>
<li>支持容错机制的研究</li>
<li>可以计算系统的总花费</li>
<li>支持目前广泛接受的工作流的基本特征及其调度算法</li>
<li>不仅支持调度算法的评估，而且考虑了不同任务调度/执行的费用和失败情况</li>
<li>聚类的策略是静态的，并且没有考虑动态资源的特点</li>
<li>不考虑来自不同网格中间件服务的中间件开销</li>
</ul>
<h2 id="Models-and-Features"><a href="#Models-and-Features" class="headerlink" title="Models and Features"></a>Models and Features</h2><h3 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h3><ul>
<li><p><strong>Workflow Mapper</strong>: Model workflows as DAGs.</p>
<p>用于导入以XML格式化的DAG文件和其他元数据信息，如文件大小等等，并且在mapping之后，创建一系列tasks并将这些安排到执行环节</p>
</li>
<li><p><strong>Clustering Engine</strong>:Merge tasks into jobs so as to reduce the scheduling overheads</p>
</li>
<li><p><strong>Workflow Engine</strong>: Manages jobs based on their dependencies to assure that a job may only be released when all of its parent jobs have completed successfully</p>
</li>
<li><p><strong>Workflow Scheduler and Job Execution</strong>: </p>
<ul>
<li><p><strong>Workflow Scheduler</strong> is used to match jobs to worker nodes based on the criteria selected by user.</p>
<p><strong>Support dynamic workflow algorithms</strong>:dynamic algorithms, jobs are matched to a worker node in the remote scheduler whenever a worker node becomes idle </p>
<p><img src="/2018/11/15/WorkflowSim-Structure/1.png" alt="1"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Layered-Overhead"><a href="#Layered-Overhead" class="headerlink" title="Layered Overhead"></a>Layered Overhead</h3><p>Classify workflow overheads into five categories.</p>
<ul>
<li><p><strong>Workflow Engine Delay</strong>: 度量作业的最后一个父作业完成时间与作业提交到本地队列的时间之间的时间。</p>
</li>
<li><p><strong>Queue Delay</strong>:工作流引擎向本地队列提交作业与本地调度程序看到作业运行的时间之间的时间</p>
</li>
<li><p><strong>Postscript Delay and Prescript Delay</strong>:在执行作业之前和之后在某些执行系统下执行轻量级脚本所花费的时间。</p>
</li>
<li><p><strong>Data Transfer Delay</strong>: 数据在结点之间传输所需要的时间，包含三种过程： staging data in, cleaning up, staging data out.</p>
</li>
<li><p><strong>Clustering Delay</strong>:实际任务运行时总和与工作流计划程序看到的作业运行时之间的差异。</p>
<p><img src="/2018/11/15/WorkflowSim-Structure/4.png" alt="4"></p>
</li>
</ul>
<h3 id="Layered-Failures-and-Job-Retry"><a href="#Layered-Failures-and-Job-Retry" class="headerlink" title="Layered Failures and Job Retry"></a>Layered Failures and Job Retry</h3><ul>
<li><p>Divide transient failures into two categories: <strong>task failure</strong> and <strong>job failure</strong></p>
</li>
<li><p>Two components to simulate: </p>
<ul>
<li><p><strong>Failure Generator</strong>: inject task/job failures at each execution site.</p>
<p>故障生成器根据用户指定的分布和平均故障率随机生成任务/作业故障。</p>
</li>
<li><p><strong>Failure Monitor</strong>: 收集失败记录（例如，资源ID，作业ID，任务ID）并将它们返回到工作流管理系统，以便它可以动态调整调度策略</p>
<p>Functionality is added to the Workflow Scheduler, which <strong>checks the status of a job</strong> and takes action based on the strategies that a user selects. </p>
<p><strong>Reclustering</strong> is a technique that we have proposed that attempts to adjust the task clustering strategy based on the detected failure rate </p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Cloud Workflow Scheduling</category>
      </categories>
      <tags>
        <tag>WorkflowSim toolkit introduction</tag>
      </tags>
  </entry>
  <entry>
    <title>TS prediction</title>
    <url>/2020/09/06/TS-prediction/</url>
    <content><![CDATA[<h2 id="TS-Prediction"><a href="#TS-Prediction" class="headerlink" title="TS Prediction"></a>TS Prediction</h2><h3 id="TS-prediction-with-code"><a href="#TS-prediction-with-code" class="headerlink" title="TS_prediction with code"></a>TS_prediction with code</h3><ul>
<li>GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series <a href="https://github.com/edebrouwer/gru_ode_bayes">code</a></li>
<li>N-BEATS: Neural basis expansion analysis for interpretable time series forecasting <a href="https://github.com/forecasts-and-nn/n-beats">code</a></li>
<li>Latent ODEs for Irregularly-Sampled Time Series <a href="https://github.com/Ldhlwh/Latent-ODE">code</a></li>
<li>Learning Interpretable Deep State Space Model for Probabilistic Time Series Forecasting <a href>code</a></li>
<li>Tensorized LSTM with Adaptive Shared Memory for Learning Trends in Multivariate Time Series <a href="https://github.com/DerronXu/DeepTrends/tree/master">code</a></li>
<li>System Identification with Time-Aware Neural Sequence Models <a href="https://github.com/tdmeeste/TimeAwareRNN">code</a></li>
<li>Joint Modeling of Local and Global Temporal Dynamics for Multivariate time series forecasting with missing values</li>
<li>Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models <a href="https://github.com/vincent-leguen/DILATE">code</a></li>
</ul>
<h3 id="Ts-prediction-without-code"><a href="#Ts-prediction-without-code" class="headerlink" title="Ts_prediction without code"></a>Ts_prediction without code</h3><ul>
<li>DeepAR: Probabilistic forecasting with autoregressive recurrent networks</li>
<li>Unsupervised Scalable Representation Learning for Multivariate Time Series</li>
<li>A dual-stage attention-based recurrent neural network for time series prediction</li>
<li>Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models <a href="https://github.com/vincent-leguen/DILATE">code</a></li>
<li>A Memory-Network Based Solution for Multivariate Time-Series Forecasting</li>
<li>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</li>
<li>A Multi-Horizon Quantile Recurrent Forecaster</li>
<li>Time-series Generative Adversarial Networks</li>
<li>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</li>
<li>Deep state space models for time series forecasting</li>
<li>Deep factors for forecasting</li>
<li>You May Not Need Order in Time Series Forecasting</li>
</ul>
<h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><ul>
<li>Electronic Health Records: Mimic-iii, a freely accessible critical care database.   <a href="https://mimic.physionet.org/gettingstarted/access/">url</a>  GRU-ODE_Bayes</li>
<li>Climate change: <a href="https://cdiac.ess-dive.lbl.gov/epubs/ndp/ushcn/daily_doc.html">url</a> GRU-ODE-Bayes</li>
<li><a href="https://www.cs.ucr.edu/~eamonn/time_series_data/">Time Series Classification</a></li>
<li><a href="https://www.kaggle.com/c/web-traffic-time-series-forecasting/data">Web Traffic Time Series Forecasting</a></li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>Applications</tag>
      </tags>
  </entry>
  <entry>
    <title>python numpy练习</title>
    <url>/2018/11/11/python-numpy%E7%BB%83%E4%B9%A0/</url>
    <content><![CDATA[<p><strong>以下为转载内容</strong></p>
<h1 id="100-numpy-exercises"><a href="#100-numpy-exercises" class="headerlink" title="100 numpy exercises"></a>100 numpy exercises</h1><p>This is a collection of exercises that have been collected in the numpy mailing list, on stack overflow and in the numpy documentation. The goal of this collection is to offer a quick reference for both old and new users but also to provide a set of exercises for those who teach.</p>
<p>If you find an error or think you’ve a better way to solve some of them, feel free to open an issue at <a href="https://github.com/rougier/numpy-100">https://github.com/rougier/numpy-100</a></p>
<h4 id="1-Import-the-numpy-package-under-the-name-np-★☆☆"><a href="#1-Import-the-numpy-package-under-the-name-np-★☆☆" class="headerlink" title="1. Import the numpy package under the name np (★☆☆)"></a>1. Import the numpy package under the name <code>np</code> (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h4 id="2-Print-the-numpy-version-and-the-configuration-★☆☆"><a href="#2-Print-the-numpy-version-and-the-configuration-★☆☆" class="headerlink" title="2. Print the numpy version and the configuration (★☆☆)"></a>2. Print the numpy version and the configuration (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.__version__)</span><br><span class="line">np.show_config()</span><br></pre></td></tr></table></figure>
<h4 id="3-Create-a-null-vector-of-size-10-★☆☆"><a href="#3-Create-a-null-vector-of-size-10-★☆☆" class="headerlink" title="3. Create a null vector of size 10 (★☆☆)"></a>3. Create a null vector of size 10 (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.zeros(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="4-How-to-find-the-memory-size-of-any-array-★☆☆"><a href="#4-How-to-find-the-memory-size-of-any-array-★☆☆" class="headerlink" title="4.  How to find the memory size of any array (★☆☆)"></a>4.  How to find the memory size of any array (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.zeros((<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;%d bytes&quot;</span> % (Z.size * Z.itemsize))</span><br></pre></td></tr></table></figure>
<h4 id="5-How-to-get-the-documentation-of-the-numpy-add-function-from-the-command-line-★☆☆"><a href="#5-How-to-get-the-documentation-of-the-numpy-add-function-from-the-command-line-★☆☆" class="headerlink" title="5.  How to get the documentation of the numpy add function from the command line? (★☆☆)"></a>5.  How to get the documentation of the numpy add function from the command line? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%run `python -c <span class="string">&quot;import numpy; numpy.info(numpy.add)&quot;</span>`</span><br></pre></td></tr></table></figure>
<h4 id="6-Create-a-null-vector-of-size-10-but-the-fifth-value-which-is-1-★☆☆"><a href="#6-Create-a-null-vector-of-size-10-but-the-fifth-value-which-is-1-★☆☆" class="headerlink" title="6.  Create a null vector of size 10 but the fifth value which is 1 (★☆☆)"></a>6.  Create a null vector of size 10 but the fifth value which is 1 (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.zeros(<span class="number">10</span>)</span><br><span class="line">Z[<span class="number">4</span>] = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="7-Create-a-vector-with-values-ranging-from-10-to-49-★☆☆"><a href="#7-Create-a-vector-with-values-ranging-from-10-to-49-★☆☆" class="headerlink" title="7.  Create a vector with values ranging from 10 to 49 (★☆☆)"></a>7.  Create a vector with values ranging from 10 to 49 (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.arange(<span class="number">10</span>,<span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="8-Reverse-a-vector-first-element-becomes-last-★☆☆"><a href="#8-Reverse-a-vector-first-element-becomes-last-★☆☆" class="headerlink" title="8.  Reverse a vector (first element becomes last) (★☆☆)"></a>8.  Reverse a vector (first element becomes last) (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.arange(<span class="number">50</span>)</span><br><span class="line">Z = Z[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="9-Create-a-3x3-matrix-with-values-ranging-from-0-to-8-★☆☆"><a href="#9-Create-a-3x3-matrix-with-values-ranging-from-0-to-8-★☆☆" class="headerlink" title="9.  Create a 3x3 matrix with values ranging from 0 to 8 (★☆☆)"></a>9.  Create a 3x3 matrix with values ranging from 0 to 8 (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="10-Find-indices-of-non-zero-elements-from-1-2-0-0-4-0-★☆☆"><a href="#10-Find-indices-of-non-zero-elements-from-1-2-0-0-4-0-★☆☆" class="headerlink" title="10. Find indices of non-zero elements from [1,2,0,0,4,0] (★☆☆)"></a>10. Find indices of non-zero elements from [1,2,0,0,4,0] (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nz = np.nonzero([<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">4</span>,<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(nz)</span><br></pre></td></tr></table></figure>
<h4 id="11-Create-a-3x3-identity-matrix-★☆☆"><a href="#11-Create-a-3x3-identity-matrix-★☆☆" class="headerlink" title="11. Create a 3x3 identity matrix (★☆☆)"></a>11. Create a 3x3 identity matrix (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.eye(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="12-Create-a-3x3x3-array-with-random-values-★☆☆"><a href="#12-Create-a-3x3x3-array-with-random-values-★☆☆" class="headerlink" title="12. Create a 3x3x3 array with random values (★☆☆)"></a>12. Create a 3x3x3 array with random values (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.random((<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="13-Create-a-10x10-array-with-random-values-and-find-the-minimum-and-maximum-values-★☆☆"><a href="#13-Create-a-10x10-array-with-random-values-and-find-the-minimum-and-maximum-values-★☆☆" class="headerlink" title="13. Create a 10x10 array with random values and find the minimum and maximum values (★☆☆)"></a>13. Create a 10x10 array with random values and find the minimum and maximum values (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.random((<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">Zmin, Zmax = Z.<span class="built_in">min</span>(), Z.<span class="built_in">max</span>()</span><br><span class="line"><span class="built_in">print</span>(Zmin, Zmax)</span><br></pre></td></tr></table></figure>
<h4 id="14-Create-a-random-vector-of-size-30-and-find-the-mean-value-★☆☆"><a href="#14-Create-a-random-vector-of-size-30-and-find-the-mean-value-★☆☆" class="headerlink" title="14. Create a random vector of size 30 and find the mean value (★☆☆)"></a>14. Create a random vector of size 30 and find the mean value (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.random(<span class="number">30</span>)</span><br><span class="line">m = Z.mean()</span><br><span class="line"><span class="built_in">print</span>(m)</span><br></pre></td></tr></table></figure>
<h4 id="15-Create-a-2d-array-with-1-on-the-border-and-0-inside-★☆☆"><a href="#15-Create-a-2d-array-with-1-on-the-border-and-0-inside-★☆☆" class="headerlink" title="15. Create a 2d array with 1 on the border and 0 inside (★☆☆)"></a>15. Create a 2d array with 1 on the border and 0 inside (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.ones((<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">Z[<span class="number">1</span>:-<span class="number">1</span>,<span class="number">1</span>:-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="16-How-to-add-a-border-filled-with-0’s-around-an-existing-array-★☆☆"><a href="#16-How-to-add-a-border-filled-with-0’s-around-an-existing-array-★☆☆" class="headerlink" title="16. How to add a border (filled with 0’s) around an existing array? (★☆☆)"></a>16. How to add a border (filled with 0’s) around an existing array? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.ones((<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">Z = np.pad(Z, pad_width=<span class="number">1</span>, mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="17-What-is-the-result-of-the-following-expression-★☆☆"><a href="#17-What-is-the-result-of-the-following-expression-★☆☆" class="headerlink" title="17. What is the result of the following expression? (★☆☆)"></a>17. What is the result of the following expression? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="number">0</span> * np.nan)</span><br><span class="line"><span class="built_in">print</span>(np.nan == np.nan)</span><br><span class="line"><span class="built_in">print</span>(np.inf &gt; np.nan)</span><br><span class="line"><span class="built_in">print</span>(np.nan - np.nan)</span><br><span class="line"><span class="built_in">print</span>(np.nan <span class="keyword">in</span> <span class="built_in">set</span>([np.nan]))</span><br><span class="line"><span class="built_in">print</span>(<span class="number">0.3</span> == <span class="number">3</span> * <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="18-Create-a-5x5-matrix-with-values-1-2-3-4-just-below-the-diagonal-★☆☆"><a href="#18-Create-a-5x5-matrix-with-values-1-2-3-4-just-below-the-diagonal-★☆☆" class="headerlink" title="18. Create a 5x5 matrix with values 1,2,3,4 just below the diagonal (★☆☆)"></a>18. Create a 5x5 matrix with values 1,2,3,4 just below the diagonal (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.diag(<span class="number">1</span>+np.arange(<span class="number">4</span>),k=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="19-Create-a-8x8-matrix-and-fill-it-with-a-checkerboard-pattern-★☆☆"><a href="#19-Create-a-8x8-matrix-and-fill-it-with-a-checkerboard-pattern-★☆☆" class="headerlink" title="19. Create a 8x8 matrix and fill it with a checkerboard pattern (★☆☆)"></a>19. Create a 8x8 matrix and fill it with a checkerboard pattern (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.zeros((<span class="number">8</span>,<span class="number">8</span>),dtype=<span class="built_in">int</span>)</span><br><span class="line">Z[<span class="number">1</span>::<span class="number">2</span>,::<span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">Z[::<span class="number">2</span>,<span class="number">1</span>::<span class="number">2</span>] = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="20-Consider-a-6-7-8-shape-array-what-is-the-index-x-y-z-of-the-100th-element"><a href="#20-Consider-a-6-7-8-shape-array-what-is-the-index-x-y-z-of-the-100th-element" class="headerlink" title="20. Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?"></a>20. Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.unravel_index(<span class="number">99</span>,(<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)))</span><br></pre></td></tr></table></figure>
<h4 id="21-Create-a-checkerboard-8x8-matrix-using-the-tile-function-★☆☆"><a href="#21-Create-a-checkerboard-8x8-matrix-using-the-tile-function-★☆☆" class="headerlink" title="21. Create a checkerboard 8x8 matrix using the tile function (★☆☆)"></a>21. Create a checkerboard 8x8 matrix using the tile function (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.tile( np.array([[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>]]), (<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="22-Normalize-a-5x5-random-matrix-★☆☆"><a href="#22-Normalize-a-5x5-random-matrix-★☆☆" class="headerlink" title="22. Normalize a 5x5 random matrix (★☆☆)"></a>22. Normalize a 5x5 random matrix (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.random((<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">Z = (Z - np.mean (Z)) / (np.std (Z))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="23-Create-a-custom-dtype-that-describes-a-color-as-four-unsigned-bytes-RGBA-★☆☆"><a href="#23-Create-a-custom-dtype-that-describes-a-color-as-four-unsigned-bytes-RGBA-★☆☆" class="headerlink" title="23. Create a custom dtype that describes a color as four unsigned bytes (RGBA) (★☆☆)"></a>23. Create a custom dtype that describes a color as four unsigned bytes (RGBA) (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">color = np.dtype([(<span class="string">&quot;r&quot;</span>, np.ubyte, <span class="number">1</span>),</span><br><span class="line">​                  (<span class="string">&quot;g&quot;</span>, np.ubyte, <span class="number">1</span>),</span><br><span class="line">​                  (<span class="string">&quot;b&quot;</span>, np.ubyte, <span class="number">1</span>),</span><br><span class="line">​                  (<span class="string">&quot;a&quot;</span>, np.ubyte, <span class="number">1</span>)])</span><br></pre></td></tr></table></figure>
<h4 id="24-Multiply-a-5x3-matrix-by-a-3x2-matrix-real-matrix-product-★☆☆"><a href="#24-Multiply-a-5x3-matrix-by-a-3x2-matrix-real-matrix-product-★☆☆" class="headerlink" title="24. Multiply a 5x3 matrix by a 3x2 matrix (real matrix product) (★☆☆)"></a>24. Multiply a 5x3 matrix by a 3x2 matrix (real matrix product) (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.dot(np.ones((<span class="number">5</span>,<span class="number">3</span>)), np.ones((<span class="number">3</span>,<span class="number">2</span>)))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternative solution, in Python 3.5 and above</span></span><br><span class="line">Z = np.ones((<span class="number">5</span>,<span class="number">3</span>)) @ np.ones((<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="25-Given-a-1D-array-negate-all-elements-which-are-between-3-and-8-in-place-★☆☆"><a href="#25-Given-a-1D-array-negate-all-elements-which-are-between-3-and-8-in-place-★☆☆" class="headerlink" title="25. Given a 1D array, negate all elements which are between 3 and 8, in place. (★☆☆)"></a>25. Given a 1D array, negate all elements which are between 3 and 8, in place. (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Evgeni Burovski</span></span><br><span class="line"></span><br><span class="line">Z = np.arange(<span class="number">11</span>)</span><br><span class="line">Z[(<span class="number">3</span> &lt; Z) &amp; (Z &lt;= <span class="number">8</span>)] *= -<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="26-What-is-the-output-of-the-following-script-★☆☆"><a href="#26-What-is-the-output-of-the-following-script-★☆☆" class="headerlink" title="26. What is the output of the following script? (★☆☆)"></a>26. What is the output of the following script? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Jake VanderPlas</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>(<span class="built_in">range</span>(<span class="number">5</span>),-<span class="number">1</span>))</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>(<span class="built_in">range</span>(<span class="number">5</span>),-<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h4 id="27-Consider-an-integer-vector-Z-which-of-these-expressions-are-legal-★☆☆"><a href="#27-Consider-an-integer-vector-Z-which-of-these-expressions-are-legal-★☆☆" class="headerlink" title="27. Consider an integer vector Z, which of these expressions are legal? (★☆☆)"></a>27. Consider an integer vector Z, which of these expressions are legal? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z**Z</span><br><span class="line"><span class="number">2</span> &lt;&lt; Z &gt;&gt; <span class="number">2</span></span><br><span class="line">Z &lt;- Z</span><br><span class="line"><span class="number">1j</span>*Z</span><br><span class="line">Z/<span class="number">1</span>/<span class="number">1</span></span><br><span class="line">Z&lt;Z&gt;Z</span><br></pre></td></tr></table></figure>
<h4 id="28-What-are-the-result-of-the-following-expressions"><a href="#28-What-are-the-result-of-the-following-expressions" class="headerlink" title="28. What are the result of the following expressions?"></a>28. What are the result of the following expressions?</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.array(<span class="number">0</span>) / np.array(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(np.array(<span class="number">0</span>) // np.array(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(np.array([np.nan]).astype(<span class="built_in">int</span>).astype(<span class="built_in">float</span>))</span><br></pre></td></tr></table></figure>
<h4 id="29-How-to-round-away-from-zero-a-float-array-★☆☆"><a href="#29-How-to-round-away-from-zero-a-float-array-★☆☆" class="headerlink" title="29. How to round away from zero a float array ? (★☆☆)"></a>29. How to round away from zero a float array ? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Charles R Harris</span></span><br><span class="line"></span><br><span class="line">Z = np.random.uniform(-<span class="number">10</span>,+<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span> (np.copysign(np.ceil(np.<span class="built_in">abs</span>(Z)), Z))</span><br></pre></td></tr></table></figure>
<h4 id="30-How-to-find-common-values-between-two-arrays-★☆☆"><a href="#30-How-to-find-common-values-between-two-arrays-★☆☆" class="headerlink" title="30. How to find common values between two arrays? (★☆☆)"></a>30. How to find common values between two arrays? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z1 = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">Z2 = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(np.intersect1d(Z1,Z2))</span><br></pre></td></tr></table></figure>
<h4 id="31-How-to-ignore-all-numpy-warnings-not-recommended-★☆☆"><a href="#31-How-to-ignore-all-numpy-warnings-not-recommended-★☆☆" class="headerlink" title="31. How to ignore all numpy warnings (not recommended)? (★☆☆)"></a>31. How to ignore all numpy warnings (not recommended)? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Suicide mode on</span></span><br><span class="line">defaults = np.seterr(<span class="built_in">all</span>=<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">Z = np.ones(<span class="number">1</span>) / <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Back to sanity</span></span><br><span class="line">_ = np.seterr(**defaults)</span><br></pre></td></tr></table></figure>
<p>An equivalent way, with a context manager:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> np.errstate(divide=<span class="string">&#x27;ignore&#x27;</span>):</span><br><span class="line">​    Z = np.ones(<span class="number">1</span>) / <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h4 id="32-Is-the-following-expressions-true-★☆☆"><a href="#32-Is-the-following-expressions-true-★☆☆" class="headerlink" title="32. Is the following expressions true? (★☆☆)"></a>32. Is the following expressions true? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.sqrt(-<span class="number">1</span>) == np.emath.sqrt(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="33-How-to-get-the-dates-of-yesterday-today-and-tomorrow-★☆☆"><a href="#33-How-to-get-the-dates-of-yesterday-today-and-tomorrow-★☆☆" class="headerlink" title="33. How to get the dates of yesterday, today and tomorrow? (★☆☆)"></a>33. How to get the dates of yesterday, today and tomorrow? (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yesterday = np.datetime64(<span class="string">&#x27;today&#x27;</span>, <span class="string">&#x27;D&#x27;</span>) - np.timedelta64(<span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>)</span><br><span class="line">today     = np.datetime64(<span class="string">&#x27;today&#x27;</span>, <span class="string">&#x27;D&#x27;</span>)</span><br><span class="line">tomorrow  = np.datetime64(<span class="string">&#x27;today&#x27;</span>, <span class="string">&#x27;D&#x27;</span>) + np.timedelta64(<span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="34-How-to-get-all-the-dates-corresponding-to-the-month-of-July-2016-★★☆"><a href="#34-How-to-get-all-the-dates-corresponding-to-the-month-of-July-2016-★★☆" class="headerlink" title="34. How to get all the dates corresponding to the month of July 2016? (★★☆)"></a>34. How to get all the dates corresponding to the month of July 2016? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.arange(<span class="string">&#x27;2016-07&#x27;</span>, <span class="string">&#x27;2016-08&#x27;</span>, dtype=<span class="string">&#x27;datetime64[D]&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="35-How-to-compute-A-B-A-2-in-place-without-copy-★★☆"><a href="#35-How-to-compute-A-B-A-2-in-place-without-copy-★★☆" class="headerlink" title="35. How to compute ((A+B)*(-A/2)) in place (without copy)? (★★☆)"></a>35. How to compute ((A+B)*(-A/2)) in place (without copy)? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.ones(<span class="number">3</span>)*<span class="number">1</span></span><br><span class="line">B = np.ones(<span class="number">3</span>)*<span class="number">2</span></span><br><span class="line">C = np.ones(<span class="number">3</span>)*<span class="number">3</span></span><br><span class="line">np.add(A,B,out=B)</span><br><span class="line">np.divide(A,<span class="number">2</span>,out=A)</span><br><span class="line">np.negative(A,out=A)</span><br><span class="line">np.multiply(A,B,out=A)</span><br></pre></td></tr></table></figure>
<h4 id="36-Extract-the-integer-part-of-a-random-array-using-5-different-methods-★★☆"><a href="#36-Extract-the-integer-part-of-a-random-array-using-5-different-methods-★★☆" class="headerlink" title="36. Extract the integer part of a random array using 5 different methods (★★☆)"></a>36. Extract the integer part of a random array using 5 different methods (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.uniform(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (Z - Z%<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span> (np.floor(Z))</span><br><span class="line"><span class="built_in">print</span> (np.ceil(Z)-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span> (Z.astype(<span class="built_in">int</span>))</span><br><span class="line"><span class="built_in">print</span> (np.trunc(Z))</span><br></pre></td></tr></table></figure>
<h4 id="37-Create-a-5x5-matrix-with-row-values-ranging-from-0-to-4-★★☆"><a href="#37-Create-a-5x5-matrix-with-row-values-ranging-from-0-to-4-★★☆" class="headerlink" title="37. Create a 5x5 matrix with row values ranging from 0 to 4 (★★☆)"></a>37. Create a 5x5 matrix with row values ranging from 0 to 4 (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.zeros((<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">Z += np.arange(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="38-Consider-a-generator-function-that-generates-10-integers-and-use-it-to-build-an-array-★☆☆"><a href="#38-Consider-a-generator-function-that-generates-10-integers-and-use-it-to-build-an-array-★☆☆" class="headerlink" title="38. Consider a generator function that generates 10 integers and use it to build an array (★☆☆)"></a>38. Consider a generator function that generates 10 integers and use it to build an array (★☆☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>():</span><br><span class="line">​    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">​        <span class="keyword">yield</span> x</span><br><span class="line">Z = np.fromiter(generate(),dtype=<span class="built_in">float</span>,count=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="39-Create-a-vector-of-size-10-with-values-ranging-from-0-to-1-both-excluded-★★☆"><a href="#39-Create-a-vector-of-size-10-with-values-ranging-from-0-to-1-both-excluded-★★☆" class="headerlink" title="39. Create a vector of size 10 with values ranging from 0 to 1, both excluded (★★☆)"></a>39. Create a vector of size 10 with values ranging from 0 to 1, both excluded (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">11</span>,endpoint=<span class="literal">False</span>)[<span class="number">1</span>:]</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="40-Create-a-random-vector-of-size-10-and-sort-it-★★☆"><a href="#40-Create-a-random-vector-of-size-10-and-sort-it-★★☆" class="headerlink" title="40. Create a random vector of size 10 and sort it (★★☆)"></a>40. Create a random vector of size 10 and sort it (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.random(<span class="number">10</span>)</span><br><span class="line">Z.sort()</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="41-How-to-sum-a-small-array-faster-than-np-sum-★★☆"><a href="#41-How-to-sum-a-small-array-faster-than-np-sum-★★☆" class="headerlink" title="41. How to sum a small array faster than np.sum? (★★☆)"></a>41. How to sum a small array faster than np.sum? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Evgeni Burovski</span></span><br><span class="line"></span><br><span class="line">Z = np.arange(<span class="number">10</span>)</span><br><span class="line">np.add.reduce(Z)</span><br></pre></td></tr></table></figure>
<h4 id="42-Consider-two-random-array-A-and-B-check-if-they-are-equal-★★☆"><a href="#42-Consider-two-random-array-A-and-B-check-if-they-are-equal-★★☆" class="headerlink" title="42. Consider two random array A and B, check if they are equal (★★☆)"></a>42. Consider two random array A and B, check if they are equal (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.random.randint(<span class="number">0</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">B = np.random.randint(<span class="number">0</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming identical shape of the arrays and a tolerance for the comparison of values</span></span><br><span class="line">equal = np.allclose(A,B)</span><br><span class="line"><span class="built_in">print</span>(equal)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Checking both the shape and the element values, no tolerance (values have to be exactly equal)</span></span><br><span class="line">equal = np.array_equal(A,B)</span><br><span class="line"><span class="built_in">print</span>(equal)</span><br></pre></td></tr></table></figure>
<h4 id="43-Make-an-array-immutable-read-only-★★☆"><a href="#43-Make-an-array-immutable-read-only-★★☆" class="headerlink" title="43. Make an array immutable (read-only) (★★☆)"></a>43. Make an array immutable (read-only) (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.zeros(<span class="number">10</span>)</span><br><span class="line">Z.flags.writeable = <span class="literal">False</span></span><br><span class="line">Z[<span class="number">0</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="44-Consider-a-random-10x2-matrix-representing-cartesian-coordinates-convert-them-to-polar-coordinates-★★☆"><a href="#44-Consider-a-random-10x2-matrix-representing-cartesian-coordinates-convert-them-to-polar-coordinates-★★☆" class="headerlink" title="44. Consider a random 10x2 matrix representing cartesian coordinates, convert them to polar coordinates (★★☆)"></a>44. Consider a random 10x2 matrix representing cartesian coordinates, convert them to polar coordinates (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.random((<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">X,Y = Z[:,<span class="number">0</span>], Z[:,<span class="number">1</span>]</span><br><span class="line">R = np.sqrt(X**<span class="number">2</span>+Y**<span class="number">2</span>)</span><br><span class="line">T = np.arctan2(Y,X)</span><br><span class="line"><span class="built_in">print</span>(R)</span><br><span class="line"><span class="built_in">print</span>(T)</span><br></pre></td></tr></table></figure>
<h4 id="45-Create-random-vector-of-size-10-and-replace-the-maximum-value-by-0-★★☆"><a href="#45-Create-random-vector-of-size-10-and-replace-the-maximum-value-by-0-★★☆" class="headerlink" title="45. Create random vector of size 10 and replace the maximum value by 0 (★★☆)"></a>45. Create random vector of size 10 and replace the maximum value by 0 (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.random(<span class="number">10</span>)</span><br><span class="line">Z[Z.argmax()] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="46-Create-a-structured-array-with-x-and-y-coordinates-covering-the-0-1-x-0-1-area-★★☆"><a href="#46-Create-a-structured-array-with-x-and-y-coordinates-covering-the-0-1-x-0-1-area-★★☆" class="headerlink" title="46. Create a structured array with x and y coordinates covering the [0,1]x[0,1] area (★★☆)"></a>46. Create a structured array with <code>x</code> and <code>y</code> coordinates covering the [0,1]x[0,1] area (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.zeros((<span class="number">5</span>,<span class="number">5</span>), [(<span class="string">&#x27;x&#x27;</span>,<span class="built_in">float</span>),(<span class="string">&#x27;y&#x27;</span>,<span class="built_in">float</span>)])</span><br><span class="line">Z[<span class="string">&#x27;x&#x27;</span>], Z[<span class="string">&#x27;y&#x27;</span>] = np.meshgrid(np.linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">5</span>),</span><br><span class="line">​                             np.linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="47-Given-two-arrays-X-and-Y-construct-the-Cauchy-matrix-C-Cij-1-xi-yj"><a href="#47-Given-two-arrays-X-and-Y-construct-the-Cauchy-matrix-C-Cij-1-xi-yj" class="headerlink" title="47. Given two arrays, X and Y, construct the Cauchy matrix C (Cij =1/(xi - yj))"></a>47. Given two arrays, X and Y, construct the Cauchy matrix C (Cij =1/(xi - yj))</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Evgeni Burovski</span></span><br><span class="line"></span><br><span class="line">X = np.arange(<span class="number">8</span>)</span><br><span class="line">Y = X + <span class="number">0.5</span></span><br><span class="line">C = <span class="number">1.0</span> / np.subtract.outer(X, Y)</span><br><span class="line"><span class="built_in">print</span>(np.linalg.det(C))</span><br></pre></td></tr></table></figure>
<h4 id="48-Print-the-minimum-and-maximum-representable-value-for-each-numpy-scalar-type-★★☆"><a href="#48-Print-the-minimum-and-maximum-representable-value-for-each-numpy-scalar-type-★★☆" class="headerlink" title="48. Print the minimum and maximum representable value for each numpy scalar type (★★☆)"></a>48. Print the minimum and maximum representable value for each numpy scalar type (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dtype <span class="keyword">in</span> [np.int8, np.int32, np.int64]:</span><br><span class="line">   <span class="built_in">print</span>(np.iinfo(dtype).<span class="built_in">min</span>)</span><br><span class="line">   <span class="built_in">print</span>(np.iinfo(dtype).<span class="built_in">max</span>)</span><br><span class="line"><span class="keyword">for</span> dtype <span class="keyword">in</span> [np.float32, np.float64]:</span><br><span class="line">   <span class="built_in">print</span>(np.finfo(dtype).<span class="built_in">min</span>)</span><br><span class="line">   <span class="built_in">print</span>(np.finfo(dtype).<span class="built_in">max</span>)</span><br><span class="line">   <span class="built_in">print</span>(np.finfo(dtype).eps)</span><br></pre></td></tr></table></figure>
<h4 id="49-How-to-print-all-the-values-of-an-array-★★☆"><a href="#49-How-to-print-all-the-values-of-an-array-★★☆" class="headerlink" title="49. How to print all the values of an array? (★★☆)"></a>49. How to print all the values of an array? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.set_printoptions(threshold=np.nan)</span><br><span class="line">Z = np.zeros((<span class="number">16</span>,<span class="number">16</span>))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="50-How-to-find-the-closest-value-to-a-given-scalar-in-a-vector-★★☆"><a href="#50-How-to-find-the-closest-value-to-a-given-scalar-in-a-vector-★★☆" class="headerlink" title="50. How to find the closest value (to a given scalar) in a vector? (★★☆)"></a>50. How to find the closest value (to a given scalar) in a vector? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.arange(<span class="number">100</span>)</span><br><span class="line">v = np.random.uniform(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line">index = (np.<span class="built_in">abs</span>(Z-v)).argmin()</span><br><span class="line"><span class="built_in">print</span>(Z[index])</span><br></pre></td></tr></table></figure>
<h4 id="51-Create-a-structured-array-representing-a-position-x-y-and-a-color-r-g-b-★★☆"><a href="#51-Create-a-structured-array-representing-a-position-x-y-and-a-color-r-g-b-★★☆" class="headerlink" title="51. Create a structured array representing a position (x,y) and a color (r,g,b) (★★☆)"></a>51. Create a structured array representing a position (x,y) and a color (r,g,b) (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.zeros(<span class="number">10</span>, [ (<span class="string">&#x27;position&#x27;</span>, [ (<span class="string">&#x27;x&#x27;</span>, <span class="built_in">float</span>, <span class="number">1</span>),</span><br><span class="line">​                                  (<span class="string">&#x27;y&#x27;</span>, <span class="built_in">float</span>, <span class="number">1</span>)]),</span><br><span class="line">​                   (<span class="string">&#x27;color&#x27;</span>,    [ (<span class="string">&#x27;r&#x27;</span>, <span class="built_in">float</span>, <span class="number">1</span>),</span><br><span class="line">​                                  (<span class="string">&#x27;g&#x27;</span>, <span class="built_in">float</span>, <span class="number">1</span>),</span><br><span class="line">​                                  (<span class="string">&#x27;b&#x27;</span>, <span class="built_in">float</span>, <span class="number">1</span>)])])</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="52-Consider-a-random-vector-with-shape-100-2-representing-coordinates-find-point-by-point-distances-★★☆"><a href="#52-Consider-a-random-vector-with-shape-100-2-representing-coordinates-find-point-by-point-distances-★★☆" class="headerlink" title="52. Consider a random vector with shape (100,2) representing coordinates, find point by point distances (★★☆)"></a>52. Consider a random vector with shape (100,2) representing coordinates, find point by point distances (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.random((<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">X,Y = np.atleast_2d(Z[:,<span class="number">0</span>], Z[:,<span class="number">1</span>])</span><br><span class="line">D = np.sqrt( (X-X.T)**<span class="number">2</span> + (Y-Y.T)**<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(D)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Much faster with scipy</span></span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="comment"># Thanks Gavin Heverly-Coulson (#issue 1)</span></span><br><span class="line"><span class="keyword">import</span> scipy.spatial</span><br><span class="line"></span><br><span class="line">Z = np.random.random((<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">D = scipy.spatial.distance.cdist(Z,Z)</span><br><span class="line"><span class="built_in">print</span>(D)</span><br></pre></td></tr></table></figure>
<h4 id="53-How-to-convert-a-float-32-bits-array-into-an-integer-32-bits-in-place"><a href="#53-How-to-convert-a-float-32-bits-array-into-an-integer-32-bits-in-place" class="headerlink" title="53. How to convert a float (32 bits) array into an integer (32 bits) in place?"></a>53. How to convert a float (32 bits) array into an integer (32 bits) in place?</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.arange(<span class="number">10</span>, dtype=np.float32)</span><br><span class="line">Z = Z.astype(np.int32, copy=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="54-How-to-read-the-following-file-★★☆"><a href="#54-How-to-read-the-following-file-★★☆" class="headerlink" title="54. How to read the following file? (★★☆)"></a>54. How to read the following file? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> StringIO</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fake file</span></span><br><span class="line">s = StringIO(<span class="string">&quot;&quot;&quot;1, 2, 3, 4, 5\n</span></span><br><span class="line"><span class="string">​                6,  ,  , 7, 8\n</span></span><br><span class="line"><span class="string">​                 ,  , 9,10,11\n&quot;&quot;&quot;</span>)</span><br><span class="line">Z = np.genfromtxt(s, delimiter=<span class="string">&quot;,&quot;</span>, dtype=np.<span class="built_in">int</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="55-What-is-the-equivalent-of-enumerate-for-numpy-arrays-★★☆"><a href="#55-What-is-the-equivalent-of-enumerate-for-numpy-arrays-★★☆" class="headerlink" title="55. What is the equivalent of enumerate for numpy arrays? (★★☆)"></a>55. What is the equivalent of enumerate for numpy arrays? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> index, value <span class="keyword">in</span> np.ndenumerate(Z):</span><br><span class="line">​    <span class="built_in">print</span>(index, value)</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> np.ndindex(Z.shape):</span><br><span class="line">​    <span class="built_in">print</span>(index, Z[index])</span><br></pre></td></tr></table></figure>
<h4 id="56-Generate-a-generic-2D-Gaussian-like-array-★★☆"><a href="#56-Generate-a-generic-2D-Gaussian-like-array-★★☆" class="headerlink" title="56. Generate a generic 2D Gaussian-like array (★★☆)"></a>56. Generate a generic 2D Gaussian-like array (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, Y = np.meshgrid(np.linspace(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">10</span>), np.linspace(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">10</span>))</span><br><span class="line">D = np.sqrt(X*X+Y*Y)</span><br><span class="line">sigma, mu = <span class="number">1.0</span>, <span class="number">0.0</span></span><br><span class="line">G = np.exp(-( (D-mu)**<span class="number">2</span> / ( <span class="number">2.0</span> * sigma**<span class="number">2</span> ) ) )</span><br><span class="line"><span class="built_in">print</span>(G)</span><br></pre></td></tr></table></figure>
<h4 id="57-How-to-randomly-place-p-elements-in-a-2D-array-★★☆"><a href="#57-How-to-randomly-place-p-elements-in-a-2D-array-★★☆" class="headerlink" title="57. How to randomly place p elements in a 2D array? (★★☆)"></a>57. How to randomly place p elements in a 2D array? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Divakar</span></span><br><span class="line"></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line">p = <span class="number">3</span></span><br><span class="line">Z = np.zeros((n,n))</span><br><span class="line">np.put(Z, np.random.choice(<span class="built_in">range</span>(n*n), p, replace=<span class="literal">False</span>),<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="58-Subtract-the-mean-of-each-row-of-a-matrix-★★☆"><a href="#58-Subtract-the-mean-of-each-row-of-a-matrix-★★☆" class="headerlink" title="58. Subtract the mean of each row of a matrix (★★☆)"></a>58. Subtract the mean of each row of a matrix (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Warren Weckesser</span></span><br><span class="line"></span><br><span class="line">X = np.random.rand(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Recent versions of numpy</span></span><br><span class="line">Y = X - X.mean(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Older versions of numpy</span></span><br><span class="line">Y = X - X.mean(axis=<span class="number">1</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure>
<h4 id="59-How-to-I-sort-an-array-by-the-nth-column-★★☆"><a href="#59-How-to-I-sort-an-array-by-the-nth-column-★★☆" class="headerlink" title="59. How to I sort an array by the nth column? (★★☆)"></a>59. How to I sort an array by the nth column? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Steve Tjoa</span></span><br><span class="line"></span><br><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"><span class="built_in">print</span>(Z[Z[:,<span class="number">1</span>].argsort()])</span><br></pre></td></tr></table></figure>
<h4 id="60-How-to-tell-if-a-given-2D-array-has-null-columns-★★☆"><a href="#60-How-to-tell-if-a-given-2D-array-has-null-columns-★★☆" class="headerlink" title="60. How to tell if a given 2D array has null columns? (★★☆)"></a>60. How to tell if a given 2D array has null columns? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Warren Weckesser</span></span><br><span class="line"></span><br><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">3</span>,(<span class="number">3</span>,<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>((~Z.<span class="built_in">any</span>(axis=<span class="number">0</span>)).<span class="built_in">any</span>())</span><br></pre></td></tr></table></figure>
<h4 id="61-Find-the-nearest-value-from-a-given-value-in-an-array-★★☆"><a href="#61-Find-the-nearest-value-from-a-given-value-in-an-array-★★☆" class="headerlink" title="61. Find the nearest value from a given value in an array (★★☆)"></a>61. Find the nearest value from a given value in an array (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">z = <span class="number">0.5</span></span><br><span class="line">m = Z.flat[np.<span class="built_in">abs</span>(Z - z).argmin()]</span><br><span class="line"><span class="built_in">print</span>(m)</span><br></pre></td></tr></table></figure>
<h4 id="62-Considering-two-arrays-with-shape-1-3-and-3-1-how-to-compute-their-sum-using-an-iterator-★★☆"><a href="#62-Considering-two-arrays-with-shape-1-3-and-3-1-how-to-compute-their-sum-using-an-iterator-★★☆" class="headerlink" title="62. Considering two arrays with shape (1,3) and (3,1), how to compute their sum using an iterator? (★★☆)"></a>62. Considering two arrays with shape (1,3) and (3,1), how to compute their sum using an iterator? (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.arange(<span class="number">3</span>).reshape(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">B = np.arange(<span class="number">3</span>).reshape(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">it = np.nditer([A,B,<span class="literal">None</span>])</span><br><span class="line"><span class="keyword">for</span> x,y,z <span class="keyword">in</span> it: z[...] = x + y</span><br><span class="line"><span class="built_in">print</span>(it.operands[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h4 id="63-Create-an-array-class-that-has-a-name-attribute-★★☆"><a href="#63-Create-an-array-class-that-has-a-name-attribute-★★☆" class="headerlink" title="63. Create an array class that has a name attribute (★★☆)"></a>63. Create an array class that has a name attribute (★★☆)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NamedArray</span>(np.ndarray):</span><br><span class="line">​    <span class="keyword">def</span> <span class="title function_">__new__</span>(<span class="params">cls, array, name=<span class="string">&quot;no name&quot;</span></span>):</span><br><span class="line">​        obj = np.asarray(array).view(cls)</span><br><span class="line">​        obj.name = name</span><br><span class="line">​        <span class="keyword">return</span> obj</span><br><span class="line">​    <span class="keyword">def</span> <span class="title function_">__array_finalize__</span>(<span class="params">self, obj</span>):</span><br><span class="line">​        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span></span><br><span class="line">​        self.info = <span class="built_in">getattr</span>(obj, <span class="string">&#x27;name&#x27;</span>, <span class="string">&quot;no name&quot;</span>)</span><br><span class="line"></span><br><span class="line">Z = NamedArray(np.arange(<span class="number">10</span>), <span class="string">&quot;range_10&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (Z.name)</span><br></pre></td></tr></table></figure>
<h4 id="64-Consider-a-given-vector-how-to-add-1-to-each-element-indexed-by-a-second-vector-be-careful-with-repeated-indices-★★★"><a href="#64-Consider-a-given-vector-how-to-add-1-to-each-element-indexed-by-a-second-vector-be-careful-with-repeated-indices-★★★" class="headerlink" title="64. Consider a given vector, how to add 1 to each element indexed by a second vector (be careful with repeated indices)? (★★★)"></a>64. Consider a given vector, how to add 1 to each element indexed by a second vector (be careful with repeated indices)? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Brett Olsen</span></span><br><span class="line"></span><br><span class="line">Z = np.ones(<span class="number">10</span>)</span><br><span class="line">I = np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(Z),<span class="number">20</span>)</span><br><span class="line">Z += np.bincount(I, minlength=<span class="built_in">len</span>(Z))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Another solution</span></span><br><span class="line"><span class="comment"># Author: Bartosz Telenczuk</span></span><br><span class="line">np.add.at(Z, I, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="65-How-to-accumulate-elements-of-a-vector-X-to-an-array-F-based-on-an-index-list-I-★★★"><a href="#65-How-to-accumulate-elements-of-a-vector-X-to-an-array-F-based-on-an-index-list-I-★★★" class="headerlink" title="65. How to accumulate elements of a vector (X) to an array (F) based on an index list (I)? (★★★)"></a>65. How to accumulate elements of a vector (X) to an array (F) based on an index list (I)? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Alan G Isaac</span></span><br><span class="line"></span><br><span class="line">X = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">I = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">9</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line">F = np.bincount(I,X)</span><br><span class="line"><span class="built_in">print</span>(F)</span><br></pre></td></tr></table></figure>
<h4 id="66-Considering-a-w-h-3-image-of-dtype-ubyte-compute-the-number-of-unique-colors-★★★"><a href="#66-Considering-a-w-h-3-image-of-dtype-ubyte-compute-the-number-of-unique-colors-★★★" class="headerlink" title="66. Considering a (w,h,3) image of (dtype=ubyte), compute the number of unique colors (★★★)"></a>66. Considering a (w,h,3) image of (dtype=ubyte), compute the number of unique colors (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Nadav Horesh</span></span><br><span class="line"></span><br><span class="line">w,h = <span class="number">16</span>,<span class="number">16</span></span><br><span class="line">I = np.random.randint(<span class="number">0</span>,<span class="number">2</span>,(h,w,<span class="number">3</span>)).astype(np.ubyte)</span><br><span class="line">F = I[...,<span class="number">0</span>]*<span class="number">256</span>*<span class="number">256</span> + I[...,<span class="number">1</span>]*<span class="number">256</span> +I[...,<span class="number">2</span>]</span><br><span class="line">n = <span class="built_in">len</span>(np.unique(F))</span><br><span class="line"><span class="built_in">print</span>(np.unique(I))</span><br></pre></td></tr></table></figure>
<h4 id="67-Considering-a-four-dimensions-array-how-to-get-sum-over-the-last-two-axis-at-once-★★★"><a href="#67-Considering-a-four-dimensions-array-how-to-get-sum-over-the-last-two-axis-at-once-★★★" class="headerlink" title="67. Considering a four dimensions array, how to get sum over the last two axis at once? (★★★)"></a>67. Considering a four dimensions array, how to get sum over the last two axis at once? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,(<span class="number">3</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># solution by passing a tuple of axes (introduced in numpy 1.7.0)</span></span><br><span class="line"><span class="built_in">sum</span> = A.<span class="built_in">sum</span>(axis=(-<span class="number">2</span>,-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>)</span><br><span class="line"><span class="comment"># solution by flattening the last two dimensions into one</span></span><br><span class="line"><span class="comment"># (useful for functions that don&#x27;t accept tuples for axis argument)</span></span><br><span class="line"><span class="built_in">sum</span> = A.reshape(A.shape[:-<span class="number">2</span>] + (-<span class="number">1</span>,)).<span class="built_in">sum</span>(axis=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>)</span><br></pre></td></tr></table></figure>
<h4 id="68-Considering-a-one-dimensional-vector-D-how-to-compute-means-of-subsets-of-D-using-a-vector-S-of-same-size-describing-subset-indices-★★★"><a href="#68-Considering-a-one-dimensional-vector-D-how-to-compute-means-of-subsets-of-D-using-a-vector-S-of-same-size-describing-subset-indices-★★★" class="headerlink" title="68. Considering a one-dimensional vector D, how to compute means of subsets of D using a vector S of same size describing subset  indices? (★★★)"></a>68. Considering a one-dimensional vector D, how to compute means of subsets of D using a vector S of same size describing subset  indices? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Jaime Fernández del Río</span></span><br><span class="line"></span><br><span class="line">D = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line">S = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">100</span>)</span><br><span class="line">D_sums = np.bincount(S, weights=D)</span><br><span class="line">D_counts = np.bincount(S)</span><br><span class="line">D_means = D_sums / D_counts</span><br><span class="line"><span class="built_in">print</span>(D_means)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pandas solution as a reference due to more intuitive code</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="built_in">print</span>(pd.Series(D).groupby(S).mean())</span><br></pre></td></tr></table></figure>
<h4 id="69-How-to-get-the-diagonal-of-a-dot-product-★★★"><a href="#69-How-to-get-the-diagonal-of-a-dot-product-★★★" class="headerlink" title="69. How to get the diagonal of a dot product? (★★★)"></a>69. How to get the diagonal of a dot product? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Mathieu Blondel</span></span><br><span class="line"></span><br><span class="line">A = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">B = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Slow version  </span></span><br><span class="line">np.diag(np.dot(A, B))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fast version</span></span><br><span class="line">np.<span class="built_in">sum</span>(A * B.T, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Faster version</span></span><br><span class="line">np.einsum(<span class="string">&quot;ij,ji-&gt;i&quot;</span>, A, B)</span><br></pre></td></tr></table></figure>
<h4 id="70-Consider-the-vector-1-2-3-4-5-how-to-build-a-new-vector-with-3-consecutive-zeros-interleaved-between-each-value-★★★"><a href="#70-Consider-the-vector-1-2-3-4-5-how-to-build-a-new-vector-with-3-consecutive-zeros-interleaved-between-each-value-★★★" class="headerlink" title="70. Consider the vector [1, 2, 3, 4, 5], how to build a new vector with 3 consecutive zeros interleaved between each value? (★★★)"></a>70. Consider the vector [1, 2, 3, 4, 5], how to build a new vector with 3 consecutive zeros interleaved between each value? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Warren Weckesser</span></span><br><span class="line"></span><br><span class="line">Z = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">nz = <span class="number">3</span></span><br><span class="line">Z0 = np.zeros(<span class="built_in">len</span>(Z) + (<span class="built_in">len</span>(Z)-<span class="number">1</span>)*(nz))</span><br><span class="line">Z0[::nz+<span class="number">1</span>] = Z</span><br><span class="line"><span class="built_in">print</span>(Z0)</span><br></pre></td></tr></table></figure>
<h4 id="71-Consider-an-array-of-dimension-5-5-3-how-to-mulitply-it-by-an-array-with-dimensions-5-5-★★★"><a href="#71-Consider-an-array-of-dimension-5-5-3-how-to-mulitply-it-by-an-array-with-dimensions-5-5-★★★" class="headerlink" title="71. Consider an array of dimension (5,5,3), how to mulitply it by an array with dimensions (5,5)? (★★★)"></a>71. Consider an array of dimension (5,5,3), how to mulitply it by an array with dimensions (5,5)? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.ones((<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line">B = <span class="number">2</span>*np.ones((<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(A * B[:,:,<span class="literal">None</span>])</span><br></pre></td></tr></table></figure>
<h4 id="72-How-to-swap-two-rows-of-an-array-★★★"><a href="#72-How-to-swap-two-rows-of-an-array-★★★" class="headerlink" title="72. How to swap two rows of an array? (★★★)"></a>72. How to swap two rows of an array? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Eelco Hoogendoorn</span></span><br><span class="line"></span><br><span class="line">A = np.arange(<span class="number">25</span>).reshape(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">A[[<span class="number">0</span>,<span class="number">1</span>]] = A[[<span class="number">1</span>,<span class="number">0</span>]]</span><br><span class="line"><span class="built_in">print</span>(A)</span><br></pre></td></tr></table></figure>
<h4 id="73-Consider-a-set-of-10-triplets-describing-10-triangles-with-shared-vertices-find-the-set-of-unique-line-segments-composing-all-the-triangles-★★★"><a href="#73-Consider-a-set-of-10-triplets-describing-10-triangles-with-shared-vertices-find-the-set-of-unique-line-segments-composing-all-the-triangles-★★★" class="headerlink" title="73. Consider a set of 10 triplets describing 10 triangles (with shared vertices), find the set of unique line segments composing all the  triangles (★★★)"></a>73. Consider a set of 10 triplets describing 10 triangles (with shared vertices), find the set of unique line segments composing all the  triangles (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Nicolas P. Rougier</span></span><br><span class="line"></span><br><span class="line">faces = np.random.randint(<span class="number">0</span>,<span class="number">100</span>,(<span class="number">10</span>,<span class="number">3</span>))</span><br><span class="line">F = np.roll(faces.repeat(<span class="number">2</span>,axis=<span class="number">1</span>),-<span class="number">1</span>,axis=<span class="number">1</span>)</span><br><span class="line">F = F.reshape(<span class="built_in">len</span>(F)*<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">F = np.sort(F,axis=<span class="number">1</span>)</span><br><span class="line">G = F.view( dtype=[(<span class="string">&#x27;p0&#x27;</span>,F.dtype),(<span class="string">&#x27;p1&#x27;</span>,F.dtype)] )</span><br><span class="line">G = np.unique(G)</span><br><span class="line"><span class="built_in">print</span>(G)</span><br></pre></td></tr></table></figure>
<h4 id="74-Given-an-array-C-that-is-a-bincount-how-to-produce-an-array-A-such-that-np-bincount-A-C-★★★"><a href="#74-Given-an-array-C-that-is-a-bincount-how-to-produce-an-array-A-such-that-np-bincount-A-C-★★★" class="headerlink" title="74. Given an array C that is a bincount, how to produce an array A such that np.bincount(A) == C? (★★★)"></a>74. Given an array C that is a bincount, how to produce an array A such that np.bincount(A) == C? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Jaime Fernández del Río</span></span><br><span class="line"></span><br><span class="line">C = np.bincount([<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">6</span>])</span><br><span class="line">A = np.repeat(np.arange(<span class="built_in">len</span>(C)), C)</span><br><span class="line"><span class="built_in">print</span>(A)</span><br></pre></td></tr></table></figure>
<h4 id="75-How-to-compute-averages-using-a-sliding-window-over-an-array-★★★"><a href="#75-How-to-compute-averages-using-a-sliding-window-over-an-array-★★★" class="headerlink" title="75. How to compute averages using a sliding window over an array? (★★★)"></a>75. How to compute averages using a sliding window over an array? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Jaime Fernández del Río</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">moving_average</span>(<span class="params">a, n=<span class="number">3</span></span>) :</span><br><span class="line">    ret = np.cumsum(a, dtype=<span class="built_in">float</span>)</span><br><span class="line">    ret[n:] = ret[n:] - ret[:-n]</span><br><span class="line">    <span class="keyword">return</span> ret[n - <span class="number">1</span>:] / n</span><br><span class="line">Z = np.arange(<span class="number">20</span>)</span><br><span class="line"><span class="built_in">print</span>(moving_average(Z, n=<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h4 id="76-Consider-a-one-dimensional-array-Z-build-a-two-dimensional-array-whose-first-row-is-Z-0-Z-1-Z-2-and-each-subsequent-row-is-shifted-by-1-last-row-should-be-Z-3-Z-2-Z-1-★★★"><a href="#76-Consider-a-one-dimensional-array-Z-build-a-two-dimensional-array-whose-first-row-is-Z-0-Z-1-Z-2-and-each-subsequent-row-is-shifted-by-1-last-row-should-be-Z-3-Z-2-Z-1-★★★" class="headerlink" title="76. Consider a one-dimensional array Z, build a two-dimensional array whose first row is (Z[0],Z[1],Z[2]) and each subsequent row is  shifted by 1 (last row should be (Z[-3],Z[-2],Z[-1]) (★★★)"></a>76. Consider a one-dimensional array Z, build a two-dimensional array whose first row is (Z[0],Z[1],Z[2]) and each subsequent row is  shifted by 1 (last row should be (Z[-3],Z[-2],Z[-1]) (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Joe Kington / Erik Rigtorp</span></span><br><span class="line"><span class="keyword">from</span> numpy.lib <span class="keyword">import</span> stride_tricks</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rolling</span>(<span class="params">a, window</span>):</span><br><span class="line">    shape = (a.size - window + <span class="number">1</span>, window)</span><br><span class="line">    strides = (a.itemsize, a.itemsize)</span><br><span class="line">    <span class="keyword">return</span> stride_tricks.as_strided(a, shape=shape, strides=strides)</span><br><span class="line">Z = rolling(np.arange(<span class="number">10</span>), <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="77-How-to-negate-a-boolean-or-to-change-the-sign-of-a-float-inplace-★★★"><a href="#77-How-to-negate-a-boolean-or-to-change-the-sign-of-a-float-inplace-★★★" class="headerlink" title="77. How to negate a boolean, or to change the sign of a float inplace? (★★★)"></a>77. How to negate a boolean, or to change the sign of a float inplace? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Nathaniel J. Smith</span></span><br><span class="line"></span><br><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">2</span>,<span class="number">100</span>)</span><br><span class="line">np.logical_not(Z, out=Z)</span><br><span class="line"></span><br><span class="line">Z = np.random.uniform(-<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">100</span>)</span><br><span class="line">np.negative(Z, out=Z)</span><br></pre></td></tr></table></figure>
<h4 id="78-Consider-2-sets-of-points-P0-P1-describing-lines-2d-and-a-point-p-how-to-compute-distance-from-p-to-each-line-i-P0-i-P1-i-★★★"><a href="#78-Consider-2-sets-of-points-P0-P1-describing-lines-2d-and-a-point-p-how-to-compute-distance-from-p-to-each-line-i-P0-i-P1-i-★★★" class="headerlink" title="78. Consider 2 sets of points P0,P1 describing lines (2d) and a point p, how to compute distance from p to each line i  (P0[i],P1[i])? (★★★)"></a>78. Consider 2 sets of points P0,P1 describing lines (2d) and a point p, how to compute distance from p to each line i  (P0[i],P1[i])? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">distance</span>(<span class="params">P0, P1, p</span>):</span><br><span class="line">    T = P1 - P0</span><br><span class="line">    L = (T**<span class="number">2</span>).<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">    U = -((P0[:,<span class="number">0</span>]-p[...,<span class="number">0</span>])*T[:,<span class="number">0</span>] + (P0[:,<span class="number">1</span>]-p[...,<span class="number">1</span>])*T[:,<span class="number">1</span>]) / L</span><br><span class="line">    U = U.reshape(<span class="built_in">len</span>(U),<span class="number">1</span>)</span><br><span class="line">    D = P0 + U*T - p</span><br><span class="line">    <span class="keyword">return</span> np.sqrt((D**<span class="number">2</span>).<span class="built_in">sum</span>(axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">P0 = np.random.uniform(-<span class="number">10</span>,<span class="number">10</span>,(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">P1 = np.random.uniform(-<span class="number">10</span>,<span class="number">10</span>,(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">p  = np.random.uniform(-<span class="number">10</span>,<span class="number">10</span>,( <span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(distance(P0, P1, p))</span><br></pre></td></tr></table></figure>
<h4 id="79-Consider-2-sets-of-points-P0-P1-describing-lines-2d-and-a-set-of-points-P-how-to-compute-distance-from-each-point-j-P-j-to-each-line-i-P0-i-P1-i-★★★"><a href="#79-Consider-2-sets-of-points-P0-P1-describing-lines-2d-and-a-set-of-points-P-how-to-compute-distance-from-each-point-j-P-j-to-each-line-i-P0-i-P1-i-★★★" class="headerlink" title="79. Consider 2 sets of points P0,P1 describing lines (2d) and a set of points P, how to compute distance from each point j (P[j]) to each line i (P0[i],P1[i])? (★★★)"></a>79. Consider 2 sets of points P0,P1 describing lines (2d) and a set of points P, how to compute distance from each point j (P[j]) to each line i (P0[i],P1[i])? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Italmassov Kuanysh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># based on distance function from previous question</span></span><br><span class="line">P0 = np.random.uniform(-<span class="number">10</span>, <span class="number">10</span>, (<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">P1 = np.random.uniform(-<span class="number">10</span>,<span class="number">10</span>,(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">p = np.random.uniform(-<span class="number">10</span>, <span class="number">10</span>, (<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(np.array([distance(P0,P1,p_i) <span class="keyword">for</span> p_i <span class="keyword">in</span> p]))</span><br></pre></td></tr></table></figure>
<h4 id="80-Consider-an-arbitrary-array-write-a-function-that-extract-a-subpart-with-a-fixed-shape-and-centered-on-a-given-element-pad-with-a-fill-value-when-necessary-★★★"><a href="#80-Consider-an-arbitrary-array-write-a-function-that-extract-a-subpart-with-a-fixed-shape-and-centered-on-a-given-element-pad-with-a-fill-value-when-necessary-★★★" class="headerlink" title="80. Consider an arbitrary array, write a function that extract a subpart with a fixed shape and centered on a given element (pad with a fill value when necessary) (★★★)"></a>80. Consider an arbitrary array, write a function that extract a subpart with a fixed shape and centered on a given element (pad with a <code>fill</code> value when necessary) (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Nicolas Rougier</span></span><br><span class="line"></span><br><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">shape = (<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">fill  = <span class="number">0</span></span><br><span class="line">position = (<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">R = np.ones(shape, dtype=Z.dtype)*fill</span><br><span class="line">P  = np.array(<span class="built_in">list</span>(position)).astype(<span class="built_in">int</span>)</span><br><span class="line">Rs = np.array(<span class="built_in">list</span>(R.shape)).astype(<span class="built_in">int</span>)</span><br><span class="line">Zs = np.array(<span class="built_in">list</span>(Z.shape)).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">R_start = np.zeros((<span class="built_in">len</span>(shape),)).astype(<span class="built_in">int</span>)</span><br><span class="line">R_stop  = np.array(<span class="built_in">list</span>(shape)).astype(<span class="built_in">int</span>)</span><br><span class="line">Z_start = (P-Rs//<span class="number">2</span>)</span><br><span class="line">Z_stop  = (P+Rs//<span class="number">2</span>)+Rs%<span class="number">2</span></span><br><span class="line"></span><br><span class="line">R_start = (R_start - np.minimum(Z_start,<span class="number">0</span>)).tolist()</span><br><span class="line">Z_start = (np.maximum(Z_start,<span class="number">0</span>)).tolist()</span><br><span class="line">R_stop = np.maximum(R_start, (R_stop - np.maximum(Z_stop-Zs,<span class="number">0</span>))).tolist()</span><br><span class="line">Z_stop = (np.minimum(Z_stop,Zs)).tolist()</span><br><span class="line"></span><br><span class="line">r = [<span class="built_in">slice</span>(start,stop) <span class="keyword">for</span> start,stop <span class="keyword">in</span> <span class="built_in">zip</span>(R_start,R_stop)]</span><br><span class="line">z = [<span class="built_in">slice</span>(start,stop) <span class="keyword">for</span> start,stop <span class="keyword">in</span> <span class="built_in">zip</span>(Z_start,Z_stop)]</span><br><span class="line">R[r] = Z[z]</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"><span class="built_in">print</span>(R)</span><br></pre></td></tr></table></figure>
<h4 id="81-Consider-an-array-Z-1-2-3-4-5-6-7-8-9-10-11-12-13-14-how-to-generate-an-array-R-1-2-3-4-2-3-4-5-3-4-5-6-…-11-12-13-14-★★★"><a href="#81-Consider-an-array-Z-1-2-3-4-5-6-7-8-9-10-11-12-13-14-how-to-generate-an-array-R-1-2-3-4-2-3-4-5-3-4-5-6-…-11-12-13-14-★★★" class="headerlink" title="81. Consider an array Z = [1,2,3,4,5,6,7,8,9,10,11,12,13,14], how to generate an array R = [[1,2,3,4], [2,3,4,5], [3,4,5,6], …, [11,12,13,14]]? (★★★)"></a>81. Consider an array Z = [1,2,3,4,5,6,7,8,9,10,11,12,13,14], how to generate an array R = [[1,2,3,4], [2,3,4,5], [3,4,5,6], …, [11,12,13,14]]? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Stefan van der Walt</span></span><br><span class="line"></span><br><span class="line">Z = np.arange(<span class="number">1</span>,<span class="number">15</span>,dtype=np.uint32)</span><br><span class="line">R = stride_tricks.as_strided(Z,(<span class="number">11</span>,<span class="number">4</span>),(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(R)</span><br></pre></td></tr></table></figure>
<h4 id="82-Compute-a-matrix-rank-★★★"><a href="#82-Compute-a-matrix-rank-★★★" class="headerlink" title="82. Compute a matrix rank (★★★)"></a>82. Compute a matrix rank (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Stefan van der Walt</span></span><br><span class="line"></span><br><span class="line">Z = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">U, S, V = np.linalg.svd(Z) <span class="comment"># Singular Value Decomposition</span></span><br><span class="line">rank = np.<span class="built_in">sum</span>(S &gt; <span class="number">1e-10</span>)</span><br><span class="line"><span class="built_in">print</span>(rank)</span><br></pre></td></tr></table></figure>
<h4 id="83-How-to-find-the-most-frequent-value-in-an-array"><a href="#83-How-to-find-the-most-frequent-value-in-an-array" class="headerlink" title="83. How to find the most frequent value in an array?"></a>83. How to find the most frequent value in an array?</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(np.bincount(Z).argmax())</span><br></pre></td></tr></table></figure>
<h4 id="84-Extract-all-the-contiguous-3x3-blocks-from-a-random-10x10-matrix-★★★"><a href="#84-Extract-all-the-contiguous-3x3-blocks-from-a-random-10x10-matrix-★★★" class="headerlink" title="84. Extract all the contiguous 3x3 blocks from a random 10x10 matrix (★★★)"></a>84. Extract all the contiguous 3x3 blocks from a random 10x10 matrix (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Chris Barker</span></span><br><span class="line"></span><br><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">5</span>,(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">i = <span class="number">1</span> + (Z.shape[<span class="number">0</span>]-<span class="number">3</span>)</span><br><span class="line">j = <span class="number">1</span> + (Z.shape[<span class="number">1</span>]-<span class="number">3</span>)</span><br><span class="line">C = stride_tricks.as_strided(Z, shape=(i, j, n, n), strides=Z.strides + Z.strides)</span><br><span class="line"><span class="built_in">print</span>(C)</span><br></pre></td></tr></table></figure>
<h4 id="85-Create-a-2D-array-subclass-such-that-Z-i-j-Z-j-i-★★★"><a href="#85-Create-a-2D-array-subclass-such-that-Z-i-j-Z-j-i-★★★" class="headerlink" title="85. Create a 2D array subclass such that Z[i,j] == Z[j,i] (★★★)"></a>85. Create a 2D array subclass such that Z[i,j] == Z[j,i] (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Eric O. Lebigot</span></span><br><span class="line"><span class="comment"># Note: only works for 2d array and value setting using indices</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Symetric</span>(np.ndarray):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setitem__</span>(<span class="params">self, index, value</span>):</span><br><span class="line">        i,j = index</span><br><span class="line">        <span class="built_in">super</span>(Symetric, self).__setitem__((i,j), value)</span><br><span class="line">        <span class="built_in">super</span>(Symetric, self).__setitem__((j,i), value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">symetric</span>(<span class="params">Z</span>):</span><br><span class="line">    <span class="keyword">return</span> np.asarray(Z + Z.T - np.diag(Z.diagonal())).view(Symetric)</span><br><span class="line"></span><br><span class="line">S = symetric(np.random.randint(<span class="number">0</span>,<span class="number">10</span>,(<span class="number">5</span>,<span class="number">5</span>)))</span><br><span class="line">S[<span class="number">2</span>,<span class="number">3</span>] = <span class="number">42</span></span><br><span class="line"><span class="built_in">print</span>(S)</span><br></pre></td></tr></table></figure>
<h4 id="86-Consider-a-set-of-p-matrices-wich-shape-n-n-and-a-set-of-p-vectors-with-shape-n-1-How-to-compute-the-sum-of-of-the-p-matrix-products-at-once-result-has-shape-n-1-★★★"><a href="#86-Consider-a-set-of-p-matrices-wich-shape-n-n-and-a-set-of-p-vectors-with-shape-n-1-How-to-compute-the-sum-of-of-the-p-matrix-products-at-once-result-has-shape-n-1-★★★" class="headerlink" title="86. Consider a set of p matrices wich shape (n,n) and a set of p vectors with shape (n,1). How to compute the sum of of the p matrix products at once? (result has shape (n,1)) (★★★)"></a>86. Consider a set of p matrices wich shape (n,n) and a set of p vectors with shape (n,1). How to compute the sum of of the p matrix products at once? (result has shape (n,1)) (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Stefan van der Walt</span></span><br><span class="line"></span><br><span class="line">p, n = <span class="number">10</span>, <span class="number">20</span></span><br><span class="line">M = np.ones((p,n,n))</span><br><span class="line">V = np.ones((p,n,<span class="number">1</span>))</span><br><span class="line">S = np.tensordot(M, V, axes=[[<span class="number">0</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(S)</span><br><span class="line"></span><br><span class="line"><span class="comment"># It works, because:</span></span><br><span class="line"><span class="comment"># M is (p,n,n)</span></span><br><span class="line"><span class="comment"># V is (p,n,1)</span></span><br><span class="line"><span class="comment"># Thus, summing over the paired axes 0 and 0 (of M and V independently),</span></span><br><span class="line"><span class="comment"># and 2 and 1, to remain with a (n,1) vector.</span></span><br></pre></td></tr></table></figure>
<h4 id="87-Consider-a-16x16-array-how-to-get-the-block-sum-block-size-is-4x4-★★★"><a href="#87-Consider-a-16x16-array-how-to-get-the-block-sum-block-size-is-4x4-★★★" class="headerlink" title="87. Consider a 16x16 array, how to get the block-sum (block size is 4x4)? (★★★)"></a>87. Consider a 16x16 array, how to get the block-sum (block size is 4x4)? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Robert Kern</span></span><br><span class="line"></span><br><span class="line">Z = np.ones((<span class="number">16</span>,<span class="number">16</span>))</span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">S = np.add.reduceat(np.add.reduceat(Z, np.arange(<span class="number">0</span>, Z.shape[<span class="number">0</span>], k), axis=<span class="number">0</span>),</span><br><span class="line">​                                       np.arange(<span class="number">0</span>, Z.shape[<span class="number">1</span>], k), axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(S)</span><br></pre></td></tr></table></figure>
<h4 id="88-How-to-implement-the-Game-of-Life-using-numpy-arrays-★★★"><a href="#88-How-to-implement-the-Game-of-Life-using-numpy-arrays-★★★" class="headerlink" title="88. How to implement the Game of Life using numpy arrays? (★★★)"></a>88. How to implement the Game of Life using numpy arrays? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Nicolas Rougier</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">iterate</span>(<span class="params">Z</span>):</span><br><span class="line">​    <span class="comment"># Count neighbours</span></span><br><span class="line">​    N = (Z[<span class="number">0</span>:-<span class="number">2</span>,<span class="number">0</span>:-<span class="number">2</span>] + Z[<span class="number">0</span>:-<span class="number">2</span>,<span class="number">1</span>:-<span class="number">1</span>] + Z[<span class="number">0</span>:-<span class="number">2</span>,<span class="number">2</span>:] +</span><br><span class="line">​         Z[<span class="number">1</span>:-<span class="number">1</span>,<span class="number">0</span>:-<span class="number">2</span>]                + Z[<span class="number">1</span>:-<span class="number">1</span>,<span class="number">2</span>:] +</span><br><span class="line">​         Z[<span class="number">2</span>:  ,<span class="number">0</span>:-<span class="number">2</span>] + Z[<span class="number">2</span>:  ,<span class="number">1</span>:-<span class="number">1</span>] + Z[<span class="number">2</span>:  ,<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply rules</span></span><br><span class="line">    birth = (N==<span class="number">3</span>) &amp; (Z[<span class="number">1</span>:-<span class="number">1</span>,<span class="number">1</span>:-<span class="number">1</span>]==<span class="number">0</span>)</span><br><span class="line">    survive = ((N==<span class="number">2</span>) | (N==<span class="number">3</span>)) &amp; (Z[<span class="number">1</span>:-<span class="number">1</span>,<span class="number">1</span>:-<span class="number">1</span>]==<span class="number">1</span>)</span><br><span class="line">    Z[...] = <span class="number">0</span></span><br><span class="line">    Z[<span class="number">1</span>:-<span class="number">1</span>,<span class="number">1</span>:-<span class="number">1</span>][birth | survive] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">2</span>,(<span class="number">50</span>,<span class="number">50</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>): Z = iterate(Z)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>
<h4 id="89-How-to-get-the-n-largest-values-of-an-array-★★★"><a href="#89-How-to-get-the-n-largest-values-of-an-array-★★★" class="headerlink" title="89. How to get the n largest values of an array (★★★)"></a>89. How to get the n largest values of an array (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.arange(<span class="number">10000</span>)</span><br><span class="line">np.random.shuffle(Z)</span><br><span class="line">n = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Slow</span></span><br><span class="line"><span class="built_in">print</span> (Z[np.argsort(Z)[-n:]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fast</span></span><br><span class="line"><span class="built_in">print</span> (Z[np.argpartition(-Z,n)[:n]])</span><br></pre></td></tr></table></figure>
<h4 id="90-Given-an-arbitrary-number-of-vectors-build-the-cartesian-product-every-combinations-of-every-item-★★★"><a href="#90-Given-an-arbitrary-number-of-vectors-build-the-cartesian-product-every-combinations-of-every-item-★★★" class="headerlink" title="90. Given an arbitrary number of vectors, build the cartesian product (every combinations of every item) (★★★)"></a>90. Given an arbitrary number of vectors, build the cartesian product (every combinations of every item) (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Stefan Van der Walt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cartesian</span>(<span class="params">arrays</span>):</span><br><span class="line">    arrays = [np.asarray(a) <span class="keyword">for</span> a <span class="keyword">in</span> arrays]</span><br><span class="line">    shape = (<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> arrays)</span><br><span class="line"></span><br><span class="line">    ix = np.indices(shape, dtype=<span class="built_in">int</span>)</span><br><span class="line">    ix = ix.reshape(<span class="built_in">len</span>(arrays), -<span class="number">1</span>).T</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> n, arr <span class="keyword">in</span> <span class="built_in">enumerate</span>(arrays):</span><br><span class="line">        ix[:, n] = arrays[n][ix[:, n]]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ix</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (cartesian(([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>])))</span><br></pre></td></tr></table></figure>
<h4 id="91-How-to-create-a-record-array-from-a-regular-array-★★★"><a href="#91-How-to-create-a-record-array-from-a-regular-array-★★★" class="headerlink" title="91. How to create a record array from a regular array? (★★★)"></a>91. How to create a record array from a regular array? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.array([(<span class="string">&quot;Hello&quot;</span>, <span class="number">2.5</span>, <span class="number">3</span>),</span><br><span class="line">​              (<span class="string">&quot;World&quot;</span>, <span class="number">3.6</span>, <span class="number">2</span>)])</span><br><span class="line">R = np.core.records.fromarrays(Z.T,</span><br><span class="line">​                               names=<span class="string">&#x27;col1, col2, col3&#x27;</span>,</span><br><span class="line">​                               formats = <span class="string">&#x27;S8, f8, i8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(R)</span><br></pre></td></tr></table></figure>
<h4 id="92-Consider-a-large-vector-Z-compute-Z-to-the-power-of-3-using-3-different-methods-★★★"><a href="#92-Consider-a-large-vector-Z-compute-Z-to-the-power-of-3-using-3-different-methods-★★★" class="headerlink" title="92. Consider a large vector Z, compute Z to the power of 3 using 3 different methods (★★★)"></a>92. Consider a large vector Z, compute Z to the power of 3 using 3 different methods (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Ryan G.</span></span><br><span class="line"></span><br><span class="line">x = np.random.rand(<span class="number">5e7</span>)</span><br><span class="line"></span><br><span class="line">%timeit np.power(x,<span class="number">3</span>)</span><br><span class="line">%timeit x*x*x</span><br><span class="line">%timeit np.einsum(<span class="string">&#x27;i,i,i-&gt;i&#x27;</span>,x,x,x)</span><br></pre></td></tr></table></figure>
<h4 id="93-Consider-two-arrays-A-and-B-of-shape-8-3-and-2-2-How-to-find-rows-of-A-that-contain-elements-of-each-row-of-B-regardless-of-the-order-of-the-elements-in-B-★★★"><a href="#93-Consider-two-arrays-A-and-B-of-shape-8-3-and-2-2-How-to-find-rows-of-A-that-contain-elements-of-each-row-of-B-regardless-of-the-order-of-the-elements-in-B-★★★" class="headerlink" title="93. Consider two arrays A and B of shape (8,3) and (2,2). How to find rows of A that contain elements of each row of B regardless of the order of the elements in B? (★★★)"></a>93. Consider two arrays A and B of shape (8,3) and (2,2). How to find rows of A that contain elements of each row of B regardless of the order of the elements in B? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Gabe Schwartz</span></span><br><span class="line"></span><br><span class="line">A = np.random.randint(<span class="number">0</span>,<span class="number">5</span>,(<span class="number">8</span>,<span class="number">3</span>))</span><br><span class="line">B = np.random.randint(<span class="number">0</span>,<span class="number">5</span>,(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">C = (A[..., np.newaxis, np.newaxis] == B)</span><br><span class="line">rows = np.where(C.<span class="built_in">any</span>((<span class="number">3</span>,<span class="number">1</span>)).<span class="built_in">all</span>(<span class="number">1</span>))[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(rows)</span><br></pre></td></tr></table></figure>
<h4 id="94-Considering-a-10x3-matrix-extract-rows-with-unequal-values-e-g-2-2-3-★★★"><a href="#94-Considering-a-10x3-matrix-extract-rows-with-unequal-values-e-g-2-2-3-★★★" class="headerlink" title="94. Considering a 10x3 matrix, extract rows with unequal values (e.g. [2,2,3]) (★★★)"></a>94. Considering a 10x3 matrix, extract rows with unequal values (e.g. [2,2,3]) (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Robert Kern</span></span><br><span class="line"></span><br><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">5</span>,(<span class="number">10</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"><span class="comment"># solution for arrays of all dtypes (including string arrays and record arrays)</span></span><br><span class="line">E = np.<span class="built_in">all</span>(Z[:,<span class="number">1</span>:] == Z[:,:-<span class="number">1</span>], axis=<span class="number">1</span>)</span><br><span class="line">U = Z[~E]</span><br><span class="line"><span class="built_in">print</span>(U)</span><br><span class="line"><span class="comment"># soluiton for numerical arrays only, will work for any number of columns in Z</span></span><br><span class="line">U = Z[Z.<span class="built_in">max</span>(axis=<span class="number">1</span>) != Z.<span class="built_in">min</span>(axis=<span class="number">1</span>),:]</span><br><span class="line"><span class="built_in">print</span>(U)</span><br></pre></td></tr></table></figure>
<h4 id="95-Convert-a-vector-of-ints-into-a-matrix-binary-representation-★★★"><a href="#95-Convert-a-vector-of-ints-into-a-matrix-binary-representation-★★★" class="headerlink" title="95. Convert a vector of ints into a matrix binary representation (★★★)"></a>95. Convert a vector of ints into a matrix binary representation (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Warren Weckesser</span></span><br><span class="line"></span><br><span class="line">I = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>])</span><br><span class="line">B = ((I.reshape(-<span class="number">1</span>,<span class="number">1</span>) &amp; (<span class="number">2</span>**np.arange(<span class="number">8</span>))) != <span class="number">0</span>).astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="built_in">print</span>(B[:,::-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Author: Daniel T. McDonald</span></span><br><span class="line"></span><br><span class="line">I = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>], dtype=np.uint8)</span><br><span class="line"><span class="built_in">print</span>(np.unpackbits(I[:, np.newaxis], axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h4 id="96-Given-a-two-dimensional-array-how-to-extract-unique-rows-★★★"><a href="#96-Given-a-two-dimensional-array-how-to-extract-unique-rows-★★★" class="headerlink" title="96. Given a two dimensional array, how to extract unique rows? (★★★)"></a>96. Given a two dimensional array, how to extract unique rows? (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Jaime Fernández del Río</span></span><br><span class="line"></span><br><span class="line">Z = np.random.randint(<span class="number">0</span>,<span class="number">2</span>,(<span class="number">6</span>,<span class="number">3</span>))</span><br><span class="line">T = np.ascontiguousarray(Z).view(np.dtype((np.void, Z.dtype.itemsize * Z.shape[<span class="number">1</span>])))</span><br><span class="line">_, idx = np.unique(T, return_index=<span class="literal">True</span>)</span><br><span class="line">uZ = Z[idx]</span><br><span class="line"><span class="built_in">print</span>(uZ)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Author: Andreas Kouzelis</span></span><br><span class="line"><span class="comment"># NumPy &gt;= 1.13</span></span><br><span class="line">uZ = np.unique(Z, axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(uZ)</span><br></pre></td></tr></table></figure>
<h4 id="97-Considering-2-vectors-A-amp-B-write-the-einsum-equivalent-of-inner-outer-sum-and-mul-function-★★★"><a href="#97-Considering-2-vectors-A-amp-B-write-the-einsum-equivalent-of-inner-outer-sum-and-mul-function-★★★" class="headerlink" title="97. Considering 2 vectors A &amp; B, write the einsum equivalent of inner, outer, sum, and mul function (★★★)"></a>97. Considering 2 vectors A &amp; B, write the einsum equivalent of inner, outer, sum, and mul function (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Alex Riley</span></span><br><span class="line"><span class="comment"># Make sure to read: http://ajcr.net/Basic-guide-to-einsum/</span></span><br><span class="line"></span><br><span class="line">A = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">B = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">np.einsum(<span class="string">&#x27;i-&gt;&#x27;</span>, A)       <span class="comment"># np.sum(A)</span></span><br><span class="line">np.einsum(<span class="string">&#x27;i,i-&gt;i&#x27;</span>, A, B) <span class="comment"># A * B</span></span><br><span class="line">np.einsum(<span class="string">&#x27;i,i&#x27;</span>, A, B)    <span class="comment"># np.inner(A, B)</span></span><br><span class="line">np.einsum(<span class="string">&#x27;i,j-&gt;ij&#x27;</span>, A, B)    <span class="comment"># np.outer(A, B)</span></span><br></pre></td></tr></table></figure>
<h4 id="98-Considering-a-path-described-by-two-vectors-X-Y-how-to-sample-it-using-equidistant-samples-★★★"><a href="#98-Considering-a-path-described-by-two-vectors-X-Y-how-to-sample-it-using-equidistant-samples-★★★" class="headerlink" title="98. Considering a path described by two vectors (X,Y), how to sample it using equidistant samples (★★★)?"></a>98. Considering a path described by two vectors (X,Y), how to sample it using equidistant samples (★★★)?</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Bas Swinckels</span></span><br><span class="line"></span><br><span class="line">phi = np.arange(<span class="number">0</span>, <span class="number">10</span>*np.pi, <span class="number">0.1</span>)</span><br><span class="line">a = <span class="number">1</span></span><br><span class="line">x = a*phi*np.cos(phi)</span><br><span class="line">y = a*phi*np.sin(phi)</span><br><span class="line"></span><br><span class="line">dr = (np.diff(x)**<span class="number">2</span> + np.diff(y)**<span class="number">2</span>)**<span class="number">.5</span> <span class="comment"># segment lengths</span></span><br><span class="line">r = np.zeros_like(x)</span><br><span class="line">r[<span class="number">1</span>:] = np.cumsum(dr)                <span class="comment"># integrate path</span></span><br><span class="line">r_int = np.linspace(<span class="number">0</span>, r.<span class="built_in">max</span>(), <span class="number">200</span>) <span class="comment"># regular spaced path</span></span><br><span class="line">x_int = np.interp(r_int, r, x)       <span class="comment"># integrate path</span></span><br><span class="line">y_int = np.interp(r_int, r, y)</span><br></pre></td></tr></table></figure>
<h4 id="99-Given-an-integer-n-and-a-2D-array-X-select-from-X-the-rows-which-can-be-interpreted-as-draws-from-a-multinomial-distribution-with-n-degrees-i-e-the-rows-which-only-contain-integers-and-which-sum-to-n-★★★"><a href="#99-Given-an-integer-n-and-a-2D-array-X-select-from-X-the-rows-which-can-be-interpreted-as-draws-from-a-multinomial-distribution-with-n-degrees-i-e-the-rows-which-only-contain-integers-and-which-sum-to-n-★★★" class="headerlink" title="99. Given an integer n and a 2D array X, select from X the rows which can be interpreted as draws from a multinomial distribution with n degrees, i.e., the rows which only contain integers and which sum to n. (★★★)"></a>99. Given an integer n and a 2D array X, select from X the rows which can be interpreted as draws from a multinomial distribution with n degrees, i.e., the rows which only contain integers and which sum to n. (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Evgeni Burovski</span></span><br><span class="line"></span><br><span class="line">X = np.asarray([[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>, <span class="number">8.0</span>],</span><br><span class="line">                [<span class="number">2.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>],</span><br><span class="line">                [<span class="number">1.5</span>, <span class="number">2.5</span>, <span class="number">1.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">n = <span class="number">4</span></span><br><span class="line">M = np.logical_and.reduce(np.mod(X, <span class="number">1</span>) == <span class="number">0</span>, axis=-<span class="number">1</span>)</span><br><span class="line">M &amp;= (X.<span class="built_in">sum</span>(axis=-<span class="number">1</span>) == n)</span><br><span class="line"><span class="built_in">print</span>(X[M])</span><br></pre></td></tr></table></figure>
<h4 id="100-Compute-bootstrapped-95-confidence-intervals-for-the-mean-of-a-1D-array-X-i-e-resample-the-elements-of-an-array-with-replacement-N-times-compute-the-mean-of-each-sample-and-then-compute-percentiles-over-the-means-★★★"><a href="#100-Compute-bootstrapped-95-confidence-intervals-for-the-mean-of-a-1D-array-X-i-e-resample-the-elements-of-an-array-with-replacement-N-times-compute-the-mean-of-each-sample-and-then-compute-percentiles-over-the-means-★★★" class="headerlink" title="100. Compute bootstrapped 95% confidence intervals for the mean of a 1D array X (i.e., resample the elements of an array with replacement N times, compute the mean of each sample, and then compute percentiles over the means). (★★★)"></a>100. Compute bootstrapped 95% confidence intervals for the mean of a 1D array X (i.e., resample the elements of an array with replacement N times, compute the mean of each sample, and then compute percentiles over the means). (★★★)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Author: Jessica B. Hamrick</span></span><br><span class="line"></span><br><span class="line">X = np.random.randn(<span class="number">100</span>) <span class="comment"># random 1D array</span></span><br><span class="line">N = <span class="number">1000</span> <span class="comment"># number of bootstrap samples</span></span><br><span class="line">idx = np.random.randint(<span class="number">0</span>, X.size, (N, X.size))</span><br><span class="line">means = X[idx].mean(axis=<span class="number">1</span>)</span><br><span class="line">confint = np.percentile(means, [<span class="number">2.5</span>, <span class="number">97.5</span>])</span><br><span class="line"><span class="built_in">print</span>(confint)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>language</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>language</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>algorithm-lib</title>
    <url>/2018/11/18/algorithm-lib/</url>
    <content><![CDATA[<h1 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h1><h2 id="Partitional-Clustering-Algorithm"><a href="#Partitional-Clustering-Algorithm" class="headerlink" title="Partitional Clustering Algorithm"></a>Partitional Clustering Algorithm</h2><h3 id="k均值聚类"><a href="#k均值聚类" class="headerlink" title="k均值聚类"></a>k均值聚类</h3><h3 id="PAM-Partitioning-Around-Medoids"><a href="#PAM-Partitioning-Around-Medoids" class="headerlink" title="PAM(Partitioning Around Medoids)"></a>PAM(Partitioning Around Medoids)</h3><h3 id="CLARA-Clustering-Large-Applications"><a href="#CLARA-Clustering-Large-Applications" class="headerlink" title="CLARA(Clustering Large Applications)"></a>CLARA(Clustering Large Applications)</h3><h3 id="CLARANS-Clustering-Large-Applications-based-on-Randomlized-Search"><a href="#CLARANS-Clustering-Large-Applications-based-on-Randomlized-Search" class="headerlink" title="CLARANS(Clustering Large Applications based on Randomlized Search)"></a>CLARANS(Clustering Large Applications based on Randomlized Search)</h3><h2 id="Hierarchical-Clustering-Algorithm"><a href="#Hierarchical-Clustering-Algorithm" class="headerlink" title="Hierarchical Clustering Algorithm"></a>Hierarchical Clustering Algorithm</h2><h3 id="BIRCH-balanced-iterative-reducing-and-clustering-using-hierachies"><a href="#BIRCH-balanced-iterative-reducing-and-clustering-using-hierachies" class="headerlink" title="BIRCH(balanced iterative reducing and clustering using hierachies)"></a>BIRCH(balanced iterative reducing and clustering using hierachies)</h3><h3 id="CURE-Clustering-Using-Representatives"><a href="#CURE-Clustering-Using-Representatives" class="headerlink" title="CURE(Clustering Using Representatives)"></a>CURE(Clustering Using Representatives)</h3><h3 id="ROCKS-A-Robust-Clustering-Algorithm-for-Categorial-Attributes"><a href="#ROCKS-A-Robust-Clustering-Algorithm-for-Categorial-Attributes" class="headerlink" title="ROCKS(A Robust Clustering Algorithm for Categorial Attributes)"></a>ROCKS(A Robust Clustering Algorithm for Categorial Attributes)</h3><h2 id="Grid-based-Clustering-Algorithm"><a href="#Grid-based-Clustering-Algorithm" class="headerlink" title="Grid-based Clustering Algorithm"></a>Grid-based Clustering Algorithm</h2><h3 id="STING"><a href="#STING" class="headerlink" title="STING"></a>STING</h3><h3 id="WaveCluster"><a href="#WaveCluster" class="headerlink" title="WaveCluster"></a>WaveCluster</h3><h2 id="Model-based-Clustering-Algorithm"><a href="#Model-based-Clustering-Algorithm" class="headerlink" title="Model-based Clustering Algorithm"></a>Model-based Clustering Algorithm</h2><h3 id="Gaussian-model"><a href="#Gaussian-model" class="headerlink" title="Gaussian model"></a>Gaussian model</h3><h3 id="Latent-Dirichlet-Allocation"><a href="#Latent-Dirichlet-Allocation" class="headerlink" title="Latent Dirichlet Allocation"></a>Latent Dirichlet Allocation</h3><h2 id="Density-based-Clustering-Algorithm"><a href="#Density-based-Clustering-Algorithm" class="headerlink" title="Density-based Clustering Algorithm"></a>Density-based Clustering Algorithm</h2><h3 id="DBSCAN聚类"><a href="#DBSCAN聚类" class="headerlink" title="DBSCAN聚类"></a>DBSCAN聚类</h3><ul>
<li><a href="https://www.cnblogs.com/pinard/p/6208966.html">参考博客</a></li>
<li>关键概念：核心对象，密度直达，密度可达，密度相连</li>
<li>缺点：需要自己确定eps，Minpts</li>
</ul>
<h4 id="AE-DBSCAN-可以自己决定radius，Eps-parameters"><a href="#AE-DBSCAN-可以自己决定radius，Eps-parameters" class="headerlink" title="AE-DBSCAN(可以自己决定radius，Eps,parameters)"></a>AE-DBSCAN(可以自己决定radius，Eps,parameters)</h4><ul>
<li><strong>k-dist list</strong>:对于给定数据集D，k-dist list是数据集中对于每个点来说距离最近的k个点</li>
<li><strong>slope of a point with respect to another point</strong>:对于一个给定的$k-list=(k_1,k_2,…,k_n)$,$k_i$到$k_{i+1}$的斜率就是$k_ik_{i+1}$的斜率</li>
</ul>
<p><img src="/2018/11/18/algorithm-lib/1.png" alt="1"></p>
<h3 id="OPTICS"><a href="#OPTICS" class="headerlink" title="OPTICS"></a>OPTICS</h3><ul>
<li><a href="https://www.cnblogs.com/zhangruilin/p/5817784.html">参考博客</a></li>
<li>关键概念：核心距离，可达距离</li>
</ul>
<h3 id="DENCLUE"><a href="#DENCLUE" class="headerlink" title="DENCLUE"></a>DENCLUE</h3><h3 id="CLIQUE"><a href="#CLIQUE" class="headerlink" title="CLIQUE"></a>CLIQUE</h3><h1 id="关联规则"><a href="#关联规则" class="headerlink" title="关联规则"></a>关联规则</h1><h3 id="APriori算法"><a href="#APriori算法" class="headerlink" title="APriori算法"></a>APriori算法</h3><ul>
<li><a href="https://blog.csdn.net/qq_36219266/article/details/82707708">参考博客</a></li>
<li><a href="https://github.com/yantijin/Lean_DataMining">代码</a></li>
</ul>
<h3 id="FP-Tree算法"><a href="#FP-Tree算法" class="headerlink" title="FP_Tree算法"></a>FP_Tree算法</h3><ul>
<li><a href="https://blog.csdn.net/qq_36219266/article/details/82784299">参考博客</a></li>
<li><a href="https://github.com/yantijin/Lean_DataMining">代码</a></li>
</ul>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h3 id="ID3-C4-5"><a href="#ID3-C4-5" class="headerlink" title="ID3+C4.5"></a>ID3+C4.5</h3><ul>
<li><p><a href="https://yantijin.github.io/2018/11/17/决策树/">参考博客</a></p>
</li>
<li><p><a href="https://github.com/yantijin/Lean_DataMining">代码</a></p>
</li>
</ul>
<h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><ul>
<li><a href="https://blog.csdn.net/xiaqunfeng123/article/details/34820095">参考博客</a></li>
<li><p><a href="https://github.com/yantijin/Lean_DataMining">代码</a></p>
<h1 id="启发式算法"><a href="#启发式算法" class="headerlink" title="启发式算法"></a>启发式算法</h1><h3 id="GA"><a href="#GA" class="headerlink" title="GA"></a>GA</h3></li>
<li><p>基本构成：编码，种群初始化，选择，交叉，变异，局部搜索等</p>
</li>
</ul>
<h3 id="多目标进化算法"><a href="#多目标进化算法" class="headerlink" title="多目标进化算法"></a>多目标进化算法</h3><ul>
<li>NSGA，NSGA-II，SPEA，SPEA-II，MOEA/D,DMOEA-$\epsilon C$</li>
</ul>
<h3 id="差分进化算法"><a href="#差分进化算法" class="headerlink" title="差分进化算法"></a>差分进化算法</h3><h3 id="粒子群算法"><a href="#粒子群算法" class="headerlink" title="粒子群算法"></a>粒子群算法</h3><h3 id="分布估计算法"><a href="#分布估计算法" class="headerlink" title="分布估计算法"></a>分布估计算法</h3><h3 id="local-search"><a href="#local-search" class="headerlink" title="local search"></a>local search</h3><ul>
<li>tabu search</li>
<li>爬山法</li>
</ul>
<h3 id="蚁群算法"><a href="#蚁群算法" class="headerlink" title="蚁群算法"></a>蚁群算法</h3><h3 id="小生境"><a href="#小生境" class="headerlink" title="小生境"></a>小生境</h3><h1 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h1><h3 id="BP神经网络"><a href="#BP神经网络" class="headerlink" title="BP神经网络"></a>BP神经网络</h3><h3 id="决策树-见上"><a href="#决策树-见上" class="headerlink" title="决策树(见上)"></a>决策树(见上)</h3><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><h3 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h3><h3 id="AdaBoost提升装袋算法"><a href="#AdaBoost提升装袋算法" class="headerlink" title="AdaBoost提升装袋算法"></a>AdaBoost提升装袋算法</h3><ul>
<li><a href="https://blog.csdn.net/qq_36219266/article/details/82799071">参考博客</a></li>
<li><a href="https://github.com/yantijin/Lean_DataMining">代码</a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm Library</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo configuration on multiple computers</title>
    <url>/2018/11/04/hexo-configuration-on-multiple-computers/</url>
    <content><![CDATA[<h3 id="Todo-for-myself"><a href="#Todo-for-myself" class="headerlink" title="Todo for myself"></a>Todo for myself</h3><ul>
<li><p>Install <a href="https://nodejs.org/en/">node.js</a></p>
</li>
<li><p>Install hexo</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm  install -g hexo</span><br></pre></td></tr></table></figure>
</li>
<li><p>Download latest version in my Gituhb</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/yantijin/yantijin.github.io.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>Download latest themes configuration in another foler</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/yantijin/Theme_Next.git</span><br></pre></td></tr></table></figure>
<p>Then copy the theme into <strong>github.yantijin.io/themes</strong></p>
</li>
<li><p>Install  dependencies for your blog</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure>
</li>
<li><p>Something needs to modifiy</p>
<ul>
<li><p>About <strong>MATHJAX</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-readerer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<p> Next step is to modify the files in <strong>node_modules/kramed/lib/rules/inlime.js</strong>.</p>
<p>Change line 10 and line 20:</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line"><span class="attr">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line"><span class="comment">// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line"> <span class="attr">em</span>: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure>
<p>​</p>
</li>
</ul>
</li>
<li><p><a href="https://juejin.im/post/5acf22e6f265da23994eeac9">REF for Detail</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Entertainment</category>
      </categories>
      <tags>
        <tag>Try4New</tag>
      </tags>
  </entry>
  <entry>
    <title>安装教程</title>
    <url>/2018/12/11/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="大数据范畴"><a href="#大数据范畴" class="headerlink" title="大数据范畴"></a>大数据范畴</h1><h2 id="Ubuntu16-04-安装Hadoop伪分布式"><a href="#Ubuntu16-04-安装Hadoop伪分布式" class="headerlink" title="Ubuntu16.04 安装Hadoop伪分布式"></a>Ubuntu16.04 安装Hadoop伪分布式</h2><ul>
<li><a href="https://blog.csdn.net/m0_37138008/article/details/71078740">参考文章</a></li>
<li>主要步骤：添加新用户，配置ssh免密码登录，安装jdk，安装hadoop，修改配置文件形成伪分布式，启动后用jps测试看是否启动</li>
</ul>
<h2 id="Ubuntu中Spark安装教程"><a href="#Ubuntu中Spark安装教程" class="headerlink" title="Ubuntu中Spark安装教程"></a>Ubuntu中Spark安装教程</h2><ul>
<li><a href="https://www.cnblogs.com/qingyunzong/p/8903714.html">参考文章</a></li>
<li>主要步骤：在hadoop安装基础上进行安装，Scala安装可选，然后进行解压对配置文件进行配置即可</li>
</ul>
<h1 id="安装IDE"><a href="#安装IDE" class="headerlink" title="安装IDE"></a>安装IDE</h1><h2 id="Ubuntu下安装sublime"><a href="#Ubuntu下安装sublime" class="headerlink" title="Ubuntu下安装sublime"></a>Ubuntu下安装sublime</h2><ul>
<li><a href="https://imcn.me/html/y2017/30349.html">参考文章</a></li>
</ul>
<h3 id="插件安装"><a href="#插件安装" class="headerlink" title="插件安装"></a>插件安装</h3><ul>
<li>安装package control:<a href="https://blog.csdn.net/wxl1555/article/details/69941451">参考博客</a></li>
<li>常用插件：<ul>
<li>汉化版本： localization</li>
<li>侧边栏： sidebarEnhancemnet</li>
<li>Markdown 语法： Markdown Editing</li>
<li>Anaconda+sublimelinter+sublimeRepl</li>
<li>编码： convert2UTF8</li>
</ul>
</li>
</ul>
<h3 id="配置中文输入"><a href="#配置中文输入" class="headerlink" title="配置中文输入"></a>配置中文输入</h3><ul>
<li><a href="https://www.jianshu.com/p/bf05fb3a4709">参考文章</a></li>
<li>博客讲的很好，可以直接下载sublime-text-imfix来进行安装，然后注意，此种方法是需要在命令行中启动才能对其输入中文</li>
</ul>
]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2018/11/17/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h3 id="决策树建树的基本流程"><a href="#决策树建树的基本流程" class="headerlink" title="决策树建树的基本流程"></a>决策树建树的基本流程</h3><p><img src="/2018/11/17/%E5%86%B3%E7%AD%96%E6%A0%91/1.png" alt="1"></p>
<h3 id="划分选择（为使”纯度”越来越高）"><a href="#划分选择（为使”纯度”越来越高）" class="headerlink" title="划分选择（为使”纯度”越来越高）"></a>划分选择（为使”纯度”越来越高）</h3><ul>
<li><p><strong>信息增益</strong></p>
<p>信息熵为(越小则纯度越高)</p>
<script type="math/tex; mode=display">
Ent(D)=-\sum^{|y|}_{k=1}p_klog_2p_k</script><p>信息增益为</p>
<script type="math/tex; mode=display">
Gain(D)=Ent(D)-\sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)</script><p><strong>缺点</strong>：对于可以取值数目较多的属性有所偏好。</p>
</li>
<li><p><strong>信息增益率</strong>（克服上述所述缺点）</p>
<script type="math/tex; mode=display">
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}</script><p>其中</p>
<script type="math/tex; mode=display">
IV(a)=-\sum^V_{v=1}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}</script><p>称为属性a的固有属性值</p>
</li>
<li><p><strong>Gini系数</strong></p>
<script type="math/tex; mode=display">
Gini(D)=\sum^{|y|}_{k=1}\sum_{k'\neq k}p_kp_{k'}=1-\sum^{|y|}_{k=1}p_k^2</script><p>反应了从数据集D中随机抽取两个样本，其类别标记不一致的概率</p>
<p>基尼指数(选择最小的)</p>
<script type="math/tex; mode=display">
Gini\_index(D,a)=\sum^V_{v=1}\frac{|D^v|}{|D|}Gini(D^v)</script></li>
</ul>
<h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><ul>
<li><p><strong>预剪枝</strong></p>
<ul>
<li>若当前划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点</li>
</ul>
</li>
<li><p><strong>后剪枝</strong></p>
<ul>
<li><p>自底向上对非叶子节点进行考察，若将该节点对应的子树替换为叶结点能带来泛化能力的提升，则将该子树替换为叶结点</p>
</li>
<li><p><strong>代价复杂性剪枝</strong>：对于每个非叶子节点计算它的表面误差率增益值$\alpha$。</p>
<script type="math/tex; mode=display">
\alpha = \frac{R(t)-R(T_t)}{|N_{T_t}|-1}</script><p>$|N_{T_t}|$为子树中包含的叶子结点的个数</p>
<p>$R(t)=r(t)*p(t)$为结点的误差代价，如果该结点被剪枝,r(t)为结点t的误差率；p(t)为结点t上的数据占所有数据的比例</p>
<p>$R(T_t)$是子树Tt的误差代价，如果该节点不被剪枝。它等于子树Tt上所有叶子节点的误差代价之和。</p>
<p>找到$\alpha$值最小的非叶子节点，令其左右孩子为NULL</p>
</li>
<li><p><strong>最小误差剪枝</strong></p>
</li>
<li><p><strong>悲观误差剪枝</strong></p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>决策树</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>数理统计</title>
    <url>/2018/11/14/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/</url>
    <content><![CDATA[<h1 id="总体样本和抽样分布"><a href="#总体样本和抽样分布" class="headerlink" title="总体样本和抽样分布"></a>总体样本和抽样分布</h1><h2 id="总体样本"><a href="#总体样本" class="headerlink" title="总体样本"></a>总体样本</h2><ul>
<li><strong>总体</strong>：研究对象的全体（一般只关心某一个数量指标，如体重等）</li>
<li><strong>样本</strong>：总体分布一般未知，从总体中按照一定规则抽取一部分个体，这部分个体叫做这个总体的一个样本<ul>
<li><strong>简单随机抽样</strong>： 在相同条件下对总体X进行n次重复，独立观察，得到的样本(X1,X2,…Xn)成为来自总体X的一个简单随机样本（实际上课程中都是<strong>简单随机抽样</strong>）。</li>
<li>设X是具有分布函数F的随机变量，那么对于有同样分布函数的n个随机变量X1,X2,…Xn,称为<strong>样本</strong>，也可以将这n个随机变量（样本）写成一个随机向量(X1,X2,…Xn)。</li>
<li><strong>样本二重性</strong> 抽样前，对于(X1,X2,…Xn)这个n维随机向量，由于是在相同条件下重复独立观察，故而它的联合分布函数与联合概率密度分别为每个随机变量分布函数和概率密度函数的乘积；抽样后，得到(x1,x2,…xn)，称为(X1,X2,…Xn)的观察值，也叫作样本值。</li>
</ul>
</li>
</ul>
<h2 id="抽样分布"><a href="#抽样分布" class="headerlink" title="抽样分布"></a>抽样分布</h2><ul>
<li><strong>统计量</strong>：设X1,X2,…Xn为来自总体X 的一个样本，g(X1,X2,…Xn)是X1,X2,…Xn的函数，若 g 中不含任何未知参数，则称g(X1,X2,…Xn)是一统计量.<strong>注意：统计量是<em>不含未知参数</em>的样本的函数，本质上是随机变量</strong></li>
<li>常用统计量：<ul>
<li>样本均值：   \( \bar X=\frac{1}{n} \Sigma^{n}_{i=1}X_i \),根据辛钦大数定律，\(\bar X\)趋于E(x)<ul>
<li>进一步定义样本k阶矩， \(A_k=\frac{1}{n}\sum^{n}_{i=1}X_i^{k}\),同样的根据辛钦大数定律，\(A_k\)趋于\(E(X^k)\)</li>
</ul>
</li>
<li>样本方差： \(S^2=\frac{1}{n-1} \sum^{n}_{i=1}(X_i-\bar X)^2\),根据辛钦大数定律，\(S^2=E(x^2)-E^2(x)\)<ul>
<li>样本标准差： \(S=\sqrt{S^2}\)</li>
<li>样本k阶中心矩： \(B_k=\frac{1}{n}\sum^{n}_{i=1}(X_i-\bar X)^k\),k=2,3…</li>
</ul>
</li>
<li>中位数： 设有容量为n的样本(X1,X2,…Xn),将其按从小到大分为 \(X_{(1)}\leq X_{(2)}\leq … \leq X_{(n)}\),则中位数定义为<img src="/2018/11/14/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/1.png" alt="avatar"></li>
</ul>
</li>
</ul>
<h2 id="抽样分布及其上分位数"><a href="#抽样分布及其上分位数" class="headerlink" title="抽样分布及其上分位数"></a>抽样分布及其上分位数</h2><ul>
<li>统计上的三大分布<ul>
<li><strong>\(\chi^2\)分布</strong>：<ul>
<li>定义：设X1,X2,…Xn为来自总体N(0,1)的样本，那么称统计量<script type="math/tex; mode=display">\chi^2=X\_1^2+X\_2^2+...+X\_n^2</script>服从<strong>自由度为n</strong>的\(\chi^2\)分布，记为 \(\chi^2 \sim \chi^2(n)\),n表示独立随机变量的个数</li>
<li>性质：<ul>
<li>可加性：\(X_1\sim\chi^2(n_1)\),\(X_2\sim\chi^2(n_2)\),且X1和X2相互独立，则<script type="math/tex; mode=display">X\_1+X\_2\sim \chi^2(n\_1+n\_2)</script></li>
<li>设X1,X2,…Xn相互独立，均服从正态分布\(N(\mu,\sigma^2)\),则<script type="math/tex; mode=display">\chi^2=\frac{1}{\sigma^2}\sum^n\_{i=1}(X\_i-\mu)^2\sim\chi^2(n)</script></li>
<li>若\(X\sim \chi^2(n)\)，则 E(X)=n,D(X)=2n</li>
<li><strong>上\(\alpha\)-分位点</strong>：给定正数\(\alpha\)(0&lt;\(\alpha\)&lt;1),称满足条件：<script type="math/tex; mode=display">P ( \chi^2>\chi^2\_\alpha(n) )=\int^\infty\_{\chi^2\_\alpha(n)}f(y)dy=\alpha</script>的点\(\chi^2_\alpha(n)\)称为\(\chi^2(n)\)分布的上\(\alpha\)-分位点</li>
</ul>
</li>
</ul>
</li>
<li><strong>t分布</strong>：<ul>
<li>定义：设\(X\sim N(0,1),Y\sim \chi^2(n)\),且X与Y相互独立，则称随机变量<script type="math/tex; mode=display">t=\frac{X}{\sqrt{Y/n}}</script>服从<strong>自由度为n</strong>的t分布，记为 \(t\sim t(n)\)</li>
<li>性质：<ul>
<li>E(t)=0;D(t)=n/(n-2),对n&gt;2</li>
<li>t分布密度函数关于t=0对称，且\(\lim_{|t|\to\infty} f(t)=0\),n充分大时，t分布近似于N(0,1)</li>
<li><strong>上\(\alpha\)-分位点</strong>:给定正数\(\alpha\)(0&lt;\(\alpha\)&lt;1),称满足条件：<script type="math/tex; mode=display">P ( t>t\_\alpha(n) )=\int^\infty\_{t\_\alpha(n)}f(t)dt=\alpha</script>的点\(t_\alpha(n)\)称为\(t(n)\)分布的上\(\alpha\)-分位点<ul>
<li>根据对称性有\(t_{1-\alpha}(n)=-t_\alpha(n)\)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>F分布</strong><ul>
<li>定义： 设 \(U \sim \chi^2(n_1),V\sim \chi^2(n_2)\),U与V相互独立，则称随机变量<script type="math/tex; mode=display">F=\frac{U/n\_1}{V/n\_2}</script>服从<strong>自由度为n1及n2</strong>的F分布，记为\(F\sim F(n_1,n_2)\).</li>
<li>性质：<ul>
<li>\(\frac{1}{F}=\frac{V/n_2}{U/n1}\sim F(n_2,n_1)\)</li>
<li><strong>上\(\alpha\)-分位点</strong>:给定正数\(\alpha\)(0&lt;\(\alpha\)&lt;1),称满足条件：<script type="math/tex; mode=display">P ( F>F\_\alpha(n\_1,n\_2) )=\int^\infty\_{F\_\alpha(n\_1,n\_2)}\Psi(y)dy=\alpha</script>的点\(F_\alpha(n_1,n_2)\)称为\(F(n_1,n_2)\)分布的上\(\alpha\)-分位点<br><strong><script type="math/tex">F\_{1-\alpha}(n\_1,n\_2)=\frac{1}{F\_\alpha(n\_2,n\_1)}</script></strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>几个重要的抽样分布定理：ppt6-2 P41<ul>
<li>设X1,…Xn为来自总体X的一个样本，且\(EX=\mu,DX=\sigma^2\),则<script type="math/tex; mode=display">E\bar X=\mu,D\bar X=\frac{\sigma^2}{n},E(S^2)=\sigma^2</script></li>
<li>上述\(\bar X\)满足 \(\bar X\sim N(\mu,\frac{\sigma^2}{n})\)或者\(\frac{\bar X-\mu}{\sigma/\sqrt(n)}\sim N(0,1)\)</li>
<li><strong>(注意此处和第一条的条件不同！！)</strong>设X1,X2,…Xn是来自正态总体\(N(\mu,\sigma^2)\)的样本，\(\bar X\)和\(S^2\)分别为样本均值和方差则<script type="math/tex; mode=display">\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)</script><script type="math/tex; mode=display">\bar X和S^2相互独立</script><script type="math/tex; mode=display">\frac{\bar X-\mu}{S/\sqrt{n}}\sim t(n-1)</script></li>
<li>两正态总体样本均值差的分布 ：设\(X\sim N(\mu_1,\sigma^2),Y\sim N(\mu_2,\sigma^2)\),<strong>（注意方差相同）</strong>，X与Y独立，设X1,X2,…Xn为来自总体X的样本，Y1,Y2,…Yn为来自总体Y的样本，则有<script type="math/tex; mode=display">\frac{\bar X-\bar Y-(\mu\_1-\mu\_2)}{S\_w\sqrt{\frac{1}{n\_1}+\frac{1}{n\_2}}}\sim t(n\_1+n\_2-2)</script>其中<script type="math/tex; mode=display">S\_w^2=\frac{(n\_1-1)S\_1^2+(n\_2-1)S\_2^2}{n\_1+n\_2-2}</script></li>
<li>两正态总体样本方差比的分布设\(X\sim N(\mu_1,\sigma_1^2),Y\sim N(\mu_2,\sigma_2^2)\),<strong>（注意方差不同）</strong>，X与Y独立，设X1,X2,…Xn为来自总体X的样本，Y1,Y2,…Yn为来自总体Y的样本，则有<script type="math/tex; mode=display">\frac{S\_1^2/S\_2^2}{\sigma\_1^2/\sigma\_2^2}\sim F(n\_1-1,n\_2-1)</script></li>
</ul>
</li>
</ul>
<h1 id="参数估计（点估计-区间估计）"><a href="#参数估计（点估计-区间估计）" class="headerlink" title="参数估计（点估计+区间估计）"></a>参数估计（点估计+区间估计）</h1><h2 id="点估计—估计未知参数的值"><a href="#点估计—估计未知参数的值" class="headerlink" title="点估计—估计未知参数的值"></a>点估计—估计未知参数的值</h2><h3 id="矩估计法"><a href="#矩估计法" class="headerlink" title="矩估计法"></a>矩估计法</h3><ul>
<li>整体思想：用样本的K阶原点矩作为总体k阶原点矩的一个估计，即用\(A_k=\frac{1}{n}\sum^n_{i=1}X_i^k\)估计\(\mu_k\),进而估计未知参数\(\theta\)</li>
<li><img src="/2018/11/14/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/2.png" alt="avatar">，例题见PPT7-1，P16，P18</li>
</ul>
<h3 id="最大似然估计法"><a href="#最大似然估计法" class="headerlink" title="最大似然估计法"></a>最大似然估计法</h3><ul>
<li>整体思想： 根据样本观测值，选择参数p的估计\(\hat p\),使得样本在该样本值附近出现的可能性最大。</li>
<li><strong>最大似然估计法</strong>:<ul>
<li><strong>若X为离散型随机变量</strong>，设X1,X2,…Xn为来自X的样本，则X1,X2,…Xn的联合分布律为 \(\prod^n_{i=1}p(x_i;\theta)\),又设x1,x2,…xn为X1,X2,…Xn的样本值，易知事件{X1=x1,X2=x2,…Xn=xn}发生的概率为：<script type="math/tex; mode=display">L(\theta)=L(x\_1,...,x\_n,\theta)=\prod^n\_{i=1}p(x\_i,\theta)</script>它是关于\(\theta\)的函数，\(L(\theta)\)称为样本的似然函数<br>固定样本观测值x1,x2,…xn,挑选使得似然函数L最大的参数\(\hat\theta\),\(\hat\theta\)与x1,x2,…xn有关，称之为\(\theta\)的<strong>最大似然估计值</strong>，称\(\hat \theta(X_1,…X_n)\)为参数\(\theta\)的<strong>最大似然估计量</strong>。</li>
<li><strong>若X为连续性随机变量</strong>，设其概率密度为\(f(x;\theta)\),\(\theta\)为待估参数，设X1,X2,…Xn为来自X的样本，则X1,X2,…Xn的联合密度为：<script type="math/tex; mode=display">\prod^n\_{i=1}f(x\_i;\theta)</script>设x1,x2,…xn为X1,X2,…Xn的一个样本值，则随机点(X1,X2,…Xn)落在(x1,x2,…xn)的邻域(边长为dx1,dx2,…dxn的n维立方体)内的概率近似为:<script type="math/tex; mode=display">\prod^{n}\_{i=1}f(x\_i;\theta)dx\_i</script>取\(\theta\)的估计值\(\hat\theta\),使得上式概率取到最大值。<br>但由于\(\prod^n_{i=1}dx_i\)不随\(\theta\)变化，故只需要考虑<script type="math/tex; mode=display">L(\theta)=L(x1,...xn;\theta)=\prod^{n}\_{i=1}f(x\_i;\theta)</script>的最大值，上式为样本的似然函数，挑选使得L最大的参数\(\hat\theta\)，称之为\(\theta\)的<strong>最大似然估计值</strong>，称\(\hat\theta(X1,X2,…Xn)\)为\(\theta\)的<strong>最大似然估计量</strong>。</li>
</ul>
</li>
<li>最大似然估计法步骤：<ul>
<li>先得到总体的分布律(概率密度)</li>
<li>写出样本的联合分布律（联合密度）</li>
<li>而后将\(\theta\)看做自变量，其他看作已知量，构造似然函数\(L(\theta)\)<strong>此处应注意L是否是连续的，若连续进行下一步，不连续需要根据定义求，见PPT7-1 P50</strong></li>
<li>求似然函数L的最大值点，即求出L关于\(\theta\)的导数，或者先对L求对数，再关于\(\theta\)求导数，让导数为0；</li>
</ul>
</li>
<li>最大似然估计的性质<ul>
<li>不变性：设\(\theta\)的函数\(u=u(\theta)\)具有单值反函数\(\theta=\theta(u)\).假设\(\hat\theta\)是X的概率分布中参数\(\theta\)的最大似然估计，则\(\hat u=u(\hat\theta)\)是\(u(\theta)\)的最大似然估计。</li>
</ul>
</li>
</ul>
<h2 id="估计量的评选标准"><a href="#估计量的评选标准" class="headerlink" title="估计量的评选标准"></a>估计量的评选标准</h2><ul>
<li><strong>无偏性</strong>：若估计量\(\hat\theta=\hat\theta(X1,X2,…Xn)\)的数学期望\(E(\hat\theta)\)存在，且对任意\(\theta\)有<script type="math/tex; mode=display">E(\hat\theta)=\theta</script>则称\(\hat\theta\)为\(\theta\)的无偏估计量。<ul>
<li>样本均值\(\bar X\)和样本方差\(S^2\)分别是总体均值\(\mu\)和总体方差\(\sigma^2\)的无偏估计。</li>
<li>\(A_k=\frac{1}{n}\sum^n_{i=1}X_i^k\)是总体k阶矩\(\mu_k\)的无偏估计量。</li>
<li>\(X\sim N(\mu,\sigma^2)\)，其中\(\mu,\sigma^2\)未知，用最大似然估计法得到\(\mu,\sigma^2\)的估计量，会发现\(\hat\mu\)是无偏估计，但\(\sigma^2\)不是无偏估计。<strong>（PPT7-2 P11）</strong></li>
</ul>
</li>
<li><strong>有效性</strong>：若\(\hat\theta_1=\hat\theta_1(X1,X2,…Xn)\)和\(\hat\theta_2=\hat\theta_2(X1,…Xn)\)均为参数\(\theta\)的无偏估计，若对于任意\(\theta\)，有<script type="math/tex; mode=display">D(\hat\theta\_1\leq\hat\theta_2)</script>且至少对于某一个\(\theta\)上式中的不等号成立，则称\(\hat\theta_1\)较\(\hat\theta_2\)有效。</li>
<li><strong>相合性</strong>： 设\(\hat\theta(X1,X2,…Xn)\)为参数\(\theta\)的估计量，若对于任意\(\theta\),当n区域无穷时，\(\hat\theta(X1,X2,…Xn)\)依概率收敛于\(\theta\),则称\(\hat\theta\)为参数\(\theta\)的相合估计量，即 <script type="math/tex; mode=display">\lim\_{n\to\infty}P(|\hat\theta-\theta|<\epsilon)=1</script></li>
</ul>
<h2 id="区间估计—根据样本构造适当区间，使其以一定概率包含未知参数或未知参数的已知函数的真值"><a href="#区间估计—根据样本构造适当区间，使其以一定概率包含未知参数或未知参数的已知函数的真值" class="headerlink" title="区间估计—根据样本构造适当区间，使其以一定概率包含未知参数或未知参数的已知函数的真值"></a>区间估计—根据样本构造适当区间，使其以一定概率包含未知参数或未知参数的已知函数的真值</h2><ul>
<li><strong>置信区间</strong>：设总体X的分布函数\(F(x,\theta)\)含有一个未知参数\(\theta\),对于给定值\(\alpha\)\(0&lt;\alpha&lt;1\),若从抽自X的样本X1,X2,…Xn确定两个统计量<script type="math/tex; mode=display">\underline\theta=\underline\theta(X1,...Xn),\bar\theta=\bar\theta(X1,...Xn),(\underline\theta<\bar\theta)</script>对于任意\(\theta\)满足<script type="math/tex; mode=display">P(\underline\theta(X1,...,Xn)<\theta<\bar\theta(X1,...,Xn))\geq 1-\alpha</script>\(\underline\theta 和\bar\theta\)分别称为<strong>置信水平\(1-\alpha\)</strong>的双侧置信区间的<strong>置信下限</strong>和<strong>置信上限</strong><ul>
<li>X是连续型随机变量，对于给定\(\alpha\)，按照要求 \(P(\underline\theta&lt;\theta&lt;\bar\theta)=1-\alpha\),求出置信区间</li>
<li>X是离散型随机变量时，对于给定的\(\alpha\),常找不到区间使得\(p(\underline\theta&lt;\theta&lt;\bar\theta)=1-\alpha\),此时应找到区间使得在这区间内的概率最小为\(1-\alpha\),并尽可能接近\(1-\alpha\)</li>
</ul>
</li>
<li><strong>构造置信区间的方法</strong><ul>
<li>枢轴量法：<strong>不用管定义，直接看题PPT7-3 P14</strong></li>
</ul>
</li>
</ul>
<h2 id="正态总体均值与方差的区间估计"><a href="#正态总体均值与方差的区间估计" class="headerlink" title="正态总体均值与方差的区间估计"></a>正态总体均值与方差的区间估计</h2><ul>
<li><p>单个总体\(N(\mu,\sigma^2)\)的情形</p>
<ul>
<li>背景：X1,X2,…Xn为总体\(N(\mu,\sigma^2)\)的样本，\(\bar X和S^2\)分别为样本均值和方差，给定的置信水平为\(1-\alpha\)</li>
<li><strong>均值\(\mu\)的置信区间</strong><ul>
<li>\(\sigma^2\)已知，均值\(\mu\)的置信水平为\(1-\alpha\)的置信区间为<script type="math/tex; mode=display">(\bar X\pm \frac{\sigma}{\sqrt{n}}z\_{\frac{\alpha}{2}})</script></li>
<li>\(\sigma^2\)未知，均值\(\mu\)的置信水平\(1-\alpha\)的置信区间，由于<script type="math/tex; mode=display">t=\frac{\bar X-\mu}{S/\sqrt{n}}\sim t(n-1)</script>因此对于给定的置信水平\(1-\alpha\)，确定分位数\(t_{\frac{\alpha}{2}}(n-1)\),使得<script type="math/tex; mode=display">P(|\frac{\bar X-\mu}{S/\sqrt{n}}|<t\_{\frac{\alpha}{2}}(n-1))=1-\alpha</script>从中解得为<script type="math/tex; mode=display">(\bar X\pm \frac{S}{\sqrt{n}}t\_{\frac{\alpha}{2}}(n-1))</script></li>
</ul>
</li>
<li><strong>方差 \(\sigma^2\)的置信区间</strong>(只考虑\(\mu\)未知的情形)</li>
<li>由于\(\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)\),故而对于置信水平\(1-\alpha\)，确定分位数\(\chi^2_{1-\frac{\alpha}{2}}(n-1),\chi^2_{\frac{\alpha}{2}}(n-1)\),使得<script type="math/tex; mode=display">P(\chi^2\_{1-\frac{\alpha}{2}}(n-1)\leq \frac{(n-1)S^2}{\sigma^2} \leq \chi^2\_{\frac{\alpha}{2}}(n-1))=1-\alpha</script>从中解得置信水平为\(1-\alpha\)的置信区间为<script type="math/tex; mode=display">(\frac{\sqrt{n-1}S}{\sqrt{\chi^2\_{\frac{\alpha}{2}}(n-1)}},\frac{\sqrt{n-1}S}{\sqrt{\chi^2\_{1-\frac{\alpha}{2}}(n-1)}})</script></li>
<li>总结：<img src="/2018/11/14/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/3.png" alt="avatar"></li>
</ul>
</li>
<li><p>两个正态总体\(N(\mu_1,\sigma_1^2),N(\mu_2,\sigma^2_2)\)的情形</p>
<ul>
<li><p>背景：定的置信水平为\(1-\alpha\)，X1,X2,…Xn为第一个总体的样本，Y1,Y2,…,Yn为第二个总体的样本，已知\(\bar X和\bar Y\)为总体的样本均值，\(S^2_1和S^2_2\)为两总体的样本方差</p>
</li>
<li><p><strong>两个总体均值差\(\mu_1-\mu_2\)的置信区间</strong></p>
<ul>
<li>\(\sigma_1^2,\sigma_2^2\)已知，为\((\bar X-\bar Y\pm z_{\frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}})\)</li>
<li>\(\sigma_1^2=\sigma_2^2=\sigma^2\),但是\(\sigma^2\)未知，则结果为<script type="math/tex; mode=display">(\bar X-\bar Y\pm t\_{\frac{\alpha}{2}}(n\_1+n\_2-2)S\_w\sqrt{\frac{1}{n\_1}+\frac{1}{n\_2}})</script></li>
</ul>
</li>
<li><p><strong>两个总体方差比\(\sigma^2_1/\sigma^2_2\)的置信区间</strong>(只考虑均值未知情形)</p>
</li>
<li><p>选取\(\frac{S_1^2/S_2^2}{\sigma_1^2/\sigma_2^2}\)作为枢轴量，结果为</p>
<script type="math/tex; mode=display">(\frac{S^2\_1}{S^2\_2}\frac{1}{F\_{\frac{\alpha}{2}}(n\_1-1,n\_2-1)}<\frac{\sigma^2\_1}{\sigma^2\_2}<\frac{S^2\_1}{S^2\_2}\frac{1}{F\_{1-\frac{\alpha}{2}}(n\_1-1,n\_2-1)})</script></li>
<li><p>总结：<img src="/2018/11/14/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/4.png" alt="avatar"></p>
</li>
</ul>
</li>
</ul>
<style type="text/css">
    #biaoge{margin:6px; padding:2px; text-align:center; }
</style>

<h1 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ul>
<li>根据样本信息检验关于总体的假设是否正确</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li>参数假设检验(总体分布已知，检验关于未知参数的某个假设)；</li>
<li>非参数假设检验(总体分布未知时的假设检验问题)。</li>
</ul>
<h2 id="假设检验的基本思想和步骤："><a href="#假设检验的基本思想和步骤：" class="headerlink" title="假设检验的基本思想和步骤："></a><strong>假设检验的基本思想和步骤</strong>：</h2><ul>
<li>(1)根据实际问题要求，提出原假设(\(H_0\))和备择假设(\(H_1\))<ul>
<li>原假设：需要检验的假设</li>
<li>备择假设：与原假设对立面的全体或者一部分</li>
</ul>
</li>
<li>(2)构造检验统计量——即判断\(H_0\)是否为真<ul>
<li>做法：<strong>先假定\(H_0\)成立</strong>，然后用<strong>样本去判断其真伪。</strong>、</li>
<li>由于样本所含信息很分散，需要<strong>构造统计量\(T(X_1,X_2,…X_n)</strong>来做判断，称之为<strong>检验统计量</strong></li>
</ul>
</li>
<li>(3)给定显著性水平\(\alpha\)(\(0&lt;\alpha&lt;1\))及样本容量n<ul>
<li><strong>注意：由于作出决策的依据是一个样本，但实际上\(H_0\)为真的时候依然可能做出拒绝\(H_0\)的决策，犯这种错误的概率要控制在一个较小的数\(\alpha\)，使得</strong><script type="math/tex; mode=display">P(当H\_0为真时拒绝H\_0)\leq \alpha</script></li>
</ul>
</li>
<li>(4)检验的拒绝域<ul>
<li>按照\(P(当H_0为真拒绝H_0)=\alpha\)，求出拒绝域,注意：其中\(\alpha\)称为显著性水平，犯第一类错误的最大概率</li>
</ul>
</li>
<li>(5)将具体观察值x1,x2,…xn代入，之后做出决策，是接受原假设还是拒绝原假设<ul>
<li><strong>注意：在给定\(\alpha\)的前提下，接受还是拒绝原假设完全取决于样本值，因此所做检验可能导致以下两类错误</strong><ul>
<li>第一类错误        弃真错误</li>
<li>第二类错误        取伪错误</li>
</ul>
</li>
</ul>
</li>
</ul>
<div id="biaoge">
<table align="center">
<tbody>
<tr>
  <td> </td>
  <td colspan="2">实际情况</td>
</tr>
<tr>
  <td>决定</td>
  <td>H0为真</td>
  <td>H0不为真</td>
</tr>
<tr>
  <td>拒绝H0</td>
  <td>第一类错误</td>
  <td>正确</td>
</tr>
<tr>
  <td>接受H0</td>
  <td>正确</td>
  <td>第二类错误</td>
</tr>
</tbody>
</table>
</div>

<h2 id="假设检验的指导思想："><a href="#假设检验的指导思想：" class="headerlink" title="假设检验的指导思想："></a>假设检验的指导思想：</h2><ul>
<li>(1)控制犯第一类错误的概率不超过\(\alpha\),然后，若有必要，通过</li>
<li>(2)增大样本容量的方法来减少犯第二类错误的概率</li>
<li>只对第一类错误的概率加以控制，而不考虑犯第二类错误的概率的检验，称为<strong>显著性检验</strong>——控制第一类错误的原则</li>
<li>注意<ul>
<li>备择假设可以是<strong>双侧</strong>，也可以是<strong>单侧</strong>，假设检验分为<strong>双边(备择)假设检验</strong>，<strong>单边(备择)假设检验</strong></li>
<li>通常把<strong>有把握的、有经验的结论</strong>作为原假设，或者尽可能使<strong>后果严重的错误</strong>成为第一类错误</li>
</ul>
</li>
</ul>
<h2 id="正态总体均值的假设检验"><a href="#正态总体均值的假设检验" class="headerlink" title="正态总体均值的假设检验"></a>正态总体均值的假设检验</h2><h3 id="单个总体-N-mu-sigma-2-均值-mu-的检验"><a href="#单个总体-N-mu-sigma-2-均值-mu-的检验" class="headerlink" title="单个总体\(N(\mu,\sigma^2)\)均值\(\mu\)的检验"></a><strong>单个总体\(N(\mu,\sigma^2)\)均值\(\mu\)的检验</strong></h3><ul>
<li>背景：设总体\(X\sim N(样本\mu,\sigma^2),X_1,X_2,…X_n\)为来自总体X的样本</li>
<li><strong>\(\sigma^2=\sigma^2_0\)已知</strong>，关于均值\(\mu\)的检验(Z检验)：<ul>
<li>(1)\(H_0:\mu=\mu_0,H_1:\mu \neq\mu_0\)</li>
<li>(2)构造检验统计量<script type="math/tex; mode=display">Z=\frac{\bar X-\mu\_0}{\sigma\_0/\sqrt{n}}\sim N(0,1)</script>给定显著性水平\(\alpha\)</li>
<li>(3)按照控制第一类错误的原则，有<script type="math/tex; mode=display">P(|\bar X-\mu\_0|>C)=P(|\frac{\bar X-\mu\_0}{\sigma\_0/\sqrt{n}}|>k)=\alpha</script>由此，<script type="math/tex; mode=display">k=z\_{\frac{\alpha}{2}}\Longrightarrow C=\frac{\sigma_0}{\sqrt{n}}z\_{\frac{\alpha}{2}}</script></li>
<li>(4)拒绝域为<script type="math/tex; mode=display">W=(x\_1,...,x\_n):\frac{|\bar X-\mu\_0|}{\sigma\_0/\sqrt{n}}>z\_{\frac{\alpha}{2}}</script>查表\(z_{\frac{\alpha}{2}}\),计算\(\frac{|\bar X-\mu_0|}{\sigma_0/\sqrt{n}}\),若其大于\(z_{\frac{\alpha}{2}}\)，拒绝原假设，否则，接受原假设。</li>
</ul>
</li>
<li>单边检验，例如关于均值的假设检验，\(\sigma^2\)已知<ul>
<li>(1)\(H_0:\mu\leq\mu_0,H_1:\mu \geq\mu_0\)</li>
<li>(2)当\(\mu\leq\mu_0\)成立时，<script type="math/tex; mode=display">\{\frac{\bar X-\mu\_0}{\sigma/\sqrt{n}}>k\}\subset \{\frac{\bar X-\mu}{\sigma/\sqrt{n}}>k\}</script><script type="math/tex; mode=display">P(\frac{\bar X-\mu\_0}{\sigma/\sqrt{n}}>k)\leq P(\frac{\bar X-\mu}{\sigma/\sqrt{n}}>k)</script>且<script type="math/tex; mode=display">\frac{\bar X-\mu}{\sigma/\sqrt{n}}\sim N(0,1)</script></li>
<li>(3)构造检验统计量<script type="math/tex; mode=display">Z=\frac{\bar X-\mu\_0}{\sigma/\sqrt{n}}\sim N(0,1)</script>给定显著性水平\(\alpha\)</li>
<li>(4)按照控制第一类错误的原则，有<script type="math/tex; mode=display">P(|\frac{\bar X-\mu\_0}{\sigma/\sqrt{n}}|>k)\leq P(|\frac{\bar X-\mu}{\sigma/\sqrt{n}}|>k)=\alpha</script>由此，只要取\(k=z_{\alpha}\),可得<script type="math/tex; mode=display">P(|\frac{\bar X-\mu\_0}{\sigma/\sqrt{n}}|>z\_{\alpha})\leq \alpha</script></li>
<li>(5)拒绝域为<script type="math/tex; mode=display">W=(x\_1,...,x\_n):\frac{|\bar X-\mu\_0|}{\sigma/\sqrt{n}}>z\_{\alpha}</script>查表\(z_{\alpha}\),计算\(\frac{|\bar X-\mu_0|}{\sigma/\sqrt{n}}\),若其大于\(z_{\alpha}\),拒绝原假设，构造，接受原假设。</li>
</ul>
</li>
<li>总结：<img src="/2018/11/14/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/5.png" alt="avatar"></li>
<li><strong>\(\sigma^2=\sigma^2_0\)未知</strong>，关于均值\(\mu\)的检验(t检验)PPT8-2P15-22：</li>
<li><img src="/2018/11/14/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/6.png" alt="avatar"></li>
</ul>
<h2 id="正态总体方差的假设检验"><a href="#正态总体方差的假设检验" class="headerlink" title="正态总体方差的假设检验"></a>正态总体方差的假设检验</h2><ul>
<li>背景：设总体\(X\sim N(\mu,\sigma^2),\mu,\sigma^2\)均未知，X1,X2,…Xn为来自总体X是样本</li>
<li><img src="/2018/11/14/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/7.png" alt="avatar"></li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>概率图模型</title>
    <url>/2018/12/27/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li><p>贝叶斯网又称为<strong>信念网(belief network)</strong>，借助DAG来描述属性之间的依赖关系</p>
</li>
<li><p>一个贝叶斯网B由结构G和参数$\Theta$构成，即$B=<G,\Theta>$</G,\Theta></p>
<ul>
<li><p>贝叶斯网的结构有效表达了属性之间的条件独立性，若给定父节点集，贝叶斯网假设每个属性与它的非后裔属性独立，于是$B=<G,\Theta>$将属性$x_1,x_2,…,x_d$的联合概率分布定义为</G,\Theta></p>
<script type="math/tex; mode=display">
P_B(x_1,x_2,...,x_d)=\Pi^d_{i=1}P_B(x_i|\pi_i)=\Pi^d_{i=1}\theta_{x_i|\pi_i}</script><p><img src="/2018/12/27/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/1.jpg" alt="1"></p>
<p>例如对于上图，联合概率分布为</p>
<script type="math/tex; mode=display">
P_B(x_1,x_2,x_3,x_4,x_5)=P(x_1)P(x_2)P(x_3|x_1)P(x_4|x_1,x_2,)P(x_5|x_2)</script><p><img src="/2018/12/27/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/2.jpg" alt="2"></p>
<p>对于同父结构，给定$x_1$的取值，$x_3$和$x_4$条件独立；在V型结构，给定$x_4$的值，那么$x_1$和$x_2$必不独立，但是若$x_4$未知，则$x_1,x_2$是相互独立的；在顺序结构中，给定x的值，则y与z条件独立；</p>
<p><strong>边际独立性(marginal independence)</strong>: </p>
<script type="math/tex; mode=display">
P(x_1,x_2)=\sum_{x_4}P(x_1,x_2,x_4)=\sum_{x_4}P(x_4|x_1,x_2)P(x_1)P(x_2)=P(x_1)P(x_2)</script></li>
</ul>
</li>
</ul>
<h2 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h2><h3 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h3><ul>
<li><p>假定所关心的变量集合为Y，可观测的变量集合为O，其他变量的集合为R，“生成式”模型考虑联合分布$P(Y,R,O)$，“判别式”模型考虑调剂那份不$P(Y,R|O)$.给定一组观测变量值，推断就是要由$P(Y,R,O)$或者$P(Y,R|O)$得到条件概率分布$P(Y|O)$</p>
</li>
<li><p><strong>概率图模型</strong>是一类用图来表达变量相关关系的概率模型。大致可以分为两类：</p>
<ul>
<li><p>使用DAG表示变量之间的依赖关系，称为有向图模型或贝叶斯网</p>
</li>
<li><p>使用无向图表示变量之间的相互关系，称为无向图模型或者马尔可夫网</p>
</li>
<li><p><strong>隐马尔可夫模型</strong>(HMM)是结构最简单的动态贝叶斯网，变量可以分为两组，第一组是状态/隐变量，第二组是观测变量</p>
<p><img src="/2018/12/27/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/3.jpg" alt="3"></p>
<p>所有变量的联合概率分布为</p>
<script type="math/tex; mode=display">
P(x_1,y_1,...,x_n,y_n)=P(y_1)P(x_1|y_1)\Pi^n_{i=2}P(y_i|y_{i-1})P(x_i|y_i)</script><p><strong>参数</strong>确定：</p>
<p>状态转移概率： $A=[a_{ij}]_{N\times N}$,其中 $a_{ij}=P(y_{t+1}=s_j|y_t=s_i)$,$i,j\in[1,N]$</p>
<p>输出观测概率：$B=[b_{ij}]_{N\times M}$,其中$b_{ij} = P(x_t=o_j|y_t=s_i)$,$i\in [1,N],j\in [1,M]$</p>
<p>初始状态概率：$\pi=(\pi_1,\pi_2,…\pi_N)$,其中$\pi_i=P(y_1=s_i),i\in[1,N]$</p>
</li>
</ul>
</li>
</ul>
<h3 id="马尔可夫随机场"><a href="#马尔可夫随机场" class="headerlink" title="马尔可夫随机场"></a>马尔可夫随机场</h3><ul>
<li><p>马尔可夫随机场是典型的马尔可夫网，这是一种著名的无向图模型。图中每一个节点代表一个或者一组变量，边表示节点之间的依赖关系，马尔可夫随机场中有一组<strong>势函数</strong>，主要用于定义概率分布函数</p>
</li>
<li><p>若图中的任意两节点都有边相连接，则称该节点集为<strong>团</strong>，若在一个团中加入任何另外一个节点都不能构成团，那么此团称为<strong>极大团</strong>，在马尔可夫随机场中，多变量之间的联合概率分布能基于团分解为多个因子的乘积，每个因子仅与一个团相关，对于n和变量$x=\{x_1,x_2,…,x_n\}$,所有团构成的集合C，以及团$Q\in C$对应的变量集合为$x_{Q}$,联合概率定义为</p>
<script type="math/tex; mode=display">
P(x)=\frac{1}{Z}\Pi_{Q\in C}\psi_Q(x_Q)</script><p>其中$\psi_Q$为与团Q对应的是函数，$Z=\sum_x\Pi_{Q\in C}\psi_Q(x_Q)$为规范化因子</p>
<p>但是，若变量的个数较多，那么团的数目也会很多，计算复杂，注意到若Q不是极大团，则它一定被一个极大团$Q^<em>$包围，那么可以重新定义联合概率P,界定所有极大团集合为$C^</em>$</p>
<script type="math/tex; mode=display">
P(x)=\frac{1}{Z^*}\Pi_{Q\in C^*}\psi_Q(x_Q)</script><p>其中$Z^<em> =\sum_x\Pi_{Q\in C^</em>}\psi_Q(x_Q)$为规范化因子</p>
</li>
<li><p>若从节点集A中的节点到B中节点都必须经过C中的节点，那么C称为<strong>分离集</strong>，对于马尔可夫随机场，有<strong>全局马尔可夫性</strong>:给定两个变量子集的分离集，则这两个变量子集条件独立。也即是$x_A和x_B$在给定$x_C$的条件下独立</p>
</li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>概率图模型</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>看见_柴静</title>
    <url>/2018/11/07/%E7%9C%8B%E8%A7%81-%E6%9F%B4%E9%9D%99/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;《看见》是柴静讲述央视十年历程的自传性作品，既是柴静个人的成长告白书，在某种程度上亦可视作中国社会十年变迁的备忘录</p>
<h2 id="MARKS"><a href="#MARKS" class="headerlink" title="MARKS"></a>MARKS</h2><ul>
<li><p>白岩松有天安慰我：“人们声称的最美好的岁月其实都是最痛苦的，只是事后回忆起来的时候才那么幸福。</p>
</li>
<li><p>旁边站着一个戴赭黄头巾的维族老人，我还没来得及张口问什么，她忽然回身把我抱住，在我肩头哭了起来。我下意识地搂着她一耸一耸的肩膀，脸贴着她的脸，她的皱纹冻得冰凉。</p>
</li>
<li><p>您用什么挖的？”“当时找不到任何工具，就用自己的手挖。一开始看到一个手腕时也不能确定是我媳妇还是儿媳妇，等看到衣袖的时候我才确定是我孩子他妈。然后我就停下来了，其他人把她挖了出来。”他脸上全是灰，被泪水冲刷得深一道浅一道，翻译说到“然后我就停下来了”，我心里抽动，一时间不出下一句来。</p>
</li>
<li><p>离开的时候，我看到另一张病床上的小伙子，脖子上绑着一个痰巾，上面有一些秽迹，小腿露在被子外面，全是曲张的静脉。我们走过的时候，他连看都不看一眼。我停下来看他。他没有昏迷，眼睛是睁着的，只是什么表情也没有。日后，我在很多绝望的人脸上看过</p>
</li>
<li><p>晚上回到酒店，大家都不作声。编导天贺抽了一会儿他的大烟斗，说：“觉得么，像是《卡桑德拉大桥》里头的感觉，火车正往危险的地方开，车里的人耳边咣咣响——外面有人正把窗户钉死。”</p>
</li>
<li><p>她嗓子喑哑，听起来像是呓语，不断重复某些句子。采访差不多凌晨四点才结束，司机听得睡过去了。我不想打断她，这一年多的生活，她一直没机会说，说出来也没人信。她说：“我可以这样厚颜无耻！我都觉得自己厚颜无耻……现在想起来也还是。你可以到那条街上站在那里跟别人讨价还价。不是说卖别人，卖什么，是卖自己呀！那是跟别人讨价还价卖自己！”</p>
<p>她说在噩梦里，还会一次次回到那个地方——穿着从戒毒所被卖出来时的那条睡裙，天马上就要黑了，她就要开始站在那条街上，等着出卖自己。“你戒毒所是挽救人，还是毁灭人？</p>
</li>
<li><p>海子有句诗，深得我心：“天空一无所有，为何给我安慰。”</p>
</li>
<li><p>阿甘是看见了什么，就走过去。别的人，是看见一个目标，先订一个作战计划，然后匍匐前进，往左闪，往右躲，再弄个掩体……一辈子就看他闪转腾挪活得那叫一个花哨，最后哪儿也没到达。”</p>
</li>
<li><p>我采访陈丹青时，这位知名的画家从清华辞去了美术学院教授和博导的职务，因为现行的政治和英语考试，让他招不到他想要的学生。他说：“政治本来是一门学问，但我们的政治考试是反政治的，没有人尊敬这个学科。”</p>
</li>
<li><p>《我只是讨厌屈服》。”我说。他带点惊奇地看了我一眼，说哎对，过了一会儿，说：“在那篇文章里，那个律师说了一句话，他说权利不用来伸张的话，就只是一张纸。”这个人相信了这些写在纸上的话，然后穿着蓝白相间竖条纹的狱服，满脸胡须，坐这里看着我。他进监狱后，厂子倒了，离了婚，监狱离他的家两千里，没人给他送生活费，村里的人去看他，拾破烂的老人给了他五十块钱，老汉戴着塌得稀软的蓝布帽子，对我说：“把他换出来，把我关进去吧，我老了。”</p>
</li>
<li><p>说话，真不容易呢，我们绝大部分人都是普通人。却希望其他人都能做个公民，这样才会有人帮我们争取更多的利益、权利……”</p>
</li>
<li><p>痛苦是财富，这话是扯淡。姑娘，痛苦就是痛苦，”他说，“对痛苦的思考才是财富。”</p>
</li>
<li><p>老范坐边上，后来她写道：“说实话，他的坦率让我绝望。一个过于主动甚至积极坦白自己内心阴暗面的人，往往会让原本想去挖掘他内心弱点的人感到尴尬和一丝不安。</p>
</li>
<li><p>心理医生说有的人为什么要拼命吃东西，因为要抑制自己表达不出来的欲望。她拧过脸看着我，很专心地听。</p>
</li>
<li><p>希望和失望也绝不能是善。因为恐惧是一种痛苦，希望不能脱离恐惧而存在，所以希望和失望都表示知识的缺乏，和心灵的软弱无力。</p>
</li>
<li><p>老王没有推特写，只是伴随，她隔着栏杆，向场地中嗒然若失的埃蒙斯伸出手去，埃蒙斯将头抵在栏杆上，她俯身下去隔着栅栏揽住他，一只手护持着丈夫的脖颈，另一只手摩挲他的眉毛，像在安抚委屈孩子时的温存。音乐与现场的人声交替出现：“When you try your best but you don’t succeed，When you get what you want but not whatyou need， When you feel so tried but you can’t sleep……”老范跟我说过她为什么用这歌，她说生活到了真的艰难处才能体会，“只有最亲的人才能了解和陪伴你的伤痛”。</p>
</li>
<li><p>我说：“那个刷头皮的小男孩的细节之所以让人记了很多年，那个医生对他的情感之所以显得那么真实，是因为小男孩承受了极大的痛苦，是因为他的坚忍。西藏人有句话说，幸福是刀口舔蜜。唐山首先是个刀口，如果刀口本身的锋利和痛感感觉不到，后来的蜜汁你吮吸起来也会觉得少了滋味。”</p>
</li>
<li><p>他写了句奇怪的话——“这个世界上有很多极端认真的蠢人。”红灯又闪一下，补了一句：“当然不是指你。”我说指什么。他说：“比如一个母亲，孩子生病，她天天祈祷，但是还是去世了，这不是愚昧么？”我说：“这是爱。”他说：“爱和善是能力，而不是情感。</p>
</li>
<li><p>我俩到处找人打听求助，碰到肯帮忙的人，明白为什么有个成语叫“感激涕零”。我那阵子什么也干不了，问一个明友：“你出事的时候是什么感觉？”“一块石头落了地。”他指内心的恐惧终于到了。“如果是你亲近的人出了事呢？”“那是一块石头砸在心里。”</p>
</li>
<li><p>你对现实完全没有愤怒？”“没有。”“你知道还会有一种危险是，当我们彻底地理解了现实的合理性，很多人就放弃了。”这是我的困惑。“那可能还是因为想到自己要改变，所以没办法了，碰到障碍了，就放弃了。我也改变不了，但也不用改变，它还是会变。”“那我们做什么呢？”“把自己的事情做好。”</p>
</li>
<li><p>我最害怕的是崇拜者，因为崇拜基于的往往是幻想上崇拜，最终的结果也只能是失望。</p>
</li>
<li><p>“不光是简单，不光是家长，不管任何人，你去告诉别人应该怎么样，这就是错的方式。我就错了这么多年。”</p>
</li>
<li><p>我当法官时，常认真地履行我的职责，实际上我也是如此做的。但在我内心深处，潜伏着这么一种意识：我只是在人生的舞台上扮演着一个法官的角色。每当我判一个人死刑，都秘密地向他的灵魂祈求，要他原谅我这么做，我判他的刑只因为这是我的角色，而非因为这是我的意愿。我觉得像彼拉多一样，并且希望洗干净我的手，免得沾上人的血，尽管他也许有罪。唯有完人才够资格向罪人扔石头，但是，完人是没有的。”在这段话边上，学生时代的何帆给的批注是：“伪善。”如今，他拿出笔，划去那两个字，在旁边写上：“人性。”</p>
</li>
<li><p>“一个人得被自己的弱点绑架多少次啊，悲催的是这些弱点怎么也改不掉。但这几年来，身边的人待我，就像陈升歌里唱的，‘因为你对我的温柔，所以我懂得对别人好’，能起码认识到什么不好，最重要的，是能以‘别人可能是对的’为前提来思考一些问题。”年底开会的时候，我向组里道歉：“不好意思啊平常太暴躁啦。”大家笑，好好，原谅你。我又不干了：“哟，我就这么一说，你们真敢接受啊，谁敢说我暴躁我看看。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>read</tag>
      </tags>
  </entry>
  <entry>
    <title>模型评估与选择</title>
    <url>/2018/11/13/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/</url>
    <content><![CDATA[<h2 id="本章基本概念"><a href="#本章基本概念" class="headerlink" title="本章基本概念"></a>本章基本概念</h2><ul>
<li><strong>错误率(error rate)</strong>: 在m个样本中假设有a个样本分类错误，则错误率$E = a/m$,相应地，$1-a/m$称为<strong>精度(accuracy)</strong></li>
<li>实际预测输出与样本真实输出的误差称为“误差”(error),在训练集上的误差称为<strong>训练(training)误差\经验(empirical)误差</strong>，在新样本上的误差称为<strong>泛化误差</strong></li>
<li><strong>过拟合overfitting</strong>：泛化能力很差；<strong>欠拟合(underfitting)</strong>:对训练样本的一般性质尚未学习好。</li>
</ul>
<h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><ul>
<li>注意测试集和训练集的数据分布应该尽量保持一致，一般采用分层采样(stratified sampling)</li>
</ul>
<h3 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h3><ul>
<li>将数据集D拆分为两个互斥的集合，其中一个集合为训练集S，另一个为测试集T，$D = S\cup T,S\cap T =\emptyset$</li>
<li>一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果</li>
</ul>
<h3 id="交叉验证法-K折交叉验证"><a href="#交叉验证法-K折交叉验证" class="headerlink" title="交叉验证法/K折交叉验证"></a>交叉验证法/K折交叉验证</h3><ul>
<li>将数据集划分为k个大小相似的互斥子集，即$D = D_1\cup D_2\cup … \cup D_k,D_i\cap D_j =\emptyset(i\neq j)$,注意每个子集$D_i$都尽可能保持数据分布的一致性</li>
<li>与留出法相似，通常要随机使用不同 划分重复p次，最终的结果是这p次k折交叉验证结果的均值。</li>
</ul>
<h3 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h3><ul>
<li>对于样本集D，采用<strong>有放回采样</strong>：每次随机取一个，然后放回，直到构成一个新的样本集$D’$,样本在m次采样中始终不被采集到的概率为$(1-\frac{1}{m})^m = \frac{1}{e}\approx 0.368$,故而约有36.8%的样本未出现在D’中，用$D/D’$用作测试集</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>自助法在数据集较小，难以有效划分训练/测试集时很有用，而且能够从初始集中产生多个不同的训练集，对于集成学习等方法有很大的好处；但是他改变了初始数据集的分布，会引入<strong>估计偏差(estimation error)</strong>，因此在数据量组都的情况下，推荐使用留出法和交叉验证法。</li>
</ul>
<h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><h3 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h3><ul>
<li><strong>回归</strong>任务最常用的性能度量就是<strong>“均方误差(mean squared error)”</strong><script type="math/tex; mode=display">
E(f;D)=\frac{1}{m}\sum^m_{i=1}(f(x_i)-y_i)^2</script>更一般地，对于数据分布D和概率密度函数p(.),均方误差可描述为<script type="math/tex; mode=display">
E(f;D) = \int_{x\sim D}(f(x)-y)^2p(x)dx</script></li>
</ul>
<h3 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h3><ul>
<li><p>对于样例集D，</p>
<p><strong>错误率</strong>定义为</p>
<script type="math/tex; mode=display">
E(f;D)=\frac{1}{m}\sum^m_{i=1}\|(f(x_i)\neq y_i)</script><p><strong>精度</strong>定义为</p>
<script type="math/tex; mode=display">
acc(f;D) = \frac{1}{m}\sum^m_{i=1}\|(f(x_i) = y_i)</script><p>更一般地，对于数据分布D和概率密度函数p(.)，错误率和精度可分别描述为</p>
<script type="math/tex; mode=display">
E(f;D)=\int_{x\sim D}\|(f(x)\neq y)p(x)dx\\
acc(f;D) = \int_{x\sim D}\|(f(x) = y)p(x)dx = 1-E(f;D)</script></li>
</ul>
<ul>
<li><strong>混淆矩阵(confusion matrix)</strong>:</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>真实情况</th>
<th>预测结果</th>
<th>预测结果</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>正例</td>
<td>反例</td>
</tr>
<tr>
<td>正例</td>
<td>TP(真正例)</td>
<td>FN(假反例)</td>
</tr>
<tr>
<td>反例</td>
<td>FP(假正例)</td>
<td>TN(真反例)</td>
</tr>
</tbody>
</table>
</div>
<p>  <strong>查准率P</strong>: $P = \frac{TP}{TP+FP}$</p>
<p>  <strong>查全率R</strong>：$R = \frac{TP}{TP+FN}$</p>
<p>  查准率和查全率是相互矛盾的量，从而有P-R曲线，若一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者；对于互相交叉的，可以看<strong>平衡点</strong>，即查准率等于查全率的点，哪个比较大就认为哪个就好一点<br>  <img src="/2018/11/13/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/5.png" alt="5"><br>  在一些应用中，查准率和查全率的重视程度不同，可用<strong>F1</strong>度量来进行评价：</p>
<script type="math/tex; mode=display">
  F_{\beta} = \frac{(1+\beta^2)\times P\times R}{(\beta^2\times P)+R}</script><p>  如果是在n个二分类混淆矩阵上综合考察查准率和查全率，有两种方法</p>
<ul>
<li>在各混淆矩阵上分别计算出查准率和查全率，再计算平均自，得到<strong>宏查准率</strong>，<strong>宏查全率</strong>以及相应的<strong>宏F1</strong></li>
<li>现将各混淆矩阵的对应元素进行平均，得到TP,FP,TN,FN的平均值，而后计算出<strong>微查准率</strong>，<strong>微查全率</strong>和<strong>微F1</strong></li>
</ul>
<ul>
<li><p><strong>ROC 与 AUC</strong></p>
<p>ROC(受试者工作特征)，横纵坐标分别为：</p>
<p>假正例率：$FPR = \frac{FP}{TN+FP}$</p>
<p>真正例率：$TPR = \frac{TP}{TP+FN}$</p>
<ul>
<li>若一个学习器的<strong>ROC</strong>曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者；如果交叉，则一般用<strong>AUC</strong>(ROC曲线下的面积)来衡量<script type="math/tex; mode=display">
AUC = \frac{1}{2}\sum^{m-1}_{i=1}(x_{i+1}-x_i).(y_i+y_{i+1})</script>给定$m^+$个正实例和$m^-$个反例，令$D^+$和$D^-$分别表示正反例集合，则排序<strong>损失(loss)</strong>定义为：<script type="math/tex; mode=display">
l_{rank} = \frac{1}{m^+m^-}\sum_{x^+\in D^+}\sum_{x^-\in D^-}(\|(f(x^+)<f(x^-))+\frac{1}{2}\|(f(x^+)=f(x^-)))</script>即，若正例的预测值小于反例，则记一个罚分，若相等，则记0.5个罚分<script type="math/tex; mode=display">
AUC = 1-L_{rank}</script></li>
</ul>
</li>
<li><p><strong>代价敏感错误率与代价曲线</strong></p>
<p>不同的错误造成的代价可能是不同的，此时要赋予其权重。<br><img src="/2018/11/13/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/6.png" alt="6"><br>一般$cost_{ii} = 0$,若将第0类别判别为比第1类所造成的损失更大，则$cost_{01}&gt;cost_{10}$,损失程度相差越大，两个代价矩阵的差值越大。此时对应的代价敏感错误率为</p>
<script type="math/tex; mode=display">
E(f;D;cost)=\frac{1}{m}(\sum_{x_i\in D^+}\|(f(x_i)\neq y_i)\times cost_{01}+\sum_{x_i\in D^-}\|(f(x_i)\neq y_i)\times cost_{10})</script><p>相应的，此时ROC曲线不能直接反应出学习期的期望总体代价，而可以通过<strong>代价曲线（cost curve）</strong>来达到该目的：</p>
<ul>
<li><p>横轴为取值为[0,1]的正例概率代价：</p>
<script type="math/tex; mode=display">
P(+)cost = \frac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}</script><p>其中p为样例为正例的概率</p>
</li>
<li><p>纵轴为取值为[0,1]的归一化代价</p>
<script type="math/tex; mode=display">
cost_{norm} = \frac{FNR\times p\times cost_{01}+FPR\times(1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}</script><p>$FNR = 1-TPR$是假反例率，ROC曲线上的没一点对应了代价平面上的一条线段。</p>
<p><img src="/2018/11/13/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/7.png" alt="7"></p>
</li>
</ul>
</li>
</ul>
<h2 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h2><p>此部分内容见整理的数理统计内容</p>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>模型评估与选择</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>神经了的ODE: Neural CDE</title>
    <url>/2020/09/23/%E7%A5%9E%E7%BB%8F%E4%BA%86%E7%9A%84ODE-Neural-CDE/</url>
    <content><![CDATA[<h3 id="Neural-Controlled-Differential-Equations-for-Irregular-Time-Series"><a href="#Neural-Controlled-Differential-Equations-for-Irregular-Time-Series" class="headerlink" title="Neural Controlled Differential Equations for Irregular Time Series"></a>Neural Controlled Differential Equations for Irregular Time Series</h3><blockquote>
<p>Neural ODE的缺点是一旦初值确定，轨迹便确定，中间无法对轨迹进行修正，本文引入受控微分方程概念，使得后续拿到的数据得到进一步利用。<a href="http://arxiv.org/abs/2005.08926">ArXiv</a>, <a href="https://github.com/patrick-kidger/NeuralCDE">code</a>. <span id="more"></span></p>
</blockquote>
<ul>
<li><p>假设$\tau, T\in R$, 且$\tau &lt;T$, $v,\omega$为正整数， $X:[\tau, T]\rightarrow R^v$为一个有界连续函数(即X是满足Lipschitz性质的)。$f:R^\omega \rightarrow R^{\omega\times v}$是连续映射函数，$\zeta\in R^\omega$, 且有连续映射 $z:[\tau, T]\rightarrow R^{\omega\times v}$, 并定义</p>
<script type="math/tex; mode=display">
z_t = z_\tau + \int^t_{\tau} f(z_s)dX_s~~~\text{for}~~t\in (\tau, T]</script><p>其中 $X_s\in R^v, f(Z_s)\in R^{\omega \times v}$, 上式被称作 $\text{Controlled differential equation}$.</p>
</li>
<li><p>与一般基于ODE的时序预测方法不同，本文在考虑后续状态的同时，可以保证隐藏状态z是连续变化的。</p>
</li>
<li><p>对(1)式进行扩展，可以得到$\text{Neural Controlled Differenrial Equations}$的定义为：</p>
<script type="math/tex; mode=display">
z_t = z_{t_0} + \int^t_{t_0} f_{\theta}(z_s)dX_s~~~\text{for}~~t\in (t_0, t_n]</script><p>其中$z_{t_0} = \zeta_{\theta}(x_0, t_0)$</p>
<ul>
<li><strong>本文中$X:[\tau, T]\rightarrow R^v$是通过自然边界下三次样条插值法来确定的</strong></li>
<li><strong>$f_{\theta}:R^\omega \rightarrow R^{\omega\times (v+1)}$表示任意神经网络</strong>，<strong>$w$是超参数，表示隐藏状态的维度</strong></li>
<li><strong>$\zeta_{\theta}: R^{v+1}\rightarrow R^{\omega}$表示任意依赖于参数$\theta$的神经网络</strong></li>
</ul>
</li>
<li><p>本文将 $\text{Controlled differential equation}$转化为普通的ODE方程，从而能够通过$\text{Neural ODE}$中的方法进行求解，假设</p>
<script type="math/tex; mode=display">
g_{\theta, X}(z,s) = f_{\theta}\frac{dX}{ds}(s)</script><p>则很容易得到</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
z_t &= z_{t_0} + \int^t_{t_0} f_{\theta}(z_s)dX_s \\
&= z_{t_0} + \int^t_{t_0}f_{\theta}(z_s)\frac{dX}{ds}(s)ds\\
&= z_{t_0} + \int^t_{t_0} g_{\theta,X}(z_s, s)ds.
\end{aligned}
\end{equation}</script></li>
</ul>
<p><img src="/2020/09/23/%E7%A5%9E%E7%BB%8F%E4%BA%86%E7%9A%84ODE-Neural-CDE/1.png" alt></p>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>Neural ODE</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>线性模型</title>
    <url>/2018/11/17/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><script type="math/tex; mode=display">
y = \omega^T x+b</script><p>而线性回归的目标为试图得到</p>
<script type="math/tex; mode=display">
f(x_i)=\omega x_i +b,使得 f(x_i)\simeq y_i</script><p>对于多元线性回归，可以采用<strong>最小二乘法</strong>，其中（把b加入$\omega$中去）</p>
<script type="math/tex; mode=display">
X = \begin{bmatrix}
x_{11}&x_{12}&\cdots&x_{1d}&1\\
x_{21}&x_{22}&\cdots&x_{2d}&1\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
x_{m1}&x_{m2}&\cdots&x_{md}&1
\end{bmatrix}=\begin{bmatrix}
x_1^T&1\\
x_2^T&1\\
\vdots&\vdots\\
x_m^T&1
\end{bmatrix}\\
\\
y = (y_1;y_2;\cdots;y_m)</script><p>那么对于性能指标$E=(y-X\hat\omega)^T(y-X\hat\omega)$来说，</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial\hat \omega} = 2X^T(X\hat \omega-y)</script><script type="math/tex; mode=display">
\hat \omega^* = (X^TX)^{-1}X^Ty</script><p>故而此时的线性回归模型为</p>
<script type="math/tex; mode=display">
f(\hat x_i^T)=\hat x_i^T(X^TX)^{-1}X^Ty</script><ul>
<li>问题是有时候$X^TX$并不是满秩的，此时系统有多个解，如何选择将由学习算法的归纳偏好来决定，常见做法是引入<strong>正则化(generalization)项</strong></li>
</ul>
<h3 id="广义的线性回归"><a href="#广义的线性回归" class="headerlink" title="广义的线性回归"></a>广义的线性回归</h3><ul>
<li>对数线性回归形式<script type="math/tex; mode=display">
lny = \omega^Tx+b</script>对于更一般的，可考虑<script type="math/tex; mode=display">
y=g^{-1}(\omega^Tx+b)</script></li>
</ul>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><ul>
<li><p>形式：</p>
<script type="math/tex; mode=display">
\begin{align}
y = \frac{1}{1+e^{-(\omega^2x+b)}}
\end{align}</script><p>故而，</p>
<script type="math/tex; mode=display">
ln(\frac{y}{1-y})=\omega^Tx+b</script><p>其中,$ln(\frac{y}{1-y})$称为<strong>对数几率</strong></p>
<p>对于二分类问题，上式还等价于</p>
<script type="math/tex; mode=display">
ln\frac{p(y=1|x)}{p(y=0|x)}=\omega^Tx+b\\
p(y=1|x)=\frac{e^{\omega^Tx+b}}{1+e^{\omega^Tx+b}}\\
p(y=0|x)=\frac{1}{1+e^{\omega^Tx+b}}</script><p>可以通过<strong>极大似然法</strong>来估计$\omega$和b，对于给定数据集，他的似然函数为</p>
<script type="math/tex; mode=display">
l(\omega,b)=\sum_{i=1}^mlnp(y_i|x_i;\omega,b)</script><p>而</p>
<script type="math/tex; mode=display">
p(y_i|x_i;\omega,b)=y_ip_1(\hat x_i;\beta)+(1-y_i)p_0(\hat x_i;\beta)</script><p>最大化$l(\omega,b)$相当于最小化下式，<a href="https://blog.csdn.net/VictoriaW/article/details/77947535">推导过程</a></p>
<script type="math/tex; mode=display">
l(\beta)=\sum^{n}_{i=1}(-y_i\beta^T\hat x_i+ln(1+e^{\beta^T\hat x_i}))</script><p>上式是一个关于$\beta$的高阶可导连续凸函数，可用牛顿法求解，</p>
<script type="math/tex; mode=display">
\beta^{t+1}=\beta^t-(\frac{\partial^2l(\beta)}{\partial\beta\partial \beta^T})^{-1}\frac{\partial l(\beta)}{\partial \beta}</script></li>
</ul>
<h3 id="线性-Fisher判别分析-Linear-Discriminant-Analysis"><a href="#线性-Fisher判别分析-Linear-Discriminant-Analysis" class="headerlink" title="线性/Fisher判别分析(Linear Discriminant Analysis)"></a>线性/Fisher判别分析(Linear Discriminant Analysis)</h3><ul>
<li><p>主要思想：对给定训练集，设法将所有样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离；下面用$X_i,\mu_i,\Sigma_i$分别表示第$i\in\{0,1\}$类示例的集合，均值向量，协方差矩阵</p>
<p>则样本中心在直线上的投影为$\omega^T\mu_i$,若将样本投影到直线上，样本的协方差为$\omega^T\Sigma_i\omega$</p>
<p>那么，为了让同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega$尽可能小；而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$\parallel \omega^T\mu_0-\omega^T\mu_1\parallel_2^2$尽可能大，故而将目标函数设为</p>
<script type="math/tex; mode=display">
J=\frac{\parallel \omega^T\mu_0-\omega^T\mu_1\parallel_2^2}{\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega}</script><p>定义<strong>类内散度矩阵</strong>为</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
S_\omega &=&\Sigma_0+\Sigma_1\\
&=&\sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T
\end{aligned}
\end{equation}</script><p><strong>类间散度矩阵</strong></p>
<script type="math/tex; mode=display">
S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T</script><p>那么，<strong>目标函数</strong>可以重写为$J=\frac{\omega^TS_b\omega}{\omega^TS_\omega\omega}$,称之为<strong>广义瑞利商</strong>,因为分子分母均为$\omega$的二次项，因此目标函数的解与$\omega$的长度无关，只与其方向有关，不失一般性，令分母为1</p>
<p>最终，问题变成了</p>
<script type="math/tex; mode=display">
\min_{\omega}~~~-\omega^TS_b\omega\\
s.t.~~~\omega^TS_{\omega}\omega=1</script><p>用<strong>拉格朗日乘子法</strong>可得</p>
<script type="math/tex; mode=display">
S_b\omega=\lambda S_{\omega}\omega</script><p>注意到$S_b\omega$的方向恒为$\mu_0-\mu_1$，不妨令</p>
<script type="math/tex; mode=display">
S_b\omega=\lambda(\mu_0-\mu_1)</script><p>故而，有</p>
<script type="math/tex; mode=display">
\omega=\S^{-1}_{\omega}(\mu_0-\mu_1)</script><p><strong>对于多分类任务来说</strong>,可以对其进一步拓展。</p>
</li>
</ul>
<h3 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h3><ul>
<li><p>一般是基于一些基本策略，利用二分类学习器来解决多分类问题。</p>
</li>
<li><p><strong>拆分法</strong>:</p>
<p>给定数据集$D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\},y_i\in\{C_1,C_2,…,C_N\}​$</p>
<ul>
<li>一对一(OvO):将N个类别两两配对，产生$\frac{N(N-1)}{2}​$个二分任务，最终测试阶段，新样本同时提交给所有分类器，通过投票来选择最终分类结果</li>
<li>一对其余(OvR):每次将一个类的样例作为正例，所有其他类的样例作为反例来训练N个分类器。在测试时若有一个分类器预测为正类，则对应类别标记作为最终分类结果；若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为预测结果</li>
<li>多对多(MvM):每次将若干个类作为正类，若干个其他类作为反类，一般用<strong>纠错输出码(Error Correction Output Codes)</strong>来进行校正<ul>
<li>编码：对N个类别进行M次划分，每次划分将一部分类划为正类，一部分划为反类，从而形成一个二分类训练集，这样一共产生M个训练集，可训练处M个分类器。</li>
<li>解码：M个分类器分别对测试样本进行测试，这些测试的标记组成一个编码，将此编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><ul>
<li>有时候训练集中正反例的数目差距比较大，而前面的推导都是基于两者相同产生的，即<script type="math/tex; mode=display">
\frac{y}{1-y}>1,则为正例</script>现在应该改为,($m^+$,$m^-$分别为正负实例的个数)<script type="math/tex; mode=display">
\frac{y}{1-y}>\frac{m^+}{m^-}</script>即应该让<script type="math/tex; mode=display">
\frac{y'}{1-y'}=\frac{y}{1-y}\times\frac{m^-}{m^+}</script></li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>线性模型</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>进化算法库</title>
    <url>/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/</url>
    <content><![CDATA[<h2 id="NSGA族"><a href="#NSGA族" class="headerlink" title="NSGA族"></a>NSGA族</h2><h3 id="NSGA的缺陷："><a href="#NSGA的缺陷：" class="headerlink" title="NSGA的缺陷："></a>NSGA的缺陷：</h3><p>(1)非支配排序的时间复杂度很大，为$O(MN^3)$,其中M为目标函数的数量，N为种群规模。</p>
<p>(2)不支持精英策略。精英策略在保持好的个体及加速向Pareto前沿收敛方面有很好的表现</p>
<p>(3)需要自己指定共享参数，该参数对种群的多样性产生很大的影响</p>
<h3 id="NSGA-II"><a href="#NSGA-II" class="headerlink" title="NSGA-II"></a>NSGA-II</h3><ul>
<li><p>NSGA一II算法的基本思想为:首先,随机产生规模为N的初始种群,非支配排序后通过遗传算法的选择、交叉、变异三个基本操作得到第一代子代种群;其次,从第二代开始,将父代种群与子代种群合并,进行快速非支配排序,同时对每个非支配层中的个体进行拥挤度计算,根据非支配关系以及个体的拥挤度选取合适的个体组成新的父代种群;最后,通过遗传算法的基本操作产生新的子代种群:依此类推,直到满足程序结束的条件。</p>
</li>
<li><p>主要贡献</p>
<ul>
<li><p>(1)快速非支配排序：</p>
<p><img src="/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/1.png" alt="1"></p>
</li>
<li><p>(2)保留多样性：提出<strong>拥挤比较法</strong>替换<strong>共享函数法</strong>，$I[i].m$为I集合中第i个个体第m个目标函数的值。</p>
<p><img src="/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/2.png" alt="2"></p>
<p><img src="/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/3.png" alt="3"></p>
</li>
</ul>
</li>
</ul>
<h3 id="主程序"><a href="#主程序" class="headerlink" title="#### 主程序"></a>#### 主程序</h3><p><img src="/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/4.png" alt="4"></p>
<h2 id="SPEA算法族"><a href="#SPEA算法族" class="headerlink" title="SPEA算法族"></a>SPEA算法族</h2><h3 id="SPEA-O-N-3"><a href="#SPEA-O-N-3" class="headerlink" title="SPEA (O($N^3$))"></a>SPEA (O($N^3$))</h3><p><a href="https://blog.csdn.net/u014119694/article/details/77412740">SPEA参考自博客</a></p>
<p><img src="/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/5.png" alt="5"></p>
<p>其中适应度的分配方式以及聚类算法等请看参考博客</p>
<h3 id="SPEA-II"><a href="#SPEA-II" class="headerlink" title="SPEA-II"></a>SPEA-II</h3><ul>
<li>SPEA的缺点<ul>
<li>(1)Fitness Assignment:当P’成员只有一个时，P中所有成员的适应度是相同的。此时SPEA会退化为随机搜索算法</li>
<li>(2)Density Estimation: 群落分布太稀疏，以至于许多成员之间不存在相互支配关系。若能添加密度信息，那么就能更有效的搜索非支配成员。Clustering仅对P’有效，而对P没有影响。</li>
<li>(3)Archive Truncation:clustering算法会删减P’中的成员，这其中也有可能包含了外部支配解，造成了信息截断，不利于非支配解的扩散。</li>
</ul>
</li>
</ul>
<p><img src="/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/6.png" alt="6"></p>
<p><img src="/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/7.png" alt="7"></p>
<p><img src="/2018/11/18/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%BA%93/8.png" alt="8"></p>
<h2 id="DMOEA-epsilon-C"><a href="#DMOEA-epsilon-C" class="headerlink" title="DMOEA-$\epsilon C$"></a>DMOEA-$\epsilon C$</h2><ul>
<li>主要创新点：<ul>
<li>(1)explicitly decomposes an MOP into a series of scalar constrained optimization subproblems by selecting one of the objectives as the main objective function and associating each subproblem with an upper bound vector.  </li>
<li>(2)a main objective alternation strategy is proposed  </li>
<li>(3)a solution-to-subproblem matching procedure is designed to place the nearest solution to each subproblem and is utilized after the main objective alternation strategy </li>
<li>(4)subproblem-to-solution matching procedure is proposed to find a subproblem with the minimum constraint violation value for a new generated solution. </li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Cloud Workflow Scheduling</category>
      </categories>
      <tags>
        <tag>Cloud Workflow Scheduling</tag>
      </tags>
  </entry>
  <entry>
    <title>那些踩过的python坑</title>
    <url>/2018/12/03/%E9%82%A3%E4%BA%9B%E8%B8%A9%E8%BF%87%E7%9A%84python%E5%9D%91/</url>
    <content><![CDATA[<h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="列表复制和删除"><a href="#列表复制和删除" class="headerlink" title="列表复制和删除"></a>列表复制和删除</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">b = a</span><br><span class="line">b.remove(<span class="number">3</span>)</span><br><span class="line">a</span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="comment"># 解决办法是调用copy函数，然后再进行删除的时候就不会这样了</span></span><br><span class="line">b = a.copy()</span><br></pre></td></tr></table></figure>
<h3 id="List-Set-Dict-Tuple的区别和用法"><a href="#List-Set-Dict-Tuple的区别和用法" class="headerlink" title="List Set Dict Tuple的区别和用法"></a>List Set Dict Tuple的区别和用法</h3><ul>
<li><p>List: 不要求元素类型一样，是有序的,使用[]</p>
<ul>
<li>常用方法：append(value)，insert(pos,value)，pop(index)</li>
</ul>
</li>
<li><p>Tuple: 可以看成一种不变（不可变指的是指向的位置不可变）的”List”,是用()，也是通过下标访问，元素是固定的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = (<span class="number">3.14</span>, <span class="string">&#x27;China&#x27;</span>, <span class="string">&#x27;Jack&#x27;</span>, [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>])</span><br><span class="line">L = t[<span class="number">3</span>]</span><br><span class="line">L[<span class="number">0</span>] = <span class="number">122</span></span><br><span class="line"><span class="built_in">print</span>(t)</span><br><span class="line">(<span class="number">3.14</span>, <span class="string">&#x27;China&#x27;</span>, <span class="string">&#x27;Jack&#x27;</span>, [<span class="number">122</span>, <span class="string">&#x27;b&#x27;</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dict：活字典，可以添加键值对，是无序的，key不可变，value可变，查找速度快</p>
</li>
<li><p>set：内容元素不重复</p>
<ul>
<li>常用方法：add(value)，[已经有不会报错，但不会加进去],remove(value),[没有会报错]</li>
<li>交集：a.intersection(b); 并集： a.union(b); 差集： a.differecnce(b)</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>language</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>高维度数据的聚类</title>
    <url>/2018/12/03/%E9%AB%98%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB/</url>
    <content><![CDATA[<h1 id="高维度数据的聚类"><a href="#高维度数据的聚类" class="headerlink" title="高维度数据的聚类"></a>高维度数据的聚类</h1><p>高维度数据的聚类算法就是想在原来特征空间的任意方向的子空间上找到聚类</p>
<ul>
<li><p>实际上降维度在高维空间很多时候并不合适</p>
<p><img src="/2018/12/03/%E9%AB%98%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB/1.png" alt="1"></p>
<p>只考虑平行于坐标轴上的子空间的数量也是<strong>指数级</strong>的，为$2^d-1$种</p>
</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="基本分类："><a href="#基本分类：" class="headerlink" title="基本分类："></a>基本分类：</h3><ul>
<li>投影聚类法和子空间聚类法是想找到平行于axis的子空间来进行聚类</li>
<li>correlation clustering algorithm:是针对于类别是在任意方向上的子空间的</li>
<li>pattern-based聚类算法：有些是限制在平行于axis上，有些是对于任意方向上的子空间的特殊情况，比较杂</li>
</ul>
<h3 id="基于问题的分类"><a href="#基于问题的分类" class="headerlink" title="基于问题的分类"></a>基于问题的分类</h3><ul>
<li>Projected clustering algorithms:<ul>
<li>为每一个点找到一个准确的子空间聚类</li>
</ul>
</li>
<li>“Soft” Projected clustering algorithm<ul>
<li>不同的属性有不同的权重，所有属性构成了一个类，通常一个子空间不会“强硬”地分配给一个类</li>
</ul>
</li>
<li>Subspace clustering algorithm<ul>
<li>找到可以明确类别的所有子空间</li>
</ul>
</li>
<li>Hybrid Algorithm<ul>
<li>找到可能交叠的类，不要求找到子空间上的所有聚类，</li>
</ul>
</li>
</ul>
<h3 id="基于算法的分类："><a href="#基于算法的分类：" class="headerlink" title="基于算法的分类："></a>基于算法的分类：</h3><ul>
<li>自上而下的方法<ul>
<li>通过确定给定点集的属性子集，确定从全维空间开始的集群的子空间，使得当投影到相应的子空间时，这些点满足给定的集群标准</li>
<li><strong>局限性</strong>：为了确定一个类对应的子空间，必须知道一些聚类的成员；为了确定聚类的成员，每个类对应的子空间必须知道，这是一个<strong>循环的依赖关系</strong></li>
<li>方法：有一个<strong>条件很高的假设</strong>，一个类对应的子空间可以通过这个类或者类成员的邻域来确定</li>
</ul>
</li>
<li>从底向上的方法：<ul>
<li>类似于找频繁项集的方法，如Apriori算法</li>
<li><strong>注意</strong>:有些从底向上的算法不用Spriori算法来进行搜索，而是用一些启发式的算法</li>
</ul>
</li>
</ul>
<h2 id="现有算法的介绍和分类"><a href="#现有算法的介绍和分类" class="headerlink" title="现有算法的介绍和分类"></a>现有算法的介绍和分类</h2><h3 id="Projeceted-Clustering-Algorithm"><a href="#Projeceted-Clustering-Algorithm" class="headerlink" title="Projeceted Clustering Algorithm"></a>Projeceted Clustering Algorithm</h3><ul>
<li><strong>PROCLUS</strong>[Aggarwal et al. 1999]:分为三个阶段：初始化阶段，迭代阶段，类别改进阶段<a href="https://yantijin.github.io/2018/11/28/A-Fast-Algorithm-for-Projected-Clustering/">参考博客</a></li>
<li><strong>FINDIT</strong>[Woo et al. 2004]:采用了额外的启发式策略来提高效率和聚类的准确性</li>
<li><strong>SSPC</strong>[Yip et al. 2005]: 通过以标记对象和/或标记属性的形式使用领域知识，提供进一步提高准确性的能力。</li>
<li><strong>DBSCAN</strong>[Ester et al. 1996]: 基于密度的聚类方法</li>
<li><strong>PreDeCon</strong>[Bohm et al. 2004]: 基于DBSCAN算法提出了一种特出的距离方法来获取每个类别的子空间，称为<strong>subspace preference</strong>,计算子空间偏好度(subspace preference),表示p点聚类最好时能够达到的最大维度的子空间,其中一个维度称为与子空间偏好度相关,若这些点在p点很近的区域内的方差在一个阈值之下.计算距离则是带有权重欧氏距离,和子空间偏好度有关的维度的权重远大于1，其他的维度的权重为1。<strong>缺点</strong>是一些输入的参数很难猜</li>
<li><strong>CLTree</strong>[Liu et al. 2000]: 基本思想是把现有的所有点标记为一类，然后加入在数据空间均匀分布的一些点作为一个不同的类，然后训练决策树来区分，<strong>缺点</strong>： 根据信息增益确定属性分类计算量太大，叠加的人工数据对于结果有很大影响</li>
</ul>
<h3 id="“Soft”-Projected-Clustering-Algorithm"><a href="#“Soft”-Projected-Clustering-Algorithm" class="headerlink" title="“Soft”-Projected Clustering Algorithm"></a>“Soft”-Projected Clustering Algorithm</h3><p>目前都采用一个权重的框架（所有属性的权重都大于0），这些研究主要和数据分析以及机器学习系那个管，目的是为了在聚类的过程中能够保持对一个目标函数的优化的可能性</p>
<ul>
<li><strong>LAC(Locally Adaptive Clustering)</strong>[Domeniconi et al. 2004]：初始化k个中心以及k个d维权重向量(假设有d个属性)，通过调整权重来尽量达到k个高斯分布</li>
<li><strong>Weighted k-means</strong>[Cheng et al. 2008]: 与LAC很像，但是他允许纳入进一步的约束</li>
<li><strong>COSA</strong>[Friedman and Meulman 2004]:推导出一个可以用于任何聚类算法的相似度矩阵(woc这个思路牛批!),每个属性的权重为</li>
</ul>
<script type="math/tex; mode=display">
\frac{the~distance ~of ~point ~p ~to~ all~ k-nearest neighbor~of~ p~ in~ that~ attribute}{the~ distance~ of ~point~ p~ to~ all~ k-nearest~ neighbor~ of~ p~ in~ all ~attribbutes}</script><h3 id="Subspace-Clustering-Algorithms"><a href="#Subspace-Clustering-Algorithms" class="headerlink" title="Subspace Clustering Algorithms:"></a>Subspace Clustering Algorithms:</h3><ul>
<li><strong>CLIQUE</strong>:利用了基于网格的聚类方法，数据空间由轴平行网格划分为宽度为$\epsilon$的等分单位。只有一个网格中含有不少于$\tau$个点才会被认为是一个dense unit，那么一个cluster被定义为临近dense unit的最大集合，可以采用类似Apriori的思想，先看k维上的dense units，然后根据k维上的dense units确定(k+1)维上的dense units</li>
<li><strong>ENCLUS</strong>[Cheng et al. 1999]: 也是利用grid，但是是为了找可能存在一个或者多个clusters 的子空间</li>
<li><strong>MAFIA</strong>[Nagesh et al. 2001]: 与CLIQUE很相似，但是是采用了一个自适应调整的网格</li>
<li><strong>nCluster</strong>[Liu et al. 2007]：CLIQUE的变体，允许长度为δ的重叠窗口作为网格的一维单元。</li>
<li><strong>SUBCLU</strong>[Kailing et al. 2004]：对于DBSCAN算法来说，密度相连集合满足向下闭合特性。所以可以在子空间中按照Apriori算法的思路来进行聚类,<strong>注意</strong>：一个density threshold（不管是SUBCLU还是基于网格的），一个tighter的阈值在低维度上区分度很好，但是在高位上可能会损失cluster，而较为宽松的阈值可以发现高维度的cluster，但是在低维度的聚类上产生额外的数量的类</li>
</ul>
<h3 id="混合聚类算法-Hybrid-Clustering-Algorithms"><a href="#混合聚类算法-Hybrid-Clustering-Algorithms" class="headerlink" title="混合聚类算法(Hybrid Clustering Algorithms):"></a>混合聚类算法(Hybrid Clustering Algorithms):</h3><ul>
<li><strong>DOC</strong> [Procopiuc et al. 2002]: 使用全局密度阈值通过包含至少α个点的固定边长w的超立方体来定义子空间簇。</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm Library</category>
        <category>Clustering Algorithm</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamic system and optimal control perspective of deep learning (Part II)</title>
    <url>/2021/01/03/Dynamic-System-and-Optimal-Control-Perspective-of-Deep-Learning-Part-II/</url>
    <content><![CDATA[<h2 id="Dynamic-system-and-optimal-control-perspective-of-deep-learning-Part-II"><a href="#Dynamic-system-and-optimal-control-perspective-of-deep-learning-Part-II" class="headerlink" title="Dynamic system and optimal control perspective of deep learning (Part II)"></a>Dynamic system and optimal control perspective of deep learning (Part II)<span id="more"></span></h2><div class="pdfobject-container" data-target="Review2.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>Review</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamic system and optimal control perspective of deep learning (Part I)</title>
    <url>/2020/07/15/Dynamic-System-and-Optimal-Control-Perspective-of-Deep-Learning-Part-I/</url>
    <content><![CDATA[<h2 id="Dynamic-system-and-optimal-control-perspective-of-deep-learning-Part-I"><a href="#Dynamic-system-and-optimal-control-perspective-of-deep-learning-Part-I" class="headerlink" title="Dynamic system and optimal control perspective of deep learning (Part I)"></a>Dynamic system and optimal control perspective of deep learning (Part I)<span id="more"></span></h2><div class="pdfobject-container" data-target="Review1.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>Review</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ICLR23 submitted papers list</title>
    <url>/2022/10/24/ICLR23-submitted-papers-list/</url>
    <content><![CDATA[<h1 id="ICLR23-submitted-paper-list"><a href="#ICLR23-submitted-paper-list" class="headerlink" title="ICLR23 submitted paper list"></a>ICLR23 submitted paper list<span id="more"></span></h1><h2 id="EBM"><a href="#EBM" class="headerlink" title="EBM"></a>EBM</h2><ul>
<li><h4 id="Diffusion-Models-Already-Have-A-Semantic-Latent-Space"><a href="#Diffusion-Models-Already-Have-A-Semantic-Latent-Space" class="headerlink" title="Diffusion Models Already Have A Semantic Latent Space "></a><a href="https://openreview.net/forum?id=pd1P2eUBVfq">Diffusion Models Already Have A Semantic Latent Space </a></h4></li>
<li><h4 id="Diffusion-Models-Already-Have-A-Semantic-Latent-Space-1"><a href="#Diffusion-Models-Already-Have-A-Semantic-Latent-Space-1" class="headerlink" title="Diffusion Models Already Have A Semantic Latent Space "></a><a href="https://openreview.net/forum?id=pd1P2eUBVfq">Diffusion Models Already Have A Semantic Latent Space </a></h4></li>
<li><h4 id="Autoregressive-Diffusion-Model-for-Graph-Generation"><a href="#Autoregressive-Diffusion-Model-for-Graph-Generation" class="headerlink" title="Autoregressive Diffusion Model for Graph Generation "></a><a href="https://openreview.net/forum?id=98J48HZXxd5">Autoregressive Diffusion Model for Graph Generation </a></h4></li>
<li><h4 id="Diffusion-GAN-Training-GANs-with-Diffusion"><a href="#Diffusion-GAN-Training-GANs-with-Diffusion" class="headerlink" title="Diffusion-GAN: Training GANs with Diffusion"></a><a href="https://openreview.net/forum?id=HZf7UbpWHuA">Diffusion-GAN: Training GANs with Diffusion</a></h4></li>
<li><h4 id="Soft-Diffusion-Score-Matching-For-General-Corruptions"><a href="#Soft-Diffusion-Score-Matching-For-General-Corruptions" class="headerlink" title="Soft Diffusion: Score Matching For General Corruptions "></a><a href="https://openreview.net/forum?id=QsVditUhXR">Soft Diffusion: Score Matching For General Corruptions </a></h4></li>
<li><h4 id="Where-to-Diffuse-How-to-Diffuse-and-How-to-get-back-Learning-in-Multivariate-Diffusions"><a href="#Where-to-Diffuse-How-to-Diffuse-and-How-to-get-back-Learning-in-Multivariate-Diffusions" class="headerlink" title="Where to Diffuse, How to Diffuse and How to get back: Learning in Multivariate Diffusions"></a><a href="https://openreview.net/forum?id=osei3IzUia">Where to Diffuse, How to Diffuse and How to get back: Learning in Multivariate Diffusions</a></h4></li>
<li><h4 id="Diffusion-Models-for-Causal-Discovery-via-Topological-Ordering"><a href="#Diffusion-Models-for-Causal-Discovery-via-Topological-Ordering" class="headerlink" title="Diffusion Models for Causal Discovery via Topological Ordering "></a><a href="https://openreview.net/forum?id=Idusfje4-Wq">Diffusion Models for Causal Discovery via Topological Ordering </a></h4></li>
<li><h4 id="Diffusion-based-Image-Translation-using-disentangled-style-and-content-representation"><a href="#Diffusion-based-Image-Translation-using-disentangled-style-and-content-representation" class="headerlink" title="Diffusion-based Image Translation using disentangled style and content representation "></a><a href="https://openreview.net/forum?id=Nayau9fwXU">Diffusion-based Image Translation using disentangled style and content representation </a></h4></li>
<li><h4 id="Diffusion-Probabilistic-Modeling-of-Protein-Backbones-in-3D-for-the-motif-scaffolding-problem"><a href="#Diffusion-Probabilistic-Modeling-of-Protein-Backbones-in-3D-for-the-motif-scaffolding-problem" class="headerlink" title="Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem "></a><a href="https://openreview.net/forum?id=6TxBxqNME1Y">Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem </a></h4></li>
<li><h4 id="Blurring-Diffusion-Models"><a href="#Blurring-Diffusion-Models" class="headerlink" title="Blurring Diffusion Models"></a><a href="https://openreview.net/forum?id=OjDkC57x5sz">Blurring Diffusion Models</a></h4></li>
<li><h4 id="Truncated-Diffusion-Probabilistic-Models-and-Diffusion-based-Adversarial-Auto-Encoders"><a href="#Truncated-Diffusion-Probabilistic-Models-and-Diffusion-based-Adversarial-Auto-Encoders" class="headerlink" title="Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders"></a><a href="https://openreview.net/forum?id=HDxgaKk956l">Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders</a></h4></li>
<li><h4 id="Diffusion-Probabilistic-Fields"><a href="#Diffusion-Probabilistic-Fields" class="headerlink" title="Diffusion Probabilistic Fields"></a><a href="https://openreview.net/forum?id=ik91mY-2GN">Diffusion Probabilistic Fields</a></h4></li>
<li><h4 id="Information-Theoretic-Diffusion"><a href="#Information-Theoretic-Diffusion" class="headerlink" title="Information-Theoretic Diffusion"></a><a href="https://openreview.net/forum?id=UvmDCdSPDOW">Information-Theoretic Diffusion</a></h4></li>
<li><h4 id="Self-conditioned-Embedding-Diffusion-for-Text-Generation"><a href="#Self-conditioned-Embedding-Diffusion-for-Text-Generation" class="headerlink" title="Self-conditioned Embedding Diffusion for Text Generation"></a><a href="https://openreview.net/forum?id=OpzV3lp3IMC">Self-conditioned Embedding Diffusion for Text Generation</a></h4></li>
<li><h4 id="Neural-Diffusion-Processes"><a href="#Neural-Diffusion-Processes" class="headerlink" title="Neural Diffusion Processes "></a><a href="https://openreview.net/forum?id=09I1M8YRJBR">Neural Diffusion Processes </a></h4></li>
<li><h4 id="Denoising-Diffusion-Error-Correction-Codes"><a href="#Denoising-Diffusion-Error-Correction-Codes" class="headerlink" title="Denoising Diffusion Error Correction Codes"></a><a href="https://openreview.net/forum?id=rLwC0_MG-4w">Denoising Diffusion Error Correction Codes</a></h4></li>
<li><h4 id="Denoising-Diffusion-Samplers"><a href="#Denoising-Diffusion-Samplers" class="headerlink" title="Denoising Diffusion Samplers"></a><a href="https://openreview.net/forum?id=8pvnfTAbu1f">Denoising Diffusion Samplers</a></h4></li>
<li><h4 id="TabDDPM-Modelling-Tabular-Data-with-Diffusion-Models"><a href="#TabDDPM-Modelling-Tabular-Data-with-Diffusion-Models" class="headerlink" title="TabDDPM: Modelling Tabular Data with Diffusion Models"></a><a href="https://openreview.net/forum?id=EJka_dVXEcr">TabDDPM: Modelling Tabular Data with Diffusion Models</a></h4></li>
<li><h4 id="Denoising-MCMC-for-Accelerating-Diffusion-Based-Generative-Models"><a href="#Denoising-MCMC-for-Accelerating-Diffusion-Based-Generative-Models" class="headerlink" title="Denoising MCMC for Accelerating Diffusion-Based Generative Models "></a><a href="https://openreview.net/forum?id=Ogh8umAChpo">Denoising MCMC for Accelerating Diffusion-Based Generative Models </a></h4></li>
<li><h4 id="Sequence-to-sequence-text-generation-with-diffusion-models"><a href="#Sequence-to-sequence-text-generation-with-diffusion-models" class="headerlink" title="Sequence to sequence text generation with diffusion models"></a><a href="https://openreview.net/forum?id=jQj-_rLVXsj">Sequence to sequence text generation with diffusion models</a></h4></li>
<li><h4 id="Novel-View-Synthesis-with-Diffusion-Models"><a href="#Novel-View-Synthesis-with-Diffusion-Models" class="headerlink" title="Novel View Synthesis with Diffusion Models "></a><a href="https://openreview.net/forum?id=HtoA0oT30jC">Novel View Synthesis with Diffusion Models </a></h4></li>
<li><h4 id="DPM-Solver-Fast-Solver-for-Guided-Sampling-of-Diffusion-Probabilistic-Models"><a href="#DPM-Solver-Fast-Solver-for-Guided-Sampling-of-Diffusion-Probabilistic-Models" class="headerlink" title="DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models"></a><a href="https://openreview.net/forum?id=4vGwQqviud5">DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models</a></h4></li>
<li><h4 id="Reduce-Reuse-Recycle-Compositional-Generation-with-Energy-Based-Diffusion-Models-and-MCMC"><a href="#Reduce-Reuse-Recycle-Compositional-Generation-with-Energy-Based-Diffusion-Models-and-MCMC" class="headerlink" title="Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC"></a><a href="https://openreview.net/forum?id=OboQ71j1Bn">Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC</a></h4></li>
<li><h4 id="Quasi-Taylor-Samplers-for-Diffusion-Generative-Models-based-on-Ideal-Derivatives"><a href="#Quasi-Taylor-Samplers-for-Diffusion-Generative-Models-based-on-Ideal-Derivatives" class="headerlink" title="Quasi-Taylor Samplers for Diffusion Generative Models based on Ideal Derivatives "></a><a href="https://openreview.net/forum?id=7ks5PS09q1">Quasi-Taylor Samplers for Diffusion Generative Models based on Ideal Derivatives </a></h4></li>
<li><h4 id="DDM2-Self-Supervised-Diffusion-MRI-Denoising-with-Generative-Diffusion-Models"><a href="#DDM2-Self-Supervised-Diffusion-MRI-Denoising-with-Generative-Diffusion-Models" class="headerlink" title="DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models"></a><a href="https://openreview.net/forum?id=0vqjc50HfcC">DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models</a></h4></li>
<li><h4 id="Self-Guided-Diffusion-Models"><a href="#Self-Guided-Diffusion-Models" class="headerlink" title="Self-Guided Diffusion Models"></a><a href="https://openreview.net/forum?id=Gzmyu-Baq0">Self-Guided Diffusion Models</a></h4></li>
<li><h4 id="From-Points-to-Functions-Infinite-dimensional-Representations-in-Diffusion-Models"><a href="#From-Points-to-Functions-Infinite-dimensional-Representations-in-Diffusion-Models" class="headerlink" title="From Points to Functions: Infinite-dimensional Representations in Diffusion Models "></a><a href="https://openreview.net/forum?id=0DwzMsUNIr">From Points to Functions: Infinite-dimensional Representations in Diffusion Models </a></h4></li>
<li><h4 id="Pyramidal-Denoising-Diffusion-Probabilistic-Models"><a href="#Pyramidal-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="Pyramidal Denoising Diffusion Probabilistic Models"></a><a href="https://openreview.net/forum?id=MMKqOJgRiw4">Pyramidal Denoising Diffusion Probabilistic Models</a></h4></li>
<li><h4 id="Prosody-TTS-Self-Supervised-Prosody-Pretraining-with-Latent-Diffusion-For-Text-to-Speech"><a href="#Prosody-TTS-Self-Supervised-Prosody-Pretraining-with-Latent-Diffusion-For-Text-to-Speech" class="headerlink" title="Prosody-TTS: Self-Supervised Prosody Pretraining with Latent Diffusion For Text-to-Speech"></a><a href="https://openreview.net/forum?id=y6EnaJlhcWZ">Prosody-TTS: Self-Supervised Prosody Pretraining with Latent Diffusion For Text-to-Speech</a></h4></li>
<li><h4 id="Modeling-Temporal-Data-as-Continuous-Functions-with-Process-Diffusion"><a href="#Modeling-Temporal-Data-as-Continuous-Functions-with-Process-Diffusion" class="headerlink" title="Modeling Temporal Data as Continuous Functions with Process Diffusion"></a><a href="https://openreview.net/forum?id=1TxMUE7cF6_">Modeling Temporal Data as Continuous Functions with Process Diffusion</a></h4></li>
<li><h4 id="Sampling-is-as-easy-as-learning-the-score-theory-for-diffusion-models-with-minimal-data-assumptions"><a href="#Sampling-is-as-easy-as-learning-the-score-theory-for-diffusion-models-with-minimal-data-assumptions" class="headerlink" title="Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions "></a><a href="https://openreview.net/forum?id=zyLVMgsZ0U_">Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions </a></h4></li>
<li><h4 id="Fast-Sampling-of-Diffusion-Models-with-Exponential-Integrator"><a href="#Fast-Sampling-of-Diffusion-Models-with-Exponential-Integrator" class="headerlink" title="Fast Sampling of Diffusion Models with Exponential Integrator "></a><a href="https://openreview.net/forum?id=Loek7hfb46P">Fast Sampling of Diffusion Models with Exponential Integrator </a></h4></li>
<li><h4 id="Compositional-Image-Generation-and-Manipulation-with-Latent-Diffusion-Models"><a href="#Compositional-Image-Generation-and-Manipulation-with-Latent-Diffusion-Models" class="headerlink" title="Compositional Image Generation and Manipulation with Latent Diffusion Models "></a><a href="https://openreview.net/forum?id=SvcawuEiUVM">Compositional Image Generation and Manipulation with Latent Diffusion Models </a></h4></li>
<li><h4 id="Training-Free-Structured-Diffusion-Guidance-for-Compositional-Text-to-Image-Synthesis"><a href="#Training-Free-Structured-Diffusion-Guidance-for-Compositional-Text-to-Image-Synthesis" class="headerlink" title="Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis"></a><a href="https://openreview.net/forum?id=PUIqjT4rzq7">Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis</a></h4></li>
<li><h4 id="Score-based-Continuous-time-Discrete-Diffusion-Models"><a href="#Score-based-Continuous-time-Discrete-Diffusion-Models" class="headerlink" title="Score-based Continuous-time Discrete Diffusion Models"></a><a href="https://openreview.net/forum?id=BYWWwSY2G5s">Score-based Continuous-time Discrete Diffusion Models</a></h4></li>
<li><h4 id="DeepGRAND-Deep-Graph-Neural-Diffusion"><a href="#DeepGRAND-Deep-Graph-Neural-Diffusion" class="headerlink" title="DeepGRAND: Deep Graph Neural Diffusion"></a><a href="https://openreview.net/forum?id=wTGORH_cHPX">DeepGRAND: Deep Graph Neural Diffusion</a></h4></li>
<li><h4 id="Score-Matching-via-Differentiable-Physics"><a href="#Score-Matching-via-Differentiable-Physics" class="headerlink" title="Score Matching via Differentiable Physics "></a><a href="https://openreview.net/forum?id=t9myAV_dpCB">Score Matching via Differentiable Physics </a></h4></li>
<li><h4 id="Human-Motion-Diffusion-Model"><a href="#Human-Motion-Diffusion-Model" class="headerlink" title="Human Motion Diffusion Model"></a><a href="https://openreview.net/forum?id=SJ1kSyO2jwu">Human Motion Diffusion Model</a></h4></li>
<li><h4 id="Learning-Diffusion-Bridges-on-Constrained-Domains"><a href="#Learning-Diffusion-Bridges-on-Constrained-Domains" class="headerlink" title="Learning Diffusion Bridges on Constrained Domains "></a><a href="https://openreview.net/forum?id=WH1yCa0TbB">Learning Diffusion Bridges on Constrained Domains </a></h4></li>
<li><h4 id="Cold-Diffusion-Inverting-Arbitrary-Image-Transforms-Without-Noise"><a href="#Cold-Diffusion-Inverting-Arbitrary-Image-Transforms-Without-Noise" class="headerlink" title="Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise "></a><a href="https://openreview.net/forum?id=slHNW9yRie0">Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise </a></h4></li>
<li><h4 id="Understanding-DDPM-Latent-Codes-Through-Optimal-Transport"><a href="#Understanding-DDPM-Latent-Codes-Through-Optimal-Transport" class="headerlink" title="Understanding DDPM Latent Codes Through Optimal Transport"></a><a href="https://openreview.net/forum?id=6PIrhAx1j4i">Understanding DDPM Latent Codes Through Optimal Transport</a></h4></li>
<li><h4 id="Flow-Matching-for-Generative-Modeling"><a href="#Flow-Matching-for-Generative-Modeling" class="headerlink" title="Flow Matching for Generative Modeling "></a><a href="https://openreview.net/forum?id=PqvMRDCJT9t">Flow Matching for Generative Modeling </a></h4></li>
<li><h4 id="ResGrad-Residual-Denoising-Diffusion-Probabilistic-Models-for-Text-to-Speech"><a href="#ResGrad-Residual-Denoising-Diffusion-Probabilistic-Models-for-Text-to-Speech" class="headerlink" title="ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech"></a><a href="https://openreview.net/forum?id=4daKS8wEze5">ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech</a></h4></li>
<li><h4 id="SPI-GAN-Denoising-Diffusion-GANs-with-Straight-Path-Interpolations"><a href="#SPI-GAN-Denoising-Diffusion-GANs-with-Straight-Path-Interpolations" class="headerlink" title="SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations "></a><a href="https://openreview.net/forum?id=9XFX-DdkGp9">SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations </a></h4></li>
<li><h4 id="Approximated-Anomalous-Diffusion-Gaussian-Mixture-Score-based-Generative-Models"><a href="#Approximated-Anomalous-Diffusion-Gaussian-Mixture-Score-based-Generative-Models" class="headerlink" title="Approximated Anomalous Diffusion: Gaussian Mixture Score-based Generative Models"></a><a href="https://openreview.net/forum?id=yc9xen7EAzd">Approximated Anomalous Diffusion: Gaussian Mixture Score-based Generative Models</a></h4></li>
<li><h4 id="Neural-Lagrangian-Schr-”-o-dinger-Bridge-Diffusion-Modeling-for-Population-Dynamics"><a href="#Neural-Lagrangian-Schr-”-o-dinger-Bridge-Diffusion-Modeling-for-Population-Dynamics" class="headerlink" title="Neural Lagrangian Schr\”{o}dinger Bridge: Diffusion Modeling for Population Dynamics "></a><a href="https://openreview.net/forum?id=d3QNWD_pcFv">Neural Lagrangian Schr\”{o}dinger Bridge: Diffusion Modeling for Population Dynamics </a></h4></li>
<li><h4 id="FastDiff-2-Dually-Incorporating-GANs-into-Diffusion-Models-for-High-Quality-Speech-Synthesis"><a href="#FastDiff-2-Dually-Incorporating-GANs-into-Diffusion-Models-for-High-Quality-Speech-Synthesis" class="headerlink" title="FastDiff 2: Dually Incorporating GANs into Diffusion Models for High-Quality Speech Synthesis"></a><a href="https://openreview.net/forum?id=-x5WuMO4APy">FastDiff 2: Dually Incorporating GANs into Diffusion Models for High-Quality Speech Synthesis</a></h4></li>
<li><h4 id="f-DM-A-Multi-stage-Diffusion-Model-via-Progressive-Signal-Transformation"><a href="#f-DM-A-Multi-stage-Diffusion-Model-via-Progressive-Signal-Transformation" class="headerlink" title="f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation"></a><a href="https://openreview.net/forum?id=iBdwKIsg4m">f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation</a></h4></li>
</ul>
<h2 id="GAN-FLow-VAE"><a href="#GAN-FLow-VAE" class="headerlink" title="GAN+FLow+VAE"></a>GAN+FLow+VAE</h2><ul>
<li><h4 id="Flow-Straight-and-Fast-Learning-to-Generate-and-Transfer-Data-with-Rectified-Flow"><a href="#Flow-Straight-and-Fast-Learning-to-Generate-and-Transfer-Data-with-Rectified-Flow" class="headerlink" title="Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow"></a><a href="https://openreview.net/forum?id=XVjTT1nw5z">Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow</a></h4></li>
<li><h4 id="Normalizing-Flows-for-Interventional-Density-Estimation"><a href="#Normalizing-Flows-for-Interventional-Density-Estimation" class="headerlink" title="Normalizing Flows for Interventional Density Estimation"></a><a href="https://openreview.net/forum?id=bTy4D3KHwWU">Normalizing Flows for Interventional Density Estimation</a></h4></li>
<li><h4 id="Deep-Generative-Wasserstein-Gradient-Flows"><a href="#Deep-Generative-Wasserstein-Gradient-Flows" class="headerlink" title="Deep Generative Wasserstein Gradient Flows"></a><a href="https://openreview.net/forum?id=zjSeBTEdXp1">Deep Generative Wasserstein Gradient Flows</a></h4></li>
<li><h4 id="Building-Normalizing-Flows-with-Stochastic-Interpolants"><a href="#Building-Normalizing-Flows-with-Stochastic-Interpolants" class="headerlink" title="Building Normalizing Flows with Stochastic Interpolants"></a><a href="https://openreview.net/forum?id=li7qeBbCR1t">Building Normalizing Flows with Stochastic Interpolants</a></h4></li>
<li><h4 id="Semi-Autoregressive-Energy-Flows-Towards-Determinant-Free-Training-of-Normalizing-Flows"><a href="#Semi-Autoregressive-Energy-Flows-Towards-Determinant-Free-Training-of-Normalizing-Flows" class="headerlink" title="Semi-Autoregressive Energy Flows: Towards Determinant-Free Training of Normalizing Flows "></a><a href="https://openreview.net/forum?id=GBU1mm8_WkV">Semi-Autoregressive Energy Flows: Towards Determinant-Free Training of Normalizing Flows </a></h4></li>
<li><h4 id="AE-FLOW-Autoencoders-with-Normalizing-Flows-for-Medical-Images-Anomaly-Detection"><a href="#AE-FLOW-Autoencoders-with-Normalizing-Flows-for-Medical-Images-Anomaly-Detection" class="headerlink" title="AE-FLOW: Autoencoders with Normalizing Flows for Medical Images Anomaly Detection"></a><a href="https://openreview.net/forum?id=9OmCr1q54Z">AE-FLOW: Autoencoders with Normalizing Flows for Medical Images Anomaly Detection</a></h4></li>
<li><h4 id="Invertible-normalizing-flow-neural-networks-by-JKO-scheme"><a href="#Invertible-normalizing-flow-neural-networks-by-JKO-scheme" class="headerlink" title="Invertible normalizing flow neural networks by JKO scheme"></a><a href="https://openreview.net/forum?id=-z7O7fk_Cs">Invertible normalizing flow neural networks by JKO scheme</a></h4></li>
<li><h4 id="NEURAL-HAMILTONIAN-FLOWS-IN-GRAPH-NEURAL-NETWORKS"><a href="#NEURAL-HAMILTONIAN-FLOWS-IN-GRAPH-NEURAL-NETWORKS" class="headerlink" title="NEURAL HAMILTONIAN FLOWS IN GRAPH NEURAL NETWORKS"></a><a href="https://openreview.net/forum?id=lhPLT5gnBrH">NEURAL HAMILTONIAN FLOWS IN GRAPH NEURAL NETWORKS</a></h4></li>
<li><h4 id="Generative-Augmented-Flow-Networks"><a href="#Generative-Augmented-Flow-Networks" class="headerlink" title="Generative Augmented Flow Networks "></a><a href="https://openreview.net/forum?id=urF_CBK5XC0">Generative Augmented Flow Networks </a></h4></li>
<li><h4 id="GM-VAE-Representation-Learning-with-VAE-on-Gaussian-Manifold"><a href="#GM-VAE-Representation-Learning-with-VAE-on-Gaussian-Manifold" class="headerlink" title="GM-VAE: Representation Learning with VAE on Gaussian Manifold "></a><a href="https://openreview.net/forum?id=uV1A7jemwS8">GM-VAE: Representation Learning with VAE on Gaussian Manifold </a></h4></li>
<li><h4 id="Vector-Quantized-Wasserstein-Auto-Encoder"><a href="#Vector-Quantized-Wasserstein-Auto-Encoder" class="headerlink" title="Vector Quantized Wasserstein Auto-Encoder"></a><a href="https://openreview.net/forum?id=Z8qk2iM5uLI">Vector Quantized Wasserstein Auto-Encoder</a></h4></li>
</ul>
<h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><ul>
<li><h4 id="FDNet-Focal-Decomposed-Network-for-Efficient-Robust-and-Practical-time-series-forecasting"><a href="#FDNet-Focal-Decomposed-Network-for-Efficient-Robust-and-Practical-time-series-forecasting" class="headerlink" title="FDNet: Focal Decomposed Network for Efficient, Robust and Practical time series forecasting "></a><a href="https://openreview.net/forum?id=WXjBX7uz7lO">FDNet: Focal Decomposed Network for Efficient, Robust and Practical time series forecasting </a></h4></li>
<li><h4 id="Temporal-Dependencies-in-Feature-Importance-for-Time-Series-Prediction"><a href="#Temporal-Dependencies-in-Feature-Importance-for-Time-Series-Prediction" class="headerlink" title="Temporal Dependencies in Feature Importance for Time Series Prediction"></a><a href="https://openreview.net/forum?id=C0q9oBc3n4">Temporal Dependencies in Feature Importance for Time Series Prediction</a></h4></li>
<li><h4 id="Towards-Unsupervised-Time-Series-Representation-Learning-A-Decomposition-Perspective"><a href="#Towards-Unsupervised-Time-Series-Representation-Learning-A-Decomposition-Perspective" class="headerlink" title="Towards Unsupervised Time Series Representation Learning: A Decomposition Perspective "></a><a href="https://openreview.net/forum?id=8IMz713Bxcq">Towards Unsupervised Time Series Representation Learning: A Decomposition Perspective </a></h4></li>
<li><h4 id="MultiWave-Multiresolution-Deep-Architectures-through-Wavelet-Decomposition-for-Multivariate-Timeseries-Forecasting-and-Prediction"><a href="#MultiWave-Multiresolution-Deep-Architectures-through-Wavelet-Decomposition-for-Multivariate-Timeseries-Forecasting-and-Prediction" class="headerlink" title="MultiWave: Multiresolution Deep Architectures through Wavelet Decomposition for Multivariate Timeseries Forecasting and Prediction"></a><a href="https://openreview.net/forum?id=lZKBhpedXk">MultiWave: Multiresolution Deep Architectures through Wavelet Decomposition for Multivariate Timeseries Forecasting and Prediction</a></h4></li>
<li><h4 id="FrAug-Frequency-Domain-Augmentation-for-Time-Series-Forecasting"><a href="#FrAug-Frequency-Domain-Augmentation-for-Time-Series-Forecasting" class="headerlink" title="FrAug: Frequency Domain Augmentation for Time Series Forecasting"></a><a href="https://openreview.net/forum?id=j83rZLZgYBv">FrAug: Frequency Domain Augmentation for Time Series Forecasting</a></h4></li>
<li><h4 id="Out-of-distribution-Representation-Learning-for-Time-Series-Classification"><a href="#Out-of-distribution-Representation-Learning-for-Time-Series-Classification" class="headerlink" title="Out-of-distribution Representation Learning for Time Series Classification"></a><a href="https://openreview.net/forum?id=gUZWOE42l6Q">Out-of-distribution Representation Learning for Time Series Classification</a></h4></li>
<li><h4 id="Representing-Multi-view-Time-series-Graph-Structures-for-Multivariate-Long-term-Time-series-Forecasting"><a href="#Representing-Multi-view-Time-series-Graph-Structures-for-Multivariate-Long-term-Time-series-Forecasting" class="headerlink" title="Representing Multi-view Time-series Graph Structures for Multivariate Long-term Time-series Forecasting"></a><a href="https://openreview.net/forum?id=44GCcwJ5X2">Representing Multi-view Time-series Graph Structures for Multivariate Long-term Time-series Forecasting</a></h4></li>
<li><h4 id="Time-Transformer-AAE-Connecting-Temporal-Convolutional-Networks-and-Transformer-for-Time-Series-Generation"><a href="#Time-Transformer-AAE-Connecting-Temporal-Convolutional-Networks-and-Transformer-for-Time-Series-Generation" class="headerlink" title="Time-Transformer AAE: Connecting Temporal Convolutional Networks and Transformer for Time Series Generation"></a><a href="https://openreview.net/forum?id=fI3y_Dajlca">Time-Transformer AAE: Connecting Temporal Convolutional Networks and Transformer for Time Series Generation</a></h4></li>
<li><h4 id="Effectively-Modeling-Time-Series-with-Simple-Discrete-State-Spaces"><a href="#Effectively-Modeling-Time-Series-with-Simple-Discrete-State-Spaces" class="headerlink" title="Effectively Modeling Time Series with Simple Discrete State Spaces"></a><a href="https://openreview.net/forum?id=2EpjkjzdCAa">Effectively Modeling Time Series with Simple Discrete State Spaces</a></h4></li>
<li><h4 id="VQ-TR-Vector-Quantized-Attention-for-Time-Series-Forecasting"><a href="#VQ-TR-Vector-Quantized-Attention-for-Time-Series-Forecasting" class="headerlink" title="VQ-TR: Vector Quantized Attention for Time Series Forecasting "></a><a href="https://openreview.net/forum?id=RMnJxnLwGak">VQ-TR: Vector Quantized Attention for Time Series Forecasting </a></h4></li>
<li><h4 id="Copula-Conformal-Prediction-for-Multi-step-Time-Series-Forecasting"><a href="#Copula-Conformal-Prediction-for-Multi-step-Time-Series-Forecasting" class="headerlink" title="Copula Conformal Prediction for Multi-step Time Series Forecasting"></a><a href="https://openreview.net/forum?id=jCdoLxMZxf">Copula Conformal Prediction for Multi-step Time Series Forecasting</a></h4></li>
<li><h4 id="Irregularity-Reflection-Neural-Network-for-Time-Series-Forecasting"><a href="#Irregularity-Reflection-Neural-Network-for-Time-Series-Forecasting" class="headerlink" title="Irregularity Reflection Neural Network for Time Series Forecasting"></a><a href="https://openreview.net/forum?id=zuQQ7GrDFfH">Irregularity Reflection Neural Network for Time Series Forecasting</a></h4></li>
<li><h4 id="SpectraNet-multivariate-forecasting-and-imputation-under-distribution-shifts-and-missing-data"><a href="#SpectraNet-multivariate-forecasting-and-imputation-under-distribution-shifts-and-missing-data" class="headerlink" title="SpectraNet: multivariate forecasting and imputation under distribution shifts and missing data "></a><a href="https://openreview.net/forum?id=bSuY3hSRJPP">SpectraNet: multivariate forecasting and imputation under distribution shifts and missing data </a></h4></li>
<li><h4 id="Unsupervised-Model-Selection-for-Time-Series-Anomaly-Detection"><a href="#Unsupervised-Model-Selection-for-Time-Series-Anomaly-Detection" class="headerlink" title="Unsupervised Model Selection for Time Series Anomaly Detection"></a><a href="https://openreview.net/forum?id=gOZ_pKANaPW">Unsupervised Model Selection for Time Series Anomaly Detection</a></h4></li>
<li><h4 id="DeepTime-Deep-Time-index-Meta-learning-for-Non-stationary-Time-series-Forecasting"><a href="#DeepTime-Deep-Time-index-Meta-learning-for-Non-stationary-Time-series-Forecasting" class="headerlink" title="DeepTime: Deep Time-index Meta-learning for Non-stationary Time-series Forecasting"></a><a href="https://openreview.net/forum?id=13rQhx37o3u">DeepTime: Deep Time-index Meta-learning for Non-stationary Time-series Forecasting</a></h4></li>
<li><h4 id="Time-Series-are-Images-Vision-Transformer-for-Irregularly-Sampled-Time-Series"><a href="#Time-Series-are-Images-Vision-Transformer-for-Irregularly-Sampled-Time-Series" class="headerlink" title="Time Series are Images: Vision Transformer for Irregularly Sampled Time Series"></a><a href="https://openreview.net/forum?id=lRgEbHxowq">Time Series are Images: Vision Transformer for Irregularly Sampled Time Series</a></h4></li>
<li><h4 id="Ti-MAE-Self-Supervised-Masked-Time-Series-Autoencoders"><a href="#Ti-MAE-Self-Supervised-Masked-Time-Series-Autoencoders" class="headerlink" title="Ti-MAE: Self-Supervised Masked Time Series Autoencoders "></a><a href="https://openreview.net/forum?id=9AuIMiZhkL2">Ti-MAE: Self-Supervised Masked Time Series Autoencoders </a></h4></li>
<li><h4 id="Latent-Linear-ODEs-with-Neural-Kalman-Filtering-for-Irregular-Time-Series-Forecasting"><a href="#Latent-Linear-ODEs-with-Neural-Kalman-Filtering-for-Irregular-Time-Series-Forecasting" class="headerlink" title="Latent Linear ODEs with Neural Kalman Filtering for Irregular Time Series Forecasting"></a><a href="https://openreview.net/forum?id=a-bD9-0ycs0">Latent Linear ODEs with Neural Kalman Filtering for Irregular Time Series Forecasting</a></h4></li>
<li><h4 id="TimeSeAD-Benchmarking-Deep-Time-Series-Anomaly-Detection"><a href="#TimeSeAD-Benchmarking-Deep-Time-Series-Anomaly-Detection" class="headerlink" title="TimeSeAD: Benchmarking Deep Time-Series Anomaly Detection"></a><a href="https://openreview.net/forum?id=UiNhIyGi1MT">TimeSeAD: Benchmarking Deep Time-Series Anomaly Detection</a></h4></li>
<li><h4 id="Deep-Probabilistic-Time-Series-Forecasting-over-Long-Horizons"><a href="#Deep-Probabilistic-Time-Series-Forecasting-over-Long-Horizons" class="headerlink" title="Deep Probabilistic Time Series Forecasting over Long Horizons"></a><a href="https://openreview.net/forum?id=22h1XSEiN0">Deep Probabilistic Time Series Forecasting over Long Horizons</a></h4></li>
<li><h4 id="Memory-Learning-of-Multivariate-Asynchronous-Time-Series"><a href="#Memory-Learning-of-Multivariate-Asynchronous-Time-Series" class="headerlink" title="Memory Learning of Multivariate Asynchronous Time Series"></a><a href="https://openreview.net/forum?id=LPcxnvN9vLw">Memory Learning of Multivariate Asynchronous Time Series</a></h4></li>
<li><h4 id="Time-Series-Subsequence-Anomaly-Detection-via-Graph-Neural-Networks"><a href="#Time-Series-Subsequence-Anomaly-Detection-via-Graph-Neural-Networks" class="headerlink" title="Time Series Subsequence Anomaly Detection via Graph Neural Networks"></a><a href="https://openreview.net/forum?id=73U_NlKaNx">Time Series Subsequence Anomaly Detection via Graph Neural Networks</a></h4></li>
<li><h4 id="Dateformer-Transformer-Extends-Look-back-Horizon-to-Predict-Longer-term-Time-Series"><a href="#Dateformer-Transformer-Extends-Look-back-Horizon-to-Predict-Longer-term-Time-Series" class="headerlink" title="Dateformer: Transformer Extends Look-back Horizon to Predict Longer-term Time Series "></a><a href="https://openreview.net/forum?id=77aKxP46geN">Dateformer: Transformer Extends Look-back Horizon to Predict Longer-term Time Series </a></h4></li>
<li><h4 id="Scaleformer-Iterative-Multi-scale-Refining-Transformers-for-Time-Series-Forecasting"><a href="#Scaleformer-Iterative-Multi-scale-Refining-Transformers-for-Time-Series-Forecasting" class="headerlink" title="Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting"></a><a href="https://openreview.net/forum?id=sCrnllCtjoE">Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting</a></h4></li>
<li><h4 id="Multivariate-Time-series-Imputation-with-Disentangled-Temporal-Representations"><a href="#Multivariate-Time-series-Imputation-with-Disentangled-Temporal-Representations" class="headerlink" title="Multivariate Time-series Imputation with Disentangled Temporal Representations"></a><a href="https://openreview.net/forum?id=rdjeCNUS6TG">Multivariate Time-series Imputation with Disentangled Temporal Representations</a></h4></li>
<li><h4 id="Dynamic-Aware-GANs-Time-Series-Generation-with-Handy-Self-Supervision"><a href="#Dynamic-Aware-GANs-Time-Series-Generation-with-Handy-Self-Supervision" class="headerlink" title="Dynamic-Aware GANs: Time-Series Generation with Handy Self-Supervision"></a><a href="https://openreview.net/forum?id=Oy-e1gcBzo">Dynamic-Aware GANs: Time-Series Generation with Handy Self-Supervision</a></h4></li>
</ul>
<h2 id="ODE-SDE-PDE"><a href="#ODE-SDE-PDE" class="headerlink" title="ODE + SDE + PDE"></a>ODE + SDE + PDE</h2><ul>
<li><h4 id="Characteristic-Neural-Ordinary-Differential-Equation"><a href="#Characteristic-Neural-Ordinary-Differential-Equation" class="headerlink" title="Characteristic Neural Ordinary Differential Equation"></a><a href="https://openreview.net/forum?id=loIfC8WHevK">Characteristic Neural Ordinary Differential Equation</a></h4></li>
<li><h4 id="Oscillation-Neural-Ordinary-Differential-Equations"><a href="#Oscillation-Neural-Ordinary-Differential-Equations" class="headerlink" title="Oscillation Neural Ordinary Differential Equations "></a><a href="https://openreview.net/forum?id=afrUI9hkUJM">Oscillation Neural Ordinary Differential Equations </a></h4></li>
<li><h4 id="Continuous-Depth-Recurrent-Neural-Differential-Equations"><a href="#Continuous-Depth-Recurrent-Neural-Differential-Equations" class="headerlink" title="Continuous Depth Recurrent Neural Differential Equations"></a><a href="https://openreview.net/forum?id=-p5ZEVGtojQ">Continuous Depth Recurrent Neural Differential Equations</a></h4></li>
<li><h4 id="SYNC-SAFETY-AWARE-NEURAL-CONTROL-FOR-STABILIZING-STOCHASTIC-DELAY-DIFFERENTIAL-EQUATIONS"><a href="#SYNC-SAFETY-AWARE-NEURAL-CONTROL-FOR-STABILIZING-STOCHASTIC-DELAY-DIFFERENTIAL-EQUATIONS" class="headerlink" title="SYNC: SAFETY-AWARE NEURAL CONTROL FOR STABILIZING STOCHASTIC DELAY-DIFFERENTIAL EQUATIONS "></a><a href="https://openreview.net/forum?id=_8mS2NE-HXN">SYNC: SAFETY-AWARE NEURAL CONTROL FOR STABILIZING STOCHASTIC DELAY-DIFFERENTIAL EQUATIONS </a></h4></li>
<li><h4 id="Partial-Differential-Equation-Regularized-Neural-Networks-An-Application-to-Image-Classification"><a href="#Partial-Differential-Equation-Regularized-Neural-Networks-An-Application-to-Image-Classification" class="headerlink" title="Partial Differential Equation-Regularized Neural Networks: An Application to Image Classification"></a><a href="https://openreview.net/forum?id=YurfS_kh5ib">Partial Differential Equation-Regularized Neural Networks: An Application to Image Classification</a></h4></li>
<li><h4 id="Self-Paced-Learning-Enhanced-Physics-informed-Neural-Networks-for-Solving-Partial-Differential-Equations"><a href="#Self-Paced-Learning-Enhanced-Physics-informed-Neural-Networks-for-Solving-Partial-Differential-Equations" class="headerlink" title="Self-Paced Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations"></a><a href="https://openreview.net/forum?id=QugfmhDu5Y4">Self-Paced Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations</a></h4></li>
<li><h4 id="Delta-PINNs-physics-informed-neural-networks-on-complex-geometries"><a href="#Delta-PINNs-physics-informed-neural-networks-on-complex-geometries" class="headerlink" title="$\Delta$ -PINNs: physics-informed neural networks on complex geometries"></a><a href="https://openreview.net/forum?id=5P96KWeULzE">$\Delta$ -PINNs: physics-informed neural networks on complex geometries</a></h4></li>
<li><h4 id="Parameter-varying-neural-ordinary-differential-equations-with-partition-of-unity-networks"><a href="#Parameter-varying-neural-ordinary-differential-equations-with-partition-of-unity-networks" class="headerlink" title="Parameter-varying neural ordinary differential equations with partition-of-unity networks"></a><a href="https://openreview.net/forum?id=heDr8wIYmw_">Parameter-varying neural ordinary differential equations with partition-of-unity networks</a></h4></li>
<li><h4 id="Gated-Neural-ODEs-Trainability-Expressivity-and-Interpretability"><a href="#Gated-Neural-ODEs-Trainability-Expressivity-and-Interpretability" class="headerlink" title="Gated Neural ODEs: Trainability, Expressivity and Interpretability"></a><a href="https://openreview.net/forum?id=ArPM-xtsFrk">Gated Neural ODEs: Trainability, Expressivity and Interpretability</a></h4></li>
<li><h4 id="When-Neural-ODEs-meet-Neural-Operators"><a href="#When-Neural-ODEs-meet-Neural-Operators" class="headerlink" title="When Neural ODEs meet Neural Operators"></a><a href="https://openreview.net/forum?id=P5ZTXA7zy6">When Neural ODEs meet Neural Operators</a></h4></li>
<li><h4 id="Neural-Integral-Equations"><a href="#Neural-Integral-Equations" class="headerlink" title="Neural Integral Equations"></a><a href="https://openreview.net/forum?id=-3br92QL76O">Neural Integral Equations</a></h4></li>
<li><h4 id="Improved-Training-of-Physics-Informed-Neural-Networks-with-Model-Ensembles"><a href="#Improved-Training-of-Physics-Informed-Neural-Networks-with-Model-Ensembles" class="headerlink" title="Improved Training of Physics-Informed Neural Networks with Model Ensembles"></a><a href="https://openreview.net/forum?id=FEAIArDldTA">Improved Training of Physics-Informed Neural Networks with Model Ensembles</a></h4></li>
<li><h4 id="Robust-Neural-ODEs-via-Contractivity-promoting-Regularization"><a href="#Robust-Neural-ODEs-via-Contractivity-promoting-Regularization" class="headerlink" title="Robust Neural ODEs via Contractivity-promoting Regularization "></a><a href="https://openreview.net/forum?id=n8toFjHwyjq">Robust Neural ODEs via Contractivity-promoting Regularization </a></h4></li>
<li><h4 id="Efficient-Certified-Training-and-Robustness-Verification-of-Neural-ODEs"><a href="#Efficient-Certified-Training-and-Robustness-Verification-of-Neural-ODEs" class="headerlink" title="Efficient Certified Training and Robustness Verification of Neural ODEs "></a><a href="https://openreview.net/forum?id=KyoVpYvWWnK">Efficient Certified Training and Robustness Verification of Neural ODEs </a></h4></li>
<li><h4 id="S-SOLVER-Numerically-Stable-Adaptive-Step-Size-Solver-for-Neural-ODEs"><a href="#S-SOLVER-Numerically-Stable-Adaptive-Step-Size-Solver-for-Neural-ODEs" class="headerlink" title="S-SOLVER: Numerically Stable Adaptive Step Size Solver for Neural ODEs "></a><a href="https://openreview.net/forum?id=f23tQmoxWz-">S-SOLVER: Numerically Stable Adaptive Step Size Solver for Neural ODEs </a></h4></li>
</ul>
]]></content>
      <categories>
        <category>machine learning</category>
        <category>PaperList</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
</search>
